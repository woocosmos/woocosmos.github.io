<!DOCTYPE html>
<html>
  <head>
  <title>PyTorch 파이토치 기초 모음집 – YunsooLog – 나름 진지한 ML Engineer</title>
  <link rel="icon" href="/images/favicon.ico">
      <meta name="google-site-verification" content="NA4jg1Iffw6aA9VWjj3kqoo2jfOkPRxINJtYphd7VeI" />
    <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>
    <meta http-equiv="Permissions-Policy" content="interest-cohort=()">
    
    
    <meta name="description" content="PyTorch의 구성 요소와 동작 원리를 살펴본다.
" />
    <meta property="og:description" content="PyTorch의 구성 요소와 동작 원리를 살펴본다.
" />
    
    <meta name="author" content="YunsooLog" />

    
    <meta property="og:title" content="PyTorch 파이토치 기초 모음집" />
    <meta property="twitter:title" content="PyTorch 파이토치 기초 모음집" />
    
  <!-- Async font loading -->
<script>
  window.WebFontConfig = {
      custom: {
          families: ['Spoqa Han Sans:100,300,400,700'],
          urls: ['https://spoqa.github.io/spoqa-han-sans/css/SpoqaHanSans-kr.css']
      },
      timeout: 60000
  };
  (function(d) {
      var wf = d.createElement('script'), s = d.scripts[0];
      wf.src = 'https://ajax.googleapis.com/ajax/libs/webfont/1.5.18/webfont.js';
      s.parentNode.insertBefore(wf, s);
  })(document);
</script>

  <!--[if lt IE 9]>
    <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->

  <link rel="stylesheet" type="text/css" href="/style.css" />
  <link rel="alternate" type="application/rss+xml" title="YunsooLog - 나름 진지한 ML Engineer" href="/feed.xml" />
  <script>
    document.cookie = "promo_shown=1; SameSite=Lax; path=/";
  </script>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/contrib/auto-render.min.js"
          onload="renderMathInElement(document.body, {
            delimiters: [
              {left: '\\[', right: '\\]', display: true},
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
            ]
          });">
  </script>

  <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
</head>
  <body>
    <div class="wrapper-masthead">
  <div class="container">
    <header class="masthead clearfix">
      <a href="/" class="site-avatar"><img src="/images/avatar.png"/></a>
      
      <div class="site-info">
        <h1 class="site-name"><a href="/">YunsooLog</a></h1>
        <p class="site-description">나름 진지한 ML Engineer</p>
      </div>

      <nav>
        
        
        <a href="/about">About</a>
        
        
        
        <a href="/">Blog</a>
        
        
        
        <a href="/tags">Tags</a>
        
        
        
        <a href="/timeline">Timeline</a>
        
        
        
        <ul class="search-icon">
          <a href="/search">
          <svg 
            width="24" 
            height="24" 
            viewBox="0 0 24 24" 
            fill="none" 
            xmlns="http://www.w3.org/2000/svg">
            <path d="M10 2C5.58172 2 2 5.58172 2 10C2 14.4183 5.58172 18 10 18C11.8485 18 13.5451 17.3376 14.8458 16.2416L19.7071 21.1029C20.0976 21.4934 20.7308 21.4934 21.1213 21.1029C21.5118 20.7124 21.5118 20.0792 21.1213 19.6887L16.2416 14.809C17.3376 13.5083 18 11.8116 18 10C18 5.58172 14.4183 2 10 2ZM4 10C4 6.68629 6.68629 4 10 4C13.3137 4 16 6.68629 16 10C16 13.3137 13.3137 16 10 16C6.68629 16 4 13.3137 4 10Z" 
            fill="currentColor"></path>
          </svg>
          </a>
        </ul>

      </nav>
    </header>
  </div>
</div>

    <div id="main" role="main" class="container">
      <article class="post">
  <div class="inner-wrap">
    <h1>PyTorch 파이토치 기초 모음집</h1>

    <div>
      <span class="date">
        2024-01-18
      </span>

      <ul class="tag">
        
        <li>
          <a href="http://woocosmos.github.io/tags#PyTorch">
            PyTorch
          </a>
        </li>
        
      </ul>
    </div>

    <section class="entry">
      
      <aside>
        <nav class="nav-toc">
          <h3> 목차 </h3>
          <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#개발-환경">개발 환경</a></li>
<li class="toc-entry toc-h1"><a href="#tensor텐서">Tensor(텐서)</a></li>
<li class="toc-entry toc-h1"><a href="#autograd-자동미분">Autograd (자동미분)</a>
<ul>
<li class="toc-entry toc-h2"><a href="#예제">예제</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#학습">학습</a></li>
</ul>
        </nav>
      </aside>
      <script src="/assets/scroll-spy.js" type="text/javascript"></script>
      
      <p>PyTorch의 구성 요소와 동작 원리를 살펴본다.</p>

<h1 id="개발-환경">개발 환경</h1>

<p><strong>Windows</strong></p>

<ol>
  <li>파이썬과 패키지툴을 준비한다
    <ul>
      <li><strong>python 3.8</strong> : <a href="https://pytorch.org/get-started/locally/#windows-python">공식 문서</a>에 따르면 현재<sub>2024-09-03</sub> 윈도우에서 파이토치는 파이썬 3.8-3.11 버전만 지원된다. <em>나는 파이썬 버전 관리를 위해 윈도우용 pyenv인 <a href="https://github.com/pyenv-win/pyenv-win">pyenv-win</a>를 사용했다</em></li>
      <li><strong>pipenv</strong> : 파이썬에서 공식으로 권장하는 패키지 관리툴</li>
    </ul>
  </li>
  <li>프로젝트 폴더에 접근하여 pipenv 의 파이썬 버전과 가상환경을 설정한다
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> /path/to/project
pipenv <span class="nt">--python</span> 3.8
pipenv shell
</code></pre></div>    </div>
  </li>
  <li>생성된 <code class="language-plaintext highlighter-rouge">Pipfile</code>파일에 파이토치 설치를 위한 주소를 추가한다.
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[source]]
url = "https://download.pytorch.org/whl/cu118"
verify_ssl = true
name = "pytorch"
</code></pre></div>    </div>
  </li>
  <li>파이토치를 설치한다
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pipenv <span class="nb">install</span> <span class="nt">--index</span><span class="o">=</span>pytorch torch
</code></pre></div>    </div>
    <p><br /></p>
  </li>
</ol>

<p><strong>Ubuntu (WSL)</strong></p>
<ol>
  <li>miniconda 로 파이썬 버전을 명시한 새 환경을 생성한다
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda create <span class="nt">-n</span> my-env <span class="nv">python</span><span class="o">==</span>3.8.2
</code></pre></div>    </div>
  </li>
  <li>PyTorch 및 CUDA를 설치한다
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda <span class="nb">install </span><span class="nv">cudatoolkit</span><span class="o">=</span>11.8 <span class="nt">-c</span> conda-forge
conda <span class="nb">install </span>pytorch torchvision torchaudio pytorch-cuda<span class="o">=</span>11.8 <span class="nt">-c</span> pytorch <span class="nt">-c</span> nvidia
</code></pre></div>    </div>
    <p><br /></p>
  </li>
</ol>

<p><strong>테스트</strong><br />
pipenv shell 혹은 conda activate my-env 으로 가상 환경에 접근한 다음, 파이썬으로 아래 요소들을 호출한다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>

<span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span>
<span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">device_count</span><span class="p">()</span>
<span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">current_device</span><span class="p">()</span>
<span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">get_device_name</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">torch</span><span class="p">.</span><span class="n">__version__</span> 
<span class="n">torch</span><span class="p">.</span><span class="n">version</span><span class="p">.</span><span class="n">cuda</span>
</code></pre></div></div>

<h1 id="tensor텐서">Tensor(텐서)</h1>

<p>Tensor는 <strong>모델의 파라미터와 입출력을 인코딩할 때 사용하는, <mark>배열 및 행렬</mark>과 매우 유사한 자료구조</strong>이다. list나 numpy array로부터 <strong>텐서를 생성</strong>할 수 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 원본 데이터
</span><span class="n">my_list</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]</span>
<span class="n">my_arr</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">my_list</span><span class="p">)</span>
 
<span class="c1"># 방법1. tensor() 함수
</span><span class="n">t1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">my_arr</span><span class="p">)</span>
 
<span class="c1"># 방법2. from_numpy() 함수
</span><span class="n">t2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">my_arr</span><span class="p">)</span>
 
<span class="c1"># 방법3. as_tensor() 함수
</span><span class="n">t3</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">as_tensor</span><span class="p">(</span><span class="n">my_arr</span><span class="p">)</span>
</code></pre></div></div>

<p>전달된 배열을 sharing 하는 <code class="language-plaintext highlighter-rouge">from_numpy()</code>, <code class="language-plaintext highlighter-rouge">as_tensor()</code> 과 다르게, <code class="language-plaintext highlighter-rouge">tensor()</code>은 배열을 <strong>복제(copy)</strong>한다. 따라서 원본 배열의 요소를 수정하여도 <strong><code class="language-plaintext highlighter-rouge">tensor()</code>로 변환된 텐서는 영향을 받지 않는다</strong>.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>my_arr[0, 0] = 999
 
print(t1)   # tensor([[1, 2, 3], [4, 5, 6]])
print(t2)   # tensor([[999, 2, 3], [4, 5, 6]])
print(t3)   # tensor([[999, 2, 3], [4, 5, 6]])
</code></pre></div></div>

<p>별도 인자를 주어 <strong>dtype을 오버라이딩</strong>할 수 있다. <code class="language-plaintext highlighter-rouge">t1.dtype</code>을 출력하여 비교할 수 있다. 단, <code class="language-plaintext highlighter-rouge">from_numpy</code>는 이런 기능이 없다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">my_list</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
</code></pre></div></div>

<p>텐서를 생성할 때 <strong>shape(size)를 지정</strong>할 수 있다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,)</span>
<span class="n">rand_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
<span class="n">ones_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
<span class="n">zeros_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>
<p><br /></p>

<p>✅ <strong>torch.Tensor vs. torch.tensor</strong><br />
torch.Tensor 는 <u>클래스(class)</u>다. 따라서 모든 텐서는 torch.Tensor 클래스의 객체이다. 반면 torch.tensor() 는 텐서를 반환하는 <u>함수</u>다.<br />
각각을 인자 없이 호출할 경우, 전자는 빈 텐서 객체로 생성되지만 후자는 변환할 배열 즉 data를 인자로 받지 못해 에러가 발생한다.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Tensor() # tensor([])
torch.tensor() # TypeError: tensor() missing 1 required positional arguments: "data"
</code></pre></div></div>

<hr />

<p>PyTorch의 텐서는 CPU에 생성되는 것이 디폴트지만 <strong>GPU에 올려 연산</strong>할 수 있다. GPU의 RAM에 저장된다는 의미다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">])</span>
<span class="n">t1</span><span class="p">.</span><span class="n">is_cuda</span>  <span class="c1">#False
</span></code></pre></div></div>

<p>텐서를 GPU에 올리는 두 가지 방법이 있다.</p>
<ol>
  <li>처음부터 GPU에서 텐서를 생성한다
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gpu_tensor = torch.tensor([[1, 2], [3, 4]], device='cuda')
</code></pre></div>    </div>
  </li>
  <li>CPU에서 생성된 텐서를 GPU로 복사한다. 한번 GPU에 복사된 텐서는 계속해서 GPU에서 연산된다.
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gpu_tensor = cpu_tensor.to(device='cuda')
</code></pre></div>    </div>
    <p>다시 CPU로 돌아가게 만드는 방법도 있다.</p>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>back_to_cpu = gpu_tensor.to(dvice='cpu')    # 모델을 GPU에 올릴 때에도 사용
back_to_cpu = gpu_tensor.cpu()
</code></pre></div>    </div>
    <p><br /></p>
  </li>
</ol>

<p>✅ <strong>CUDA Caching Memory Allocator</strong></p>
<blockquote>
  <p>파이토치는 tensor를 GPU에 올릴 때 <u>메모리를 빠르게 할당하기 위해</u> <strong>caching allocator</strong>를 사용한다. allocator는 CUDA로부터 메모리 블럭들을 요청한 후, 블럭을 쪼개고 (CUDA에 반환하지 않은 채로) 재사용한다. 따라서 tensor가 지워져도 allocator는 메모리를 keep해둔다. 결국에 <u>메모리가 실제로는 사용되고 있지 않더라도</u> 이 allocator에 의해 차지되어 <strong>사용 중인 것으로 표기</strong>될 수 있다.</p>
</blockquote>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 파이토치에서 실제로 사용하고 있는 메모리를 확인하기 (1024로 두 번 나누어주어야 MB 단위가 됨)
torch.cuda.memory_allocated()

# 실제로는 사용하지 않는 메모리를 반환시키기
torch.cuda.empty_cache() 
</code></pre></div></div>
<p><br /></p>

<p>✅ <strong>in-place operations</strong><br />
언더스코어(<code class="language-plaintext highlighter-rouge">_</code>)로 끝나는 연산 함수는 텐서 변수를 in-place로 변환한다. 메모리를 아끼는 데 도움이 될 수 있지만 derivative를 계산할 때 문제가 될 수 있으므로 사용을 권장하지는 않는다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,)</span>
<span class="n">t1</span><span class="p">.</span><span class="nf">add_</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div></div>
<p><br /></p>

<h1 id="autograd-자동미분">Autograd (자동미분)</h1>

<p>Autograd는 역전파(back propagation)를 시행하여 체인룰(chain rule)에 따라 말단 노드(leaf)까지 변화도(=gradient)를 연산하는 기능이다.</p>

<dl>
  <dt>Forward</dt>
  <dd>loss function의 값을 계산하는 것. 레이어의 output이 다음 레이어의 input으로 전달됨으로써 연산된다.</dd>
  <dt>Backward</dt>
  <dd>학습 파라미터의 gradient를 계산하는 것. chain rule를 기반으로, 파라미터가 loss fuction에 기여한 가중치를 연산한다.</dd>
</dl>

<p><br />
자동미분은 뿌리 노드가 계산되기까지 사용된 모든 변수의 미분값, 즉 ‘history’ 를 그래프 형태로 저장함으로써 동작한다. 이때 그래프는 방향이 있는 비순환 그래프(DAG, directed acyclic graph) 이며 학습 iteration 마다 새로 구성된다. <em>(c.f. tensorflow의 경우 Static Computational Graphs 를 사용한다)</em><br />
예를 들어 아래 연산 과정을 그래프로 나타낼 수 있다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>A = torch.tensor([10.], requires_grad=True)
B = torch.tensor([20.], requires_grad=True)
F = A * B
G = F * 2
</code></pre></div></div>
<p><img src="https://github.com/user-attachments/assets/4d2452a0-9563-455b-b9f4-edd06de5c8b1" alt="image" class="center-image" /></p>

<p>이렇듯 역전파는 그래프를 거꾸로 거슬러 올라가며 G가 계산되기까지 A, B의 gradient를 계산하는 과정이다.<br />
<br /></p>

<p>✅ <strong>Leaf Tensor &amp; gradient</strong></p>

<p>텐서는 두 가지 <u>조건</u>에 따라 분류할 수 있다.</p>
<ul>
  <li>그래디언트 계산이 필요한가? (requires gradient)</li>
  <li>연산의 결과인가? (explicitly created by the user, 즉 사용자가 명시적으로 생성한)</li>
</ul>

<blockquote>
  <p>“그래디언트 계산이 필요한가?” 는 결국 “상수(constant)인가 변수(variable)인가?”라는 질문과 같다</p>
</blockquote>

<p>이는 텐서의 두 가지 <u>속성</u>과 관련되는 것이다.</p>
<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">requires_grad</code></strong> : boolean으로 표현</li>
  <li><strong><code class="language-plaintext highlighter-rouge">grad_fn</code></strong> : 연산에 사용된 함수(<em>ex. Add, Mul, …</em>)</li>
</ul>

<blockquote>
  <p>유저가 생성했거나 (=연산의 결과가 아님) 그래디언트 계산이 필요하지 않으면, <code class="language-plaintext highlighter-rouge">grad_fn</code>는 값이 없다(None)</p>
</blockquote>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>user_created_tensor = torch.tensor([10.], requires_grad=True)   # 텐서를 초기화하면서 gradient 계산이 필요하다고 설정하였습니다
print(user_created_tensor.requires_grad)                        # True
print(user_created_tensor.grad_fn)                              # None
 
calcuated_tensor = user_created_tensor*2                        # 이 텐서는 곱셈 연산의 결과입니다
print(calcuated_tensor.requires_grad)                           # True
print(calcuated_tensor.grad_fn)                                 # &lt;MulBackward0 object at 0x7fc5d71a5df0&gt;
</code></pre></div></div>
<p><br /></p>

<p>이처럼 텐서는 조건에 따라 leaf tensor인지, gradient를 저장할지(populated) 결정된다.</p>
<blockquote>
  <p><strong>Grad Populated</strong> : 해당 텐서에 대한 gradient 저장. backward 후 grad 속성이 존재.</p>
</blockquote>

<style>
table {
  width: 100%;
  border-collapse: collapse;
}
th, td:first-child {
  background-color: #f2f2f2; /* Green color for header row */
}
</style>

<table>
  <thead>
    <tr>
      <th>case</th>
      <th><strong>requires_grad</strong></th>
      <th><strong>grad_fn</strong></th>
      <th><strong>is_leaf</strong></th>
      <th><strong>grad</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>gradient 계산이 필요하고, 유저가 생성하였다</td>
      <td>True</td>
      <td>None</td>
      <td><mark>True</mark></td>
      <td><mark>True</mark></td>
    </tr>
    <tr>
      <td>gradient 계산이 필요하고, 연산의 결과이다</td>
      <td>True</td>
      <td>not None</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>gradient 계산이 필요하지 않고, 유저가 생성하였다</td>
      <td>False</td>
      <td>None</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>gradient 계산이 필요하지 않고, 연산의 결과이다</td>
      <td>False</td>
      <td>None</td>
      <td>True</td>
      <td>False</td>
    </tr>
  </tbody>
</table>

<p><strong><u>leaf</u></strong>는 그래프 상 자식이 없는 말단 노드를 말한다. 파이토치에서 gradient 계산이 필요하지 않은 텐서는 모두 leaf tensor로 여긴다. 중요한 것은 <strong>‘gradient 계산이 필요하고 유저가 명시적으로 생성한 텐서’를 leaf tensor로 여긴다</strong>는 것이다.</p>

<p><strong><u>grad</u></strong>는 누적 계산된 gradient다. gradient 계산이 필요한 leaf tensor에 대해 누적 연산한 결과다. 이때 어떤 tensor가 gradient 계산이 필요하더라도 <u>연산의 결과</u>라면 non-leaf tensor로 여겨 <code class="language-plaintext highlighter-rouge">grad</code>가 저장되지 않는다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print(user_created_tensor.is_leaf)  # True
print(calcuated_tensor.is_leaf)     # False
 
# gradient 계산하여 grad 가 저장되는지 확인
calcuated_tensor.backward()
print(user_created_tensor.grad)     # tensor([2.])
print(calcuated_tensor.grad)        # None (UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed)
</code></pre></div></div>

<blockquote>
  <p>왜 이러한 개념들을 알아야 하는가? 자동 미분은 어떤 변수가 계산되는 데 사용된 모든 변수의 미분값, 즉 history를 computational graph 형태로 저장함으로써 동작하기 때문이다. <code class="language-plaintext highlighter-rouge">requires_grad=True</code> 옵션으로 표시해야만 처음 연산이 시작된 곳까지 거슬러 올라가며 그 변수의 연산들을 역추적한다.</p>
</blockquote>

<p>참고 // 텐서에 저장된 gradient 관련 속성을 확인하는 코드.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">check_attr</span><span class="p">(</span><span class="n">t</span> <span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">t</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">t</span><span class="p">.</span><span class="n">grad_fn</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">t</span><span class="p">.</span><span class="n">is_leaf</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">t</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div></div>
<p><br /></p>

<p>✅ <strong>requires_grad</strong></p>

<p><code class="language-plaintext highlighter-rouge">requires_grad</code>를 통해 어떤 텐서에 대해 gradient 계산이 필요한지 설정할 수 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 방법1. 변수 초기화할 때 인자로 표시하기
</span><span class="n">t1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
 
<span class="c1"># 방법2. 변수를 생성한 후에 속성 바꾸기
</span><span class="n">t2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">t2</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>
<span class="n">t2</span><span class="p">.</span><span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">requires_grad</code>는 <u>전염</u>된다. 연산에 사용된 텐서 중 하나라도 requires_grad 가 True로 설정되어 있다면, 그 연산 결과의 requires_grad도 True이다</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 두 텐서 중 하나만 requires_grad=True로 설정한다
</span><span class="n">t1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">t2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">t1</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>
 
<span class="c1"># 연산 결과가 requires_grad=True로 나타난다
</span><span class="n">t3</span> <span class="o">=</span> <span class="n">t1</span> <span class="o">@</span> <span class="n">t2</span>
<span class="nf">print</span><span class="p">(</span><span class="n">t3</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>     <span class="c1"># True
</span></code></pre></div></div>
<p>이러한 특성을 활용해 layer freezing과 같은 기법을 구사할 수 있다. 이미지는 <code class="language-plaintext highlighter-rouge">b</code> 의 <code class="language-plaintext highlighter-rouge">requires_grad</code>를 false로 세팅한 경우.
<img src="https://github.com/user-attachments/assets/8d0d81ef-61e6-488b-a0b2-1948feac4d07" alt="image" /></p>

<p>✅ <strong>torch.no_grad()</strong></p>

<p>모델로부터 값을 <strong>추론(inference)</strong>할 때는 그래프를 생성하고 gradient를 계산하는 과정이 필요없기 때문에 context manager를 활용해 메모리를 아낄 수 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="bp">...</span>
</code></pre></div></div>

<p><b><em>eval()과는 무슨 차이?</em></b><br />
<code class="language-plaintext highlighter-rouge">eval</code> 함수는 train과 evaluation 시 다르게 동작하는 layer들 (ex. Dropout, BatchNorm) 을 eval 모드로 바꿔준다. 따라서 모델을 평가할 때는 no_grad와 eval을 모두 사용하는 것이 옳다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
   <span class="p">...</span> 
</code></pre></div></div>
<p><br /></p>

<p>✅ <strong>backward()</strong></p>

<p>지금까지의 개념을 토대로 역전파 과정을 유사코드로 구현해본다. graph 를 거꾸로 거슬러 올라가며 recursive하게 반복하다가, leaf node에 다다르면 grad_fn가 None이기 때문에 중단된다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">gradients</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">self.Tensor : 역전파 연산의 대상이 되는 텐서(=loss)</span><span class="sh">'''</span>
    <span class="n">self</span><span class="p">.</span><span class="n">Tensor</span><span class="p">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">gradients</span>
     
    <span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">inputs</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">inp</span><span class="p">.</span><span class="n">grad_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">new_gradients</span> <span class="o">=</span> <span class="n">gradients</span> <span class="o">*</span> <span class="nf">local_grad</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">inp</span><span class="p">)</span>
            <span class="n">inp</span><span class="p">.</span><span class="n">grad_fn</span><span class="p">.</span><span class="nf">backward</span><span class="p">(</span><span class="n">new_gradients</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">pass</span>
</code></pre></div></div>

<p>한편 <code class="language-plaintext highlighter-rouge">backward()</code> 함수는 <strong>scalar 텐서</strong>에만 작동한다. vector 텐서에 부를 경우 이러한 에러를 마주하게 된다.</p>
<blockquote>
  <p>RuntimeError: grad can be implicitly created only for scalar outputs</p>
</blockquote>

<p>만약 벡터 텐서에 대해 역전파를 수행하고 싶다면 Jacobian Matix를 활용하거나, 사이즈에 맞는 1값 텐서를 입력으로 주어 처리할 수 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">G</span><span class="p">.</span><span class="nf">backward</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">G</span><span class="p">.</span><span class="n">shape</span><span class="p">))</span>
</code></pre></div></div>

<h2 id="예제">예제</h2>

<p>앞서 살펴본 연산식을 활용해 backward 함수를 호출하고 grad 속성을 확인해본다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">10.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">20.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">F</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span>
<span class="n">G</span> <span class="o">=</span> <span class="n">F</span> <span class="o">*</span> <span class="mi">2</span>
</code></pre></div></div>
<p><img src="https://github.com/user-attachments/assets/4d2452a0-9563-455b-b9f4-edd06de5c8b1" alt="image" class="center-image" /></p>

<table>
  <thead>
    <tr>
      <th>case</th>
      <th><strong>node</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>gradient 계산이 필요하고, 유저가 생성하였다</td>
      <td>A, B</td>
    </tr>
    <tr>
      <td>gradient 계산이 필요하고, 연산의 결과이다</td>
      <td>F, G</td>
    </tr>
    <tr>
      <td>gradient 계산이 필요하지 않고, 유저가 생성하였다</td>
      <td>-</td>
    </tr>
    <tr>
      <td>gradient 계산이 필요하지 않고, 연산의 결과이다</td>
      <td>-</td>
    </tr>
  </tbody>
</table>

<p>여기서 최적화하고자 하는 대상은 <strong>G</strong>이다. chain rule에 따라 backward 하여 G가 출력되기까지 a, b의 gradient를 계산한다.<br />
F 또한 requires_grad=True를 상속 받기 때문에 gradient를 계산하며 <code class="language-plaintext highlighter-rouge">grad_fn</code>을 확인할 수 있다. 하지만 leaf tensor가 아니기 때문에 grad를 저장하지 <u>않는다</u>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">G</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
 
<span class="c1"># a의 gradient(가중치)가 상대적으로 높게 계산됨
</span><span class="nf">print</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span> <span class="c1"># tensor([40.])
</span><span class="nf">print</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span> <span class="c1"># tensor([20.])
</span><span class="nf">print</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span> <span class="c1"># UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward()
</span><span class="nf">print</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="n">grad_fn</span><span class="p">)</span>    <span class="c1"># &lt;MulBackward0 object at 0x7f0cb7703250&gt;
</span></code></pre></div></div>

<p>만약 F.grad를 저장하고 싶다면 backward 전에 <code class="language-plaintext highlighter-rouge">retain_grad()</code>를 호출하여 중간 텐서 F에 대해서도 gradient를 유지하도록 할 수 있다.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>F.retain_grad()  
G.backward()     

print(F.grad)  # F의 gradient 출력
</code></pre></div></div>

<p><br /></p>

<p>✅ <strong>GPU 케이스</strong></p>

<p>앞선 예제와 동일한 연산을 수행하되, 이번에는 a와 b를 GPU에 할당하는 중간 단계를 거친다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">10.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">20.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># 추가
</span><span class="n">a_cuda</span> <span class="o">=</span> <span class="n">a</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">)</span>
<span class="n">b_cuda</span> <span class="o">=</span> <span class="n">b</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">)</span>
 
<span class="n">F</span> <span class="o">=</span> <span class="n">a_cuda</span> <span class="o">*</span> <span class="n">b_cuda</span>
<span class="n">G</span> <span class="o">=</span> <span class="n">F</span> <span class="o">*</span> <span class="mi">2</span>
</code></pre></div></div>

<p>backward 결과 CPU 위의 a, b에만 gradient가 저장되고, <strong>GPU 위의 a_cuda, b_cuda는 비어 있다</strong>. 이는 CPU 위의 텐서들이 최적화 변수로 남아있고, a와 b 로부터 생성된 a_cuda, b_cuda는 leaf tensor가 아닌 <strong>중간 텐서(intermediate tensor)</strong>로 여겨지기 때문이다. 텐서를 GPU로 옮기는 intermediate한 과정으로 인한 것이다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">G</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>       <span class="c1"># tensor([40.])
</span><span class="nf">print</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>       <span class="c1"># tensor([20.])
</span><span class="nf">print</span><span class="p">(</span><span class="n">a_cuda</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># None (UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed.)
</span><span class="nf">print</span><span class="p">(</span><span class="n">b_cuda</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># None (UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed.)
</span></code></pre></div></div>

<p>따라서 <strong><u>처음부터 GPU 위에서 변수를 생성한 후 역전파를 수행</u></strong>하자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a_cuda</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">10.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">)</span>
<span class="n">b_cuda</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">20.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">)</span>
 
<span class="n">F</span> <span class="o">=</span> <span class="n">a_cuda</span> <span class="o">*</span> <span class="n">b_cuda</span>
<span class="n">G</span> <span class="o">=</span> <span class="n">F</span> <span class="o">*</span> <span class="mi">2</span>
 
<span class="n">G</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
 
<span class="nf">print</span><span class="p">(</span><span class="n">a_cuda</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>      <span class="c1"># tensor([40.], device='cuda:0')
</span><span class="nf">print</span><span class="p">(</span><span class="n">b_cuda</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>      <span class="c1"># tensor([20.], device='cuda:0')
</span></code></pre></div></div>

<h1 id="학습">학습</h1>

<p>파이토치에서의 학습 과정을 예제를 통해 살펴본다. 전체 과정은 아래 단계로 이루어진다.</p>

<ol>
  <li>DataLoader 클래스 정의 및 객체 생성</li>
  <li>model 클래스 정의 및 객체 생성</li>
  <li>loss function, optimizer 정의</li>
  <li>epoch와 training loop</li>
</ol>

<p><br />
<strong>1. DataLoader 클래스 정의 및 객체 생성</strong></p>

<p>커스텀 DataLoader 클래스에는 <code class="language-plaintext highlighter-rouge">__init__</code>, <code class="language-plaintext highlighter-rouge">__len__</code>, <code class="language-plaintext highlighter-rouge">__getitem__</code> 함수를 정의해야 한다. 이 중 <code class="language-plaintext highlighter-rouge">__getitem__</code> 함수는 인자 <code class="language-plaintext highlighter-rouge">index</code>를 통해 데이터 샘플을 반환하는 역할을 한다.<br />
학습과 평가를 위한 DataLoader 객체를 각각 생성한다: train_dataloader, test_dataloader</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="k">class</span> <span class="nc">CustomDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sample</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">/path/to/data.csv</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">sample</span><span class="p">[[</span><span class="sh">'</span><span class="s">feature1</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">feature2</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">feature3</span><span class="sh">'</span><span class="p">]])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">sample</span><span class="p">[[</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">]])</span>
 
    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">sample</span><span class="p">)</span>
     
    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">x</span><span class="p">[</span><span class="n">index</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">y</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>
 
<span class="n">training_data</span> <span class="o">=</span> <span class="nc">CustomDataset</span><span class="p">()</span>
<span class="n">train_dataloader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">training_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>DataLoader 객체에서 데이터 샘플을 뽑아보고 싶다면 iter, next 내장함수를 사용한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_iterator</span> <span class="o">=</span> <span class="nf">iter</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span>
<span class="n">features</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="n">data_iterator</span><span class="p">)</span>
</code></pre></div></div>

<p><br />
<strong>2. model 클래스 정의 및 객체 생성</strong></p>

<p>파이토치의 <code class="language-plaintext highlighter-rouge">nn.Module</code>를 상속 받아 모델 클래스를 정의한다. <code class="language-plaintext highlighter-rouge">__init__</code> 함수에서 레이어를 초기화하고 <code class="language-plaintext highlighter-rouge">forward</code> 함수에서 순전파 구조를 정의한다.<br />
입력 데이터와 아웃풋의 사이즈를 잘 고려해야 한다. 연속된 레이어를 쌓아주는 <code class="language-plaintext highlighter-rouge">nn.Sequential</code>도 쓸 수 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
 
<span class="k">class</span> <span class="nc">CustomClassifier</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">CustomClassifier</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
 
        <span class="n">self</span><span class="p">.</span><span class="n">input_size</span> <span class="o">=</span> <span class="mi">3</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="mi">10</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_dim1</span> <span class="o">=</span> <span class="mi">64</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_dim2</span> <span class="o">=</span> <span class="mi">128</span>
 
        <span class="n">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">input_size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">hidden_dim1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">hidden_dim1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">hidden_dim2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">hidden_dim2</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">output_size</span><span class="p">)</span>

        <span class="c1"># 이렇게도 할 수 있다
</span>        <span class="c1"># self.net = nn.Sequential(
</span>        <span class="c1">#     nn.Linear(self.feature_num, self.hidden_dim1),
</span>        <span class="c1">#     nn.ReLU(),
</span>        <span class="c1">#     nn.Linear(self.hidden_dim1, self.hidden_dim2),
</span>        <span class="c1">#     nn.ReLU(),
</span>        <span class="c1">#    nn.Linear(self.hidden_dim2, self.output_size),
</span>        <span class="c1"># )
</span> 
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># 이것과 같다
</span>        <span class="c1"># output = net(x)
</span>
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div>

<p>모델 클래스를 생성하고 전반적인 정보를 확인한다. <code class="language-plaintext highlighter-rouge">torchsummary</code>를 활용하면 그냥 print하는 것보다 훨씬 보기 좋게 확인할 수 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">torchsummary</span> <span class="kn">import</span> <span class="n">summary</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">CustomClassifier</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">)</span>

<span class="n">batch_size</span><span class="p">,</span> <span class="n">feature_num</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">3</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span>  <span class="n">feature_num</span><span class="p">))</span>
</code></pre></div></div>

<p><img src="https://github.com/user-attachments/assets/bc05f826-bcb2-4f33-8da5-74ebd41b3012" alt="image" class="center-image" /></p>

<p>특정 레이어의 파라미터 값을 확인하고 싶다면 <code class="language-plaintext highlighter-rouge">named_parameters()</code> 혹은 <code class="language-plaintext highlighter-rouge">parameters()</code>를 사용한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># for param in model.parameters():
</span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">named_parameters</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">param</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>
</code></pre></div></div>

<p><br />
<strong>3. loss function, optimizer 정의</strong></p>

<p>라벨 수가 10인 다중분류를 상정하여 <code class="language-plaintext highlighter-rouge">CrossEntropyLoss</code>를 손실함수로 정의하고 임의의 입력값을 통해 loss를 연산한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
 
<span class="n">output_size</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">dummy_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
<span class="n">dummy_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,))</span>
 
<span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">dummy_outputs</span><span class="p">,</span> <span class="n">dummy_labels</span><span class="p">)</span>
</code></pre></div></div>

<p>옵티마이저(otpimizer)는 아주 다양한 종류가 있지만 이곳에서는 SGD를 예시로 든다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
</code></pre></div></div>

<p><br />
<strong>4. epoch와 training loop</strong></p>

<p><u>한 번</u>의 학습 epoch를 정의했다.</p>
<ol>
  <li>dataloader 객체에서 한 배치의 데이터를 꺼내며 gradient를 초기화한다(<code class="language-plaintext highlighter-rouge">zero_grad</code>)</li>
  <li>한 배치의 데이터를 모델에 통과시키고(<code class="language-plaintext highlighter-rouge">model</code>) 그 값으로 loss를 계산한다(<code class="language-plaintext highlighter-rouge">loss_fn</code>)</li>
  <li>loss에 대해 gradient를 계산한 후(<code class="language-plaintext highlighter-rouge">loss.backward</code>), 그 값을 기반으로 모델을 업데이트한다(<code class="language-plaintext highlighter-rouge">optimizer.step</code>)</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">training_loader</span><span class="p">:</span>
    <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
    
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
 
    <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
 
    <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>
</code></pre></div></div>

<h4 class="no_toc"> ✅ 동 떨어져 있는 optimizer.zero_grad(), loss.backward(), optimizer.step()이 각각 호출되는데 어떻게 모델이 업데이트되는가? </h4>

<p>optimizer 를 정의할 때 모델의 파라미터를 넘겨주기 때문에 - <code class="language-plaintext highlighter-rouge">torch.optim.SGD(model.parameters(), lr=0.001)</code><br />
내부적으로 grad를 저장하고 값을 업데이트한다. 달리 말해 optimizer가 활용하는 값은 loss도 아닌 <code class="language-plaintext highlighter-rouge">model.parameters()</code>의 <strong><code class="language-plaintext highlighter-rouge">param.grad</code></strong> 이다.</p>

<p>이때 모델을 GPU로 옮기고 난 <u>후</u>에 optimizer를 정의하는 것이 좋다.<br />
만약 모델을 GPU로 옮기기 전에 optimizer를 정의하면, optimizer는 CPU 위 파라미터를 추적하게 된다. 그 결과 optimizer가 추적하는 파라미터와 실제 모델의 파라미터가 달라지는 문제가 발생할 수 있다.</p>

<hr />

<p>비슷한 방식으로 validation 을 정의할 수 있다. 단 모델 평가 단계이므로 모델을 <code class="language-plaintext highlighter-rouge">eval</code> 모드로 바꾸고 <code class="language-plaintext highlighter-rouge">torch.no_grad</code> 안에서 추론이 이루어진다.<br />
또한 “loss에 대해 gradient를 계산한 후 그 값을 기반으로 모델을 업데이트”하는 과정은 생략된다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>

<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">test_dataloader</span><span class="p">:</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

        <span class="n">test_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>
</code></pre></div></div>

<p>전자를 <code class="language-plaintext highlighter-rouge">train_loop</code>, 후자를 <code class="language-plaintext highlighter-rouge">test_loop</code> 이라는 함수로 정의한다면 전체 학습 loop는 이렇게 정의할 수 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="nf">train_loop</span><span class="p">()</span>
    
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nf">test_loop</span><span class="p">()</span>
</code></pre></div></div>

    </section>
  </div>

  <div class="pagination">
    
      <span class="prev" >
          <a href="http://woocosmos.github.io/websocket-base/">
            &#xE000; websocket: 클라이언트-서버의 양방향 통신
          </a>
      </span>
    
    
      <span class="next" >
          <a href="http://woocosmos.github.io/pytorch-categorical/">
            PyTorch의 확률분포 클래스 Categorical &#xE001;
          </a>
      </span>
    
  </div>

  <div class="comments">
	<div id="disqus_thread"></div>
	<script type="text/javascript">

	    var disqus_shortname = 'woocosmos';

	    (function() {
	        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
			dsq.setAttribute('data-timestamp', +new Date()); //추가
	        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	    })();

	</script>
	<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  <script type="text/javascript">
    "use strict"; 
    function addBackToTop() 
        { function o() 
            { 
                t() >= z ? R && (D.className = "", R = !1) : R || (D.className = "hidden", R = !0) 
            } 
          function t() 
            { 
                return E.scrollTop || A && document.documentElement.scrollTop || 0 
            } 
          function e(o) 
            { 
                E.scrollTop = o, A && (document.documentElement.scrollTop = o) 
            } 
    
    var n, i, d, r, c = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : {}, a = c.backgroundColor, s = void 0 === a ? "#000" : a, l = c.cornerOffset, u = void 0 === l ? 20 : l, m = c.diameter, p = void 0 === m ? 56 : m, h = c.ease, b = void 0 === h ? function (o) { return .5 * (1 - Math.cos(Math.PI * o)) } : h, v = c.id, f = void 0 === v ? "back-to-top" : v, x = c.innerHTML, g = void 0 === x ? '<svg viewBox="0 0 24 24"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path></svg>' : x, w = c.onClickScrollTo, y = void 0 === w ? 0 : w, k = c.scrollContainer, E = void 0 === k ? document.body : k, T = c.scrollDuration, C = void 0 === T ? 100 : T, M = c.showWhenScrollTopIs, z = void 0 === M ? 1 : M, L = c.size, B = void 0 === L ? p : L, H = c.textColor, I = void 0 === H ? "#fff" : H, N = c.zIndex, S = void 0 === N ? 1 : N, q = E === document.body, A = q && document.documentElement; n = Math.round(.35 * B), i = Math.round(.2 * B), d = "#" + f + "{background:" + s + ";-webkit-border-radius:50%;-moz-border-radius:50%;border-radius:50%;bottom:" + u + "px;-webkit-box-shadow:0 2px 5px 0 rgba(0,0,0,.26);-moz-box-shadow:0 2px 5px 0 rgba(0,0,0,.26);box-shadow:0 2px 5px 0 rgba(0,0,0,.26);color:" + I + ";cursor:pointer;display:block;height:" + B + "px;opacity:1;outline:0;position:fixed;right:" + u + "px;-webkit-tap-highlight-color:transparent;-webkit-touch-callout:none;-webkit-transition:bottom .2s,opacity .2s;-o-transition:bottom .2s,opacity .2s;-moz-transition:bottom .2s,opacity .2s;transition:bottom .2s,opacity .2s;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;width:" + B + "px;z-index:" + S + "}#" + f + " svg{display:block;fill:currentColor;height:" + n + "px;margin:" + i + "px auto 0;width:" + n + "px}#" + f + ".hidden{bottom:-" + B + "px;opacity:0}", (r = document.createElement("style")).appendChild(document.createTextNode(d)), document.head.insertAdjacentElement("afterbegin", r); var D = function () { var o = document.createElement("div"); return o.id = f, o.className = "hidden", o.innerHTML = g, o.addEventListener("click", function (o) { o.preventDefault(), function () { var o = "function" == typeof y ? y() : y, n = window, i = n.performance, d = n.requestAnimationFrame; if (C <= 0 || void 0 === i || void 0 === d) return e(o); var r = i.now(), c = t(), a = c - o; d(function o(t) { var n = Math.min((t - r) / C, 1); e(c - Math.round(b(n) * a)), n < 1 && d(o) }) }() }), document.body.appendChild(o), o }(), R = !0; (q ? window : E).addEventListener("scroll", o), o() } window.addEventListener("load", function () { var o = document.getElementById("back-to-top"), t = document.querySelector(".sticky-bottom"), e = document.querySelector(".fixed-bottom"); o && window.addEventListener("scroll", function () { if (t) { var n = t.getBoundingClientRect(); window.scrollY + window.innerHeight > document.body.scrollHeight - n.height ? (o.style.position = "fixed", o.style.bottom = n.height + 15 + "px") : (o.style.position = "fixed", o.style.bottom = "15px") } if (e) { var i = e.getBoundingClientRect(); o.style.position = "fixed", o.style.bottom = i.height + 15 + "px" } }) });
</script>
<script>
    addBackToTop();
</script>




</article>

    </div>
    
    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
          <!-- Refer to https://codepen.io/ruandre/pen/howFi -->
<ul class="svg-icon">

  

  

  
  <li><a href="mailto:woocosmos@gmail.com" class="icon-8 email" title="Email"><svg viewBox="0 0 512 512"><path d="M101.3 141.6v228.9h0.3 308.4 0.8V141.6H101.3zM375.7 167.8l-119.7 91.5 -119.6-91.5H375.7zM127.6 194.1l64.1 49.1 -64.1 64.1V194.1zM127.8 344.2l84.9-84.9 43.2 33.1 43-32.9 84.7 84.7L127.8 344.2 127.8 344.2zM384.4 307.8l-64.4-64.4 64.4-49.3V307.8z"/></svg><!--[if lt IE 9]><em>Email</em><![endif]--></a></li>
  

  

  

  
  <li><a href="https://github.com/woocosmos" class="icon-13 github" title="GitHub"><svg viewBox="0 0 512 512"><path d="M256 70.7c-102.6 0-185.9 83.2-185.9 185.9 0 82.1 53.3 151.8 127.1 176.4 9.3 1.7 12.3-4 12.3-8.9V389.4c-51.7 11.3-62.5-21.9-62.5-21.9 -8.4-21.5-20.6-27.2-20.6-27.2 -16.9-11.5 1.3-11.3 1.3-11.3 18.7 1.3 28.5 19.2 28.5 19.2 16.6 28.4 43.5 20.2 54.1 15.4 1.7-12 6.5-20.2 11.8-24.9 -41.3-4.7-84.7-20.6-84.7-91.9 0-20.3 7.3-36.9 19.2-49.9 -1.9-4.7-8.3-23.6 1.8-49.2 0 0 15.6-5 51.1 19.1 14.8-4.1 30.7-6.2 46.5-6.3 15.8 0.1 31.7 2.1 46.6 6.3 35.5-24 51.1-19.1 51.1-19.1 10.1 25.6 3.8 44.5 1.8 49.2 11.9 13 19.1 29.6 19.1 49.9 0 71.4-43.5 87.1-84.9 91.7 6.7 5.8 12.8 17.1 12.8 34.4 0 24.9 0 44.9 0 51 0 4.9 3 10.7 12.4 8.9 73.8-24.6 127-94.3 127-176.4C441.9 153.9 358.6 70.7 256 70.7z"/></svg><!--[if lt IE 9]><em>GitHub</em><![endif]--></a></li>
  

  

  

  
  <li><a href="https://www.linkedin.com/in/yunsoo-woo-245946213" class="icon-17 linkedin" title="LinkedIn"><svg viewBox="0 0 512 512"><path d="M186.4 142.4c0 19-15.3 34.5-34.2 34.5 -18.9 0-34.2-15.4-34.2-34.5 0-19 15.3-34.5 34.2-34.5C171.1 107.9 186.4 123.4 186.4 142.4zM181.4 201.3h-57.8V388.1h57.8V201.3zM273.8 201.3h-55.4V388.1h55.4c0 0 0-69.3 0-98 0-26.3 12.1-41.9 35.2-41.9 21.3 0 31.5 15 31.5 41.9 0 26.9 0 98 0 98h57.5c0 0 0-68.2 0-118.3 0-50-28.3-74.2-68-74.2 -39.6 0-56.3 30.9-56.3 30.9v-25.2H273.8z"/></svg><!--[if lt IE 9]><em>LinkedIn</em><![endif]--></a></li>
  

  

  

  

  

  

  

</ul>


<p>© 2024 Yunsoo Woo</p>

        </footer>
      </div>
    </div>

    <!--  -->

<script async src="https://www.googletagmanager.com/gtag/js?id=G-1YCJCQRD4F"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-1YCJCQRD4F');
</script>
    <script type="text/javascript">
    "use strict"; 
    function addBackToTop() 
        { function o() 
            { 
                t() >= z ? R && (D.className = "", R = !1) : R || (D.className = "hidden", R = !0) 
            } 
          function t() 
            { 
                return E.scrollTop || A && document.documentElement.scrollTop || 0 
            } 
          function e(o) 
            { 
                E.scrollTop = o, A && (document.documentElement.scrollTop = o) 
            } 
    
    var n, i, d, r, c = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : {}, a = c.backgroundColor, s = void 0 === a ? "#000" : a, l = c.cornerOffset, u = void 0 === l ? 20 : l, m = c.diameter, p = void 0 === m ? 56 : m, h = c.ease, b = void 0 === h ? function (o) { return .5 * (1 - Math.cos(Math.PI * o)) } : h, v = c.id, f = void 0 === v ? "back-to-top" : v, x = c.innerHTML, g = void 0 === x ? '<svg viewBox="0 0 24 24"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path></svg>' : x, w = c.onClickScrollTo, y = void 0 === w ? 0 : w, k = c.scrollContainer, E = void 0 === k ? document.body : k, T = c.scrollDuration, C = void 0 === T ? 100 : T, M = c.showWhenScrollTopIs, z = void 0 === M ? 1 : M, L = c.size, B = void 0 === L ? p : L, H = c.textColor, I = void 0 === H ? "#fff" : H, N = c.zIndex, S = void 0 === N ? 1 : N, q = E === document.body, A = q && document.documentElement; n = Math.round(.35 * B), i = Math.round(.2 * B), d = "#" + f + "{background:" + s + ";-webkit-border-radius:50%;-moz-border-radius:50%;border-radius:50%;bottom:" + u + "px;-webkit-box-shadow:0 2px 5px 0 rgba(0,0,0,.26);-moz-box-shadow:0 2px 5px 0 rgba(0,0,0,.26);box-shadow:0 2px 5px 0 rgba(0,0,0,.26);color:" + I + ";cursor:pointer;display:block;height:" + B + "px;opacity:1;outline:0;position:fixed;right:" + u + "px;-webkit-tap-highlight-color:transparent;-webkit-touch-callout:none;-webkit-transition:bottom .2s,opacity .2s;-o-transition:bottom .2s,opacity .2s;-moz-transition:bottom .2s,opacity .2s;transition:bottom .2s,opacity .2s;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;width:" + B + "px;z-index:" + S + "}#" + f + " svg{display:block;fill:currentColor;height:" + n + "px;margin:" + i + "px auto 0;width:" + n + "px}#" + f + ".hidden{bottom:-" + B + "px;opacity:0}", (r = document.createElement("style")).appendChild(document.createTextNode(d)), document.head.insertAdjacentElement("afterbegin", r); var D = function () { var o = document.createElement("div"); return o.id = f, o.className = "hidden", o.innerHTML = g, o.addEventListener("click", function (o) { o.preventDefault(), function () { var o = "function" == typeof y ? y() : y, n = window, i = n.performance, d = n.requestAnimationFrame; if (C <= 0 || void 0 === i || void 0 === d) return e(o); var r = i.now(), c = t(), a = c - o; d(function o(t) { var n = Math.min((t - r) / C, 1); e(c - Math.round(b(n) * a)), n < 1 && d(o) }) }() }), document.body.appendChild(o), o }(), R = !0; (q ? window : E).addEventListener("scroll", o), o() } window.addEventListener("load", function () { var o = document.getElementById("back-to-top"), t = document.querySelector(".sticky-bottom"), e = document.querySelector(".fixed-bottom"); o && window.addEventListener("scroll", function () { if (t) { var n = t.getBoundingClientRect(); window.scrollY + window.innerHeight > document.body.scrollHeight - n.height ? (o.style.position = "fixed", o.style.bottom = n.height + 15 + "px") : (o.style.position = "fixed", o.style.bottom = "15px") } if (e) { var i = e.getBoundingClientRect(); o.style.position = "fixed", o.style.bottom = i.height + 15 + "px" } }) });
</script>
<script>
    addBackToTop();
</script>





  </body>
</html>
