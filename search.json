[
  
    {
      "title"    : "Windows 옵시디언 앱에서 git username/password 를 계속 요구하는 문제",
      "tags"     : "생산성",
      "date"     : "2026-02-19",
      "url"  : "https://woocosmos.github.io/obsidian-git-auth/",
      "content": "  WSL이 아니라 Windows PowerShell/CMD 에서 한 번 자격 증명을 해줘야 된다!문제 상황  Windows 옵시디언 앱에서 Obsidian-git 플러그인을 활성화 했으며, 터미널 환경은 WSL 을 사용하고 있었다  SSH 방식의 인증은 permisson denied 에러가 계속 발생하였다 (터미널에서는 되는데, 옵시디언 앱에서 안 되는 현상)  HTTP 방식의 인증은 계속해서 username/password 를 요구하였다 (터미널에서는 git config --global user.name ...   요 방식으로 해결됨)해결 방법  Windows PowerShell 이나 CMD 에서 Git Credential Manager 로 로그인해야 obsidian-git 이 해당 자격증명을 사용한다# 출력되는 것 확인git config --global --get credential.helper # 아무것도 없거나, manager 가 아니라면 아래 실행git config --global credential.helper manager# obsidian 경로에서 로그인 (Github 로그인 창)git fetch"
    } ,
  
    {
      "title"    : "옵시디언(Obsidian)을 세팅해보자 - git 활용한 아이폰 동기화까지",
      "tags"     : "생산성",
      "date"     : "2026-02-16",
      "url"  : "https://woocosmos.github.io/obsidian/",
      "content": "요약  - 옵시디언을 설치하고 vault, 노트 링크, 그래프 뷰 등 기본 기능을 살펴본다  - Git 플러그인으로 데스크탑 동기화를 설정하고, aShell을 활용해 아이폰에서도 SSH 기반 git 동기화를 구현한다  - iOS 단축어를 연동하여 옵시디언 앱의 열기/닫기 시 자동 pull/push가 되도록 설정한다개요나만의 지식 데이터베이스를 구축하기 위해 옵시디언을 세팅하려 한다.최근 공식 CLI를 지원한다고 하여 다른 메모 앱보다 에이전트에 친화적이라는 생각이 들기 때문이다.기존에 사용하던 어플리케이션의 불편함을 짚어보자면,            앱      문제점                  노션      무겁고 군더더기가 너무 많음              iOS 메모앱      포맷팅이 불편하고 너무 가벼워 메모가 휘발적임      (가만 생각해보니 이제껏 나만의 지식 데이터베이스의 역할을 해주던 건 “카카오톡 나에게 보내기” 기능이었다)시작Obsidian 공홈에서 MacOS 앱을 받아 설치했다. 오른편의 Graph View가 눈에 띈다.vault는 md 파일들이 저장되는 root 폴더를 가리킨다.link는 내외부 리소스와 연결해주는 방식이고, importer는 외부의 리소스를 마이그레이션해올 수 있는 플러그인이다.새 노트를 생성한 다음 내용을 작성해보았다.이때 특정 키워드를 [[이런 식]]으로 감싸주면, 그 키워드를 제목으로 하는 새 노트를 작성할 수 있는 링크가 생성된다.이렇게 노트끼리 서로 참조할 수 있다.그래프 뷰를 통해 노트 간 관계를 확인할 수 있다. 또 #태그 형식으로 태그 기능을 사용할 수 있다.추가로 비교적 최근에 공식 도입된 base 기능은 노션의 데이터베이스와 유사하게 사용할 수 있다고 하는데 …슥 둘러 봤을 땐 index 기능에 더 가까운 것 같다.아직은 입문 단계니까 base 는 패스.git 연동Obsidian Sync 는 유료 기능이기 때문에, git을 활용하여 무료로 데이터를 동기화할 수 있다.      github 에 private repository 를 생성한다.        옵시디언의 설정 &gt; Community Plugins &gt; Git 설치한다.    vault 가 위치한 경로로 가서 git init 해준다.          왼쪽 하단 vault 이름을 클릭하면 manage vaults 가 뜨는데, 거기서 로컬 파일 시스템 내 어디에 위치하는지 경로가 보인다.      나의 경우, /Users/&lt;내 이름&gt;/Documents 에 있었음            git 플러그인 설정에서 자동 sync 모드를 켜준다.          옵시디언 화면에서 git commit/push 등 동작을 버튼으로 수행할 수 있게 된다.      참고로 passphrase 를 입력하라는 창이 계속 떴었는데, vault 폴더에서 .git/config 의 url 정보를 아래와 같이 변경하니 바로 push 할 수 있게 되었다.# 수정 전[remote \"origin\"] url = git@github.com:&lt;계정명&gt;/&lt;레포 이름&gt;.git# 수정 후[remote \"origin\"] url = https://github.com/&lt;계정명&gt;/&lt;레포 이름&gt;.git처음에 ssh 인증 방식을 사용했기 때문에 키에 대한 비밀번호를 요구했던 것 같다.아이폰에서의 연동이전에는 obsidian-git 플러그인이 iOS 를 지원하지 않아,working copy 등 다른 git 클라이언트 앱을 통해 동기화를 해야 했던 것으로 보인다.2026년 2월 현재는 어떨까.obsidian-git 공식 리드미에 따르면 모바일 환경에서는 메모리 이슈 등 동작이 불안정하다는 것을 경고하고 있다. (그냥 안 된다고 봐야 할 듯)선택지는 두 개 정도로 좁혀진다.  Gitsync (repo 한 개까지 무료라는 얘길 봄) 같은 git 클라앱을 사용하거나,  iSH 같은 터미널 앱을 깔아 git 명령어를 입력하는 것.일단 자동 push 기능은 아이폰의 단축어로 구현해야 할 텐데, iSH 와 단축어 기능을 연동하기 어렵다는 논의가 보인다. - Can we run Shortcuts by a command ?반면 aShell 은 될 것 같은 느낌.우선 … aShell 로 동기화 도전!참고한 포럼 - [Mobile] Automatic sync with GitHub on iOS (for free) via a-shell1. 앱 설치(1) 앱스토어에서 Obsidian 앱을 설치한다 - create a new vault 로 진행 (이름은 Temp)(2) 앱스토에서 aShell mini 앱을 설치한다(3) aShell 에서 pickFolder 명령어 입력 &gt; 내 아이폰에서 Obsidian 폴더를 찾아 선택해준다    1번 과정에서 설정한 Temp 가 보이는 상태2. SSH 키 등록aShell 에서 SSH 키를 생성ssh-keygen -t ed25519 -C \"&lt;이메일&gt;@gmail.com\"~home 으로 가서 .ssh/ 위치의 id_ed25519.pub 를 출력한다head id_ed25519.pub출력된 내용을 복사해서 github 의 ssh 관리탭에 키를 추가해준다.3. github 동기화다시 아까의 옵시디언 폴더로 돌아간다.앞서 만든 vault 폴더명을 ios-vault 로 이름을 변경해준다음 cd ios-vault 했다.lg2 init .lg2 remote add origin git@github.com:USERNAME/REPO-NAME.gitlg2 pull originhost/key pair 를 등록할 거냐는 애스크가 뜰 텐데 y 를 입력하면 된다.git 으로부터 데이터가 불러와졌다!!마지막으로 세팅. 출력된 내용 중에서 Consider running 다음으로 나온 명령어들을 붙여 넣고 실행한다.lg2 config user.name \"계정명\"lg2 config user.email \"이메일 주소\"이렇게 git 사용을 위한 user 정보 설정까지 마친다.4. push 테스트아이폰에서 수정한 내역을 push 하고, 맥 데스크탑앱에서 pull 하는 간단한 실험을 진행한다.(브랜치명이 master 로 되어 있어서 중간에 엄청 삽질하고, 갑자기 not a valid reference ‘main’ 같은 에러가 뜨면서 push가 안 되기도 했음)(1) 아이폰 obsidian 앱에서 수정(2) (제발 브랜치명 확인하기)lg2 checkout main(3) lg2 status 확인 후,lg2 add . &amp; lg2 commit -m 'init commit from iPhone'(4) push 한다lg2 push origin main여기서 not a valid reference ‘main’ 에러가 뜨기도 했는데 lg2 push origin refs/heads/main 이렇게 명시했을 때 성공(5) 웹에서 push 됐는지 확인하고, 데스크탑에서 pull 버튼을 눌러주면 …성공 ㅠㅠ5. 단축어 설정단축어 설정을 하려면 jump 명령어로 obsidian 폴더에 접근할 수 있어야 한다.showmarks 를 입력해서 출력 결과를 보면 북마크명이 obsidian 이 아니라 Documents 라는 이름으로 되어 있는데, 이걸 이렇게 바꿔준다.renamemark Documents obsidian이제 jump obsidian 명령어로 폴더로 이동할 수 있다. (iOS의 aShell은 bookmark 기반 접근을 이용하기 때문이라고 함)자동 pull(1) 아이폰의 단축어 앱에서 ‘자동화’ 탭으로 간다(2) 새로운 개인용 자동화를 선택하여 “앱” 을 클릭한다  Obsidian 앱이 열릴 때를 조건으로 한다(3) 동작에서 새로운 단축어를 생성, a-shell mini 를 선택해 “명령 실행” 을 선택한다(4) 셸 명령어는 차례대로 아래와 같이 설정jump obsidiancd &lt;vault 이름&gt;lg2 pull origin(5) obsidian 앱을 열 때마다 단축어 실행 팝업이 뜬다자동 push(1) 똑같이 아이폰 단축어에서 자동화를 추가하여 Obsidian 이 닫힐 때를 조건으로 한다(2) 동작에서 새로운 단축어를 생성, 이번에는 commit 일시를 기록하기 위해 변수를 설정해준다  스크립트 하기 &gt; 변수 설정 &gt; 현재 날짜(3) a-shell mini 를 선택해 “명령 실행” 으로 아래와 같이 설정jump obsidiancd &lt;vault 이름&gt;lg2 add .lg2 commit -m \"commit from iPhone &lt;시간 변수&gt;\"lg2 push origin refs/heads/main실제 commit 메세지도 잘 설정된 것을 확인하였다.정리오늘 obsidian 을 처음 입문하면서, 켠 김에 모바일 자동 동기화까지 완료했다.obsidian 유료 서비스 쓰기 싫다고 돌고 돌아 온 느낌이 있지만 ㅋㅋㅋ이 과정에서 상당히 많이 배울 수 있었다. 특히 폰으로 리눅스 만져보는 건 완전히 새로운 경험!그 다음으로는 obsidian 데이터베이스를 RAG 으로 긁어와 LLM 개인비서가 참고할 수 있게 하는 시스템을 만들어 보고 싶은데.아니면 LLM 이 자동으로 금융 뉴스 스크랩핑 해서 obsidian 에 데이터 쌓고 참조하게 만드는 것도 좋겠다.하여튼, 그 프로젝트를 실행하기 전까지 obsidian과 친해지는 시간을 충분히 가져야겠다."
    } ,
  
    {
      "title"    : "[라즈베리 파이를 활용한 홈서버 구축] 01. 킥오프",
      "tags"     : "홈서버",
      "date"     : "2024-12-22",
      "url"  : "https://woocosmos.github.io/raspberry-pi-01/",
      "content": "개요라즈베리 파이(Raspberry Pi)를 활용한 홈서버 구축 프로젝트를 시작하려 한다. 이번 포스트에서는 프로젝트의 목적과 목표를 명확히 하고, 그에 맞춰 필요한 제품 스펙을 결정하는 과정을 기록한다.계기  며칠 전, 회사 워크숍에서 버킷리스트를 작성하는 시간을 가졌다. 다들 세계여행이나 내 집 마련 같은 것을 적었는데, 나는 홈서버 구축하기를 쓰고 있었다.그러고 보니 인생 버킷리스트까지도 아니고 그냥 당장 실행하면 되지 않을까? 라는 생각이 들었다.작은 자취방에서 가지고 놀 수 있는 작은 컴퓨터를 갖고 싶다는 생각에서 시작했다. 운영체제는 리눅스로 해서 CLI 개발 환경의 고수가 되고 싶었다. VSCode 대신에 Vim 으로 코드를 작성하는 지독한 컨셉의 개발자가 되고 싶었다. (이건 농담이다)지금까지 지나온 개발 환경을 순서대로 짚어보았다.  구글 코랩 : 밑바닥부터 시작하는 데 이만한 도구가 있을까. 버튼 클릭 하나로 구글 데이터센터의 고성능 GPU 를 사용할 수 있었다.  클라우드 서버 : 프로젝트가 본격화되면서 코랩의 사용량 제한이 걸림돌으로 작용했다. 이에 AWS EC2 스팟 인스턴스를 하나 구입해서 사용하기 시작했다.  온프레미스 서버 : 국가 사업에 선정되면서 그래픽 카드를 마련할 수 있게 되었다. 사무실 한 켠에서 열과 소음을 뿜어내던 그 녀석을 SSH 로 접근해서 마음대로 다뤄볼 수 있었다.  (현재) 로컬 환경 : 이직하고 보니 컴퓨터마다 좋은 그래픽 카드가 하나씩 꽂혀 있었다. (스타트업에 있다 와서 그런지 충격적이었다) 물론 서버 인스턴스만큼은 넉넉하지 않지만, 적당한 크기의 모델 학습이나 추론은 로컬 환경에서도 거뜬했다.특히 LLM 모델이 거대화되고 API 로 접근하는 방식이 널리 퍼지면서 오히려 로컬 환경에서 개발하기 편리해졌다고 느낀다. 그러나 반대 급부로 그만한 사이즈의 모델을 튜닝하거나 다루기 위해서는 서버 환경에서의 개발 역량 또한 놓칠 수 없다고 생각한다.아쉽지만 현재 회사에서는 프로젝트 상 로컬 환경을 활용하는 것이 최선이므로, 개인적으로 홈서버를 구축하고 스스로 연습하려는 것이다. 먼 이야기지만 나중에 회사에서 서버 인스턴스를 할당 받았을 때 실수나 사고 없이 안정적으로 해내는 모습을 보여드리고 싶기도 하고.무엇보다도 전자 회로 기판을 조립해서 만든 작은 컴퓨터는 나만의 작은 공간을 창조한다는 감동을 준달까. 기계와 로우레벨의 교감을 주고 받는 것이다…목표홈서버를 구축하고 난 뒤에 구체적으로 어떤 것들을 해볼 수 있을지 목표를 정리해보았다. 목표는 제품의 스펙을 결정하는 기준이 될 것이다. (나는 보통 목표를 질문의 형식으로 작성하는데, 그 질문들에 답변할 수 있을 때 목표를 이루었다고 여기기 때문이다)  리눅스 운영체제는 어떻게 동작하는 것일까?  파이썬과 운영체제는 어떻게 상호작용하는 것일까?  데이터를 최대한 효율적으로 처리하는 방법은 무엇일까?  머신러닝 모델을 경량화하는 방법은 무엇일까?  머신러닝 모델의 추론과 운영을 어떻게 컨테이너화할까?  왜 C/C++ 개발 역량을 지닌 머신러닝 엔지니어가 우대 받는 걸까?로우레벨 환경에서 운영체제와 파이썬 언어의 동작을 이해하는 것과 매우 한정된 성능 안에서 데이터 처리를 최적화하는 것으로 요약할 수 있겠다.절대적 우선순위제품 스펙을 살펴보기 앞서 현실적으로 고려해야 할 두 가지를 먼저 언급하려 한다. 바로 가격과 사이즈다.실습, 장난감 용도이기 때문에 큰 돈을 들일 생각이 없다. 가성비 전략으로 간다. 중고 물품도 적극 고려할 예정.또한 활동 공간이 3평 남짓한 자취방에서 감당할 수 있어야 하므로 사이즈는 최대한 미니미니하게 해야 한다.구성 요소오늘날 컴퓨터는 폰 노이만 구조를 뼈대로 하여 확장된 형태이다. 이번 프로젝트에서 어떤 구성품을 마련할지 고민하면서 이 폰 노이만 컴퓨터를 참고할 수 있었다.폰 노이만 구조의 구성 요소는 CPU, 메모리, 프로그램이다. 이때 프로그램은 메모리 안에 저장되고 실행되는 명령어의 집합이다.그러니까 하드웨어 관점에서 필수 요소는 처리 장치(CPU), 기억 장치(메모리), 그리고 이들을 운영하고 제어하기 위한 입출력 장치다.CPU 와 메모리싱글보드 컴퓨터는 메인보드(Motherboard)에 처리 장치인 CPU와 주기억 장치인 RAM이 탑재되어 있는 것을 말한다. 라즈베리 파이가 대표적인 싱글보드 컴퓨터 제품.사실 엔비디아에도 싱글보드 컴퓨터 라인업이 있으며 GPU 활용이 필요하다면 이쪽이 더 나을 수도 있다.야속하게도 불과 5일 전 엔비디아에서 AI용 싱글보드 컴퓨터 개발자 키트를 매우 합리적인 가격에 출시한다는 소식을 내놨다. 젯슨 오린 나노(Jetson ORIN™ Nano) 라는 제품이고 249달러(한화 약 36만원)다. LLM 등 딥러닝 모델 처리에 특화된 제품이라고 한다.  너무나도 갖고 싶다…하지만 나의 절대적인 기준은 가격이기 때문에 욕심을 내려놓으려 한다. 젯슨 시리즈는 나중에 더 큰 집으로 이사 가면 모셔보겠다 🥲라즈베리 파이도 여러 제품이 있다. 몇 세대를 살지, 램 사이즈는 어느 정도로 할지 결정해야 한다.몇 세대를 살 것인가? ➡️ 4 세대주로 4세대와 5세대를 많이 비교하는 것 같다.2019년 출시된 4세대는 성능과 가격 측면에서 경쟁력이 있어 독보적으로 인기 있는 제품이라고 한다. 5세대는 비교적 최근인 2023년에 출시되었는데, 이전 세대에 비해 가격도 오른 데다 발열이나 어댑터 등 이슈가 있어 차라리 미니 PC를 사는 편이 낫다고 보는 견해가 많다. 나도 5세대 제품 가격 보고 ‘그냥 젯슨을 살까 …‘라고 고민했는데, 그럴 바에 4세대를 선택하겠다고 판단했다.램 사이즈는? ➡️ 4GB1, 2, 4, 8GB 옵션이 있다. 모델 올리는 것을 생각하면 4GB 이상은 되어야 할 텐데, 4GB는 7만원대이고 8GB는 11만원대로 가격 차이가 난다. 성능보다 가격이 중요한 상황이므로 전자로 결정했다.상세 스펙구체적인 스펙은 아래와 같다. 공식 사이트를 참고했다.            카테고리      내용                  프로세서      Broadcom BCM2711, 쿼드코어 Cortex-A72 (ARM v8) 64-bit SoC @ 1.8GHz              메모리(RAM)      8GB              무선 연결      2.4 GHz/5.0GHz 와이파이, Bluetooth 5.0, BLE              유선 연결(이더넷)      기가비트 이더넷              USB 포트      USB 3.0 포트 2개, USB 2.0 포트 2개              GPIO      라즈베리 파이 표준 40핀 GPIO 헤더              디스플레이 출력      micro-HDMI 포트 2개 (최대 4k60 지원)              디스플레이 인터페이스      2-레인 MIPI DSI 디스플레이 포트              그래픽      OpenGL ES 3.1, Vulkan 1.0              저장 장치      Micro SD 카드 슬롯              전원 공급      USB-C 포트/GPIO 헤더를 통한 전원 공급, 5V DC, 최소 3A      보조 기억 장치라즈베리 파이4는 SD카드를 지원한다. 운영체제 설치까지 고려해서 용량이 32GB 는 되어야 할 것 같다.우선 가장 무난하게 Sandisk Ultra 32GB (6천원대) 사용할 예정이다. 호환 가능한 SD 카드 리스트도 확인해보자.라즈베리 파이 환경에서 SD카드 제품별로 성능을 비교한 훌륭한 블로그글이 있으니 참고하시라.입출력 장치이번에 선물 받은 하기비스 미니 모니터를 연결할 예정이다. 다만 전원 등 호환이 될지 모르겠다. 알리에서 5만원대에 구입했는데, 현재 7만원대로 올랐다.하기비스 모니터 설명으로 미루어봐서 micro HDMI to USB-C 어댑터가 필요할 것으로 보인다.  One Cable Connection Mode: USB3.1 Gen2 cable connects HOST port and USB-C port of the laptop, allowing for simultaneous IPS mini screen display and interface expansion. *Note: The USB-C interface of the laptop must be Thunderbolt 3/4, USB4, or a USB-C interface that supports the DisplayPort (DP) protocol in order to use this mode키보드는 집에 있는 키크론 키보드를 활용할 것이다.사실 입출력 장치는 큰 걱정이 없다. 어차피 맥북으로 SSH 원격 접속할 예정이기 때문이다.네트워크자취방의 와이파이를 연결할 예정이다. 다만 자취방 와이파이가 매우 불안정하여 제멋대로 끊길 때가 많은데 - 체감상 4~5시간에 한 번(…) - 이게 얼마나 큰 문제로 작용할지는 … 일단 연결해봐야 알겠다.전원 공급 장치라즈베리 파이4 부터는 전원을 USB-C 타입으로 공급 받는다. 모니터를 연결했을 때 전류가 얼마나 될지 모르겠지만, 가장 안전하게 라즈베리 파이4에 맞게 나온 5V 4A 전용 어댑터면 좋을 것 같다. 가격은 7천원.기타 장비그 외 케이스, 방열판, 쿨러 등 보조 부품들이 있다. 이것은 필수 부품들을 갖추고 나서 하나씩 모을 생각이다. 이왕이면 이쁜 것들로 고심해서 고르고 싶기도 하고.결산            필수 부품      가격                  라즈베리 파이      78,000원 ~              SD 카드      6,200원 ~              micro HDMI to USB-C 어댑터      제품을 찾을 수 없음;;              전원 어댑터      7,000원              보조 장비      아직 모름              총합      83,200원 이상      대충 훑어봤지만 micro HDMI to USB-C 제품이 보이지 않는 게 약간 난감하다. micro HDMI 를 HDMI 로 변환한 후에 C타입으로 연결해야 할 것 같은 불길한 느낌. 그래도 필수 부품만 해서 10만원 이하로 맞출 수 있다면 충분히 만족스러울 것 같다.나가며오늘은 홈서버 프로젝트를 위해 목표와 장비의 스펙을 정리해보았다. 필수 부품을 하나씩 마련한 후에 운영체제 설치 등 세팅 과정을 블로그에 연재할 계획이다. 실습까지 갈 길이 멀지만 즐겁게, 꾸준히 해보자!"
    } ,
  
    {
      "title"    : "내가 무엇을, 왜 하고 있는지 인지하기 [수습기간 회고] ",
      "tags"     : "회고",
      "date"     : "2024-11-21",
      "url"  : "https://woocosmos.github.io/probation/",
      "content": "들어가며2024년 9월 지금의 조직으로 이동하여 3개월의 수습 기간 종료를 앞두고 있다. 합류와 동시에 바로 현업에 투입되었기 때문에 what/why/how를 정리하지 못한 채 정신없이 시간을 보냈다. 이번 수습 회고에서는 지금까지 해온 구체적인 ACTION을 직무 관점에서 ‘무엇을 했는지’로 추상화하고, ‘왜 그렇게 했는지’ 의도를 부여하려 한다. 그리고 과정 중에서 어려움을 느꼈거나 개선의 여지가 있는 지점을 파악할 것이다.물론 프로젝트의 내용은 밝히기 어려우니 기술 스택을 중점으로 다룰 예정이다. 일부 구체적인 사례가 필요한 경우에는 간단한 예시로 대체하겠다.직무에 대해서지금 회사에서 나의 직무는 분석가다. 여기서 분석가는 UX분석가, 데이터분석가, AI엔지니어 등을 포괄적으로 지칭하는 말이다. 그 중에서 나는 AI엔지니어라는 포지션로 입사했지만 대체로 데이터 과학자, 데이터 분석가, ML엔지니어를 오가는 다양한 업무를 접했다.이전 회사에서는 하나의 프로젝트에 소속된 상태였다. 따라서 해당 서비스의 데이터 안에서 여러 방면으로 경험할 수 있었다. 당연하게도 모든 일의 목적은 근본적으로 ‘서비스를 성공시키는 것’이기 때문에 업무마다 문제와 목표가 달라지곤 했다. 간단하게는 지표의 현황을 파악하는 것, 위험을 최소화하기 위해 pain point를 탐색하는 것, 서비스의 피처가 의도대로 작동하는지 확인하는 것, ML 기반의 새로운 컨텐츠를 리서치하는 것, 내부 개발진을 대상으로 데이터 기반의 의사결정을 도모하는 것 등등.이렇게 분석가로서 다양한 영역의 작업을 경험한다는 것은 뚜렷한 장단점을 지닌다.  장점          다양한 스킬셋을 확보할 수 있었다      비즈니스와 기술적 액션의 연결고리를 이해하게 되었다      협업과 커뮤니케이션 역량을 강화했다        단점          나의 역할과 커리어의 방향성 불분명했다      특정 영역의 전문성을 쌓기 어려웠다      특히 비즈니스와 기술적 액션을 연결 짓는 역량은 나에게 대단한 발전이었다. 어쨌든 회사는 돈을 벌어야 한다. 기술적으로 깊이를 더하거나 새로운 기술을 도입, 고도화하더라도 그것이 비즈니스 임팩트를 창출하거나 의사결정에 실질적으로 기여하지 못한다면 “그래서 이걸 해서 뭐?” 라는 허무한 질문으로 끝나버릴 수 있는 것이다. 조직과 나의 성과로 연결되는 일을 해야 한다는 중요한 깨달음을 얻었다. (물론 리서치 중심의 조직이거나 학술적 목적이 주가 되는 곳이라면 상황이 다를 수 있다)한편 커리어의 방향성이 불분명하다는 것은 여전히 고민 지점이다. 예를 들어 데이터 직군의 채용 공고를 아무거나 눌러 읽어본다. JD에 나열된 기술 역량을 살펴보면 애매하다. 채용공고만 봐도 나의 경험과 역량이 여러 포지션에 걸쳐져 있다는 것을 알 수 있다. 심지어는 특정 자격 요건에 해당 되더라도 “내가 이걸 해봤다고 말할 수 있나?” 라고 자문하게 된다. 해본 건 맞다. 그런데 그것을 전문적으로, 깊이 있게, 비즈니스 성과로 이어지도록 해냈는지는 확신을 갖기 어려운 것이다.실제로 지난 면접에서 오간 대화를 통해 나의 전문성이 부족하다는 것을 깨달을 수 있었다. 최종적으로는 불합격한 포지션이었다.  면접관  A 모델의 학습 과정에 B 모델을 접목 시킨 이유가 무엇이었나요?  나  B 모델의 XX적 특성이 문제와 연결된다고 생각해서 적용했습니다.  면접관  실제로 그 접근 방식이 잘 워킹하는지 어떻게 검증하셨나요?  나  OOO 지표가 기존보다 빠르게 수렴하는 것을 확인할 수 있었습니다.  면접관  B 모델의 내부 요소가 A 모델에 어떤 영향을 미쳐서 성능이 향상되는지 검증해본 적 있나요?  나  (일단 적용하는 데만 공수를 겁나 들였고 전반적인 성능만 확인해서 할 말이 없음)  면접관  B 모델의 +++ 를 추출해서 시각화하는 방식은 생각해보셨나요?  나  그런 검증 방식은 떠올리지 못했습니다.창피하다…그렇다. 나는 ‘일단 해본다’에서 그쳐온 것이다. 아니, 사실은 내가 정확히 무엇을, 어떻게, 왜 했는지조차 설명할 수준이 안 되었던 것이다. 일련의 깨달음을 끝으로 나는 기본기와 전문성을 갖추는 것 그리고 내가 무슨 일을 하는지 명확하게 정리하고 인지하는 것에 집중하기로 결심한다.작업 정리정리한 방식은 이렇다.  업무 일지를 기반으로 구체적인 작업을 리스트로 나열한다  각 항목을 추상적인 표현으로 바꾸고 큼직한 키워드로 분류한다  각 항목의 의도를 설명한다  어려웠거나 개선의 여지가 있는 지점을 짚는다예를 들어,  ACTION          Snowflake 에서 AAA 라는 스키마 아래에 있는 TABLE0, TABLE1, TABLE2를 참조하는 SQL 문을 작성했다        WHAT          클라우드 기반의 데이터 웨어하우스의 구조를 이해하고 여러 소스로부터 데이터를 통합하는 SQL 쿼리를 작성했다 =&gt; Data Engineering        WHY          최신 데이터를 정확하고 효율적으로 통합함으로써 데이터 분석과 시각화의 기반을 마련하고자 했다        HOW          복잡한 SQL 쿼리 : API로 fetch 한 JSON 배치가 통째로 한 셀에 들어가 매우 nested된 형식이었다. 더 효율적인 데이터 추출을 위해 최적화할 필요가 있다.      낯선 데이터베이스의 구조 파악 : 구조화된 문서로 세부 사항을 기록하고, 필요한 경우 자체 데이터 사전(Data Dictionary)을 구축할 수 있다      실제로 수행한 업무는 Snowflake 에서 AAA 라는 스키마 아래에 있는 TABLE0, TABLE1, TABLE2를 참조하는 SQL 문을 작성했다 이지만, 데이터 직무의 관점에서 무슨 일을 한 건지 객관화, 추상화할 수 있게 된다.정리 결과, 키워드는 크게 네 가지로 나눌 수 있었다. 각 키워드에 해당하는 WHAT 들을 아래 나열해본다.            협업 및 운영      데이터 분석                  - 데이터 분석을 위한 Ubuntu 개발 환경 구축  - CI/CD 도구와 협업 도구 통합 설정  - 데이터플랫폼 계정 및 라이센스 할당, 스펙 설정  - API 및 데이터 웨어하우스 활용 가이드 제공  - 외부 리소스 활용한 아이디어, 방향성 제시      - NLP 모델 활용한 다국어 데이터 처리 리서치  - LLM 프롬프트와 호출 비용 분석 - 통계 기반 가설 검증과 인사이트 도출 - 비즈니스 목적에 맞춘 모델 평가 지표 설계              데이터 엔지니어링      데이터 시각화              - 데이터 그레인 정의, API 활용 가능성 리서치  - 외부 API 기반의 데이터 수집  - 클라우드 기반 데이터 웨어하우스 연동 최적화  - 데이터베이스 메타 분석과 문서화  - SQL 쿼리 최적화 및 성능 모니터링, 검증 - 데이터 정합성 문제 해결 및 관계 테이블 설계  - 대시보드용 요약 테이블 운영 - 데이터 통합 플랫폼 활용한 파이프라인 구축      - 데이터 기반 대시보드 설계 및 구현  - 대시보드 설계 단계의 시뮬레이션 및 피드백 수집  - BI 도구와 클라우드/정적 데이터 연동  - 인포그래픽 및 웹앱 설계, 제작      지난 3개월 동안 진행한 내용 중 제일 비중이 컸던 키워드는 데이터 엔지니어링이었다. 글 서두에 합류 직후 현업에 바로 투입되었다고 언급했는데, 데이터 인프라를 셋업하기 위한 업무를 주로 진행했기 때문이다.“당장 필요하니까 해본다”의 방식으로 일을 진행해왔는데, 이렇게 정리함으로써 내가 무엇을 하고 있는지 보다 명확하게 파악할 수 있었다. 특히 구체적인 업무 내용을 데이터 분야의 용어로 추상화할 수 있다는 점이 만족스럽다. 머릿속에서 명쾌하게 설명할 수 있게 되었다고 느낀 요소들을 몇 가지 나열해보면 이렇다.데이터(통합)플랫폼데이터를 통합, 활용하기 위한 각종 시스템을 ‘데이터 통합 플랫폼’이라는 용어로 표현할 수 있었다. Snowflake, Tableau 등이 여기에 해당된다.데이터웨어하우스(DHW)데이터베이스 이론에서 항상 보던 이 용어가 바로 여기에 쓰인다는 것을 알았다. ‘대용량 데이터를 중앙 집중식으로 관리하는 저장소’ 라는 정의로는 크게 와닿지 않았는데, 실제 업무를 해온 환경을 돌아보면 데이터 레이크, 웨어하우스, 데이터 마트 등으로 모두 설명할 수 있었다.데이터 그레인 (Granularity)데이터를 적재, 분석하는 세부 수준을 일컫는 말이다. ‘데이터를 어떤 기준과 단위로 다뤄야 하지?’ 라는 고민이 곧 데이터 그레인에 대한 정의였음을 알았다.데이터베이스 메타 분석쿼리를 작성하기 위해서는 데이터베이스의 구조부터 전반적으로 파악해야 했다. 이러한 일련의 작업은 메타 분석이라는 이름으로 정리할  수 있었다.데이터 정합성샘플이 중복되거나, 일관되지 않거나, 누락되기도 했다. 또 맵핑하는 key에 따라 결과가 부정확하기도 했다. 데이터를 정확하게 추출한다는 것은 데이터의 정합성을 체크했음을 전제로 한다.요약 테이블여러 소스로부터 데이터를 수집하고 가공, 집계한 결과를 하나의 테이블에 밀어 넣었다. 단순하게 ‘대시보드용 테이블’이라고 부르던 것을 요약 테이블이라는 말로 설명할 수 있게 되었다.그 외에도 API가 무엇인지 실질적으로 이해했고, 데이터베이스의 기본 요소나 성능 최적화에 대해서도 고려할 수 있게 되었다. 면접에서 “API 가 무엇인가요?” 라고 물어보았을 때,  “Application Programming Interface의 줄임말입니다” 라고 외운 대로 읊는 수준에서 한 걸음 더 발전한 느낌이랄까.나가며마지막으로 나의 결심을 다시 한번 상기해보겠다.  기본기와 전문성을 갖추는 것  내가 무슨 일을 하는지 명확하게 정리하고 인지하는 것사실 1번과 2번은 아주 밀접하게 연결되어 있었다. 기본적인 지식이 있어야 내가 무슨 일을 하는지 이해할 수 있다.지난 3개월은 각종 불안과 불확신으로 가득했다. 특히 실력을 키워야 한다는 강박관념이 제일 컸다. 때로는 퇴근하면서 “오늘 엄청나게 바빴지만, 실제적으로 한 일은 없는 것 같다” 라는 불안감에 떨기도 했다. 하지만 이번 회고를 통해 내가 해본 적 없는 영역의 기본기를 쌓는 과정이었음을 배웠다. 어떻게 보면 퍼즐이 맞춰지는 과정과도 같았다. ‘그래서 팀원이 그때 그렇게 했구나’ 혹은 ‘그래서 SQLD 시험에 그런 내용이 포함되어 있었구나’ 라고 깨달은 것이다.어쩌면 이것은 비전공자의 숙명일지도 모른다. 이 모든 걸 전공 수업에서 진작에 배웠다면 좀 달랐을까? 하는 상상이 스쳐 지나갔지만, 이제 그런 걱정은 소용이 없다. 앞으로 무엇을 할지가 더 중요하니까. ML엔지니어답게 베이즈 업데이트해갈 뿐이다.(난데없지만 나의 성장 과정이 베이즈 확률 모델처럼 작동한다고 느껴졌다. 초보적인 이해라는 사전 확률에서 시작하여 다양한 경험과 시행착오를 통해 데이터를 쌓고 사후 확률로서 업데이트된 실력으로 나아가는 것이다 … 이런 비유가 자꾸 떠오르는 것은 인문학도의 숙명이라고 하자.)여전히 내가 무엇을 하고 싶은지 딱 잘라 말하기 어렵고 나의 직무에 대해서 자신 있게 설명하지 못한다. 그럼에도 불구하고 다행인 것은 아직도 하고 싶은 게 너무나도 많다는 것이다! ML엔지니어가 아니면 안 된다는 고집과 스페셜리스트가 되는 것에 대한 로망은 잠시 내려놓고, 초심으로 돌아가려 한다. “앞으로 무엇이 되어야 한다” 라는 강박에서 벗어나기 위함이다. “지금 당장 할 수 있는 것”을 수행하고 그 경험을 잘 정리하는 것이 어제의 나보다 나아지는 길일 테다.그럼, 내일도 힘 내보자."
    } ,
  
    {
      "title"    : "결정트리 모델(Decision Tree)이 나의 문제를 해결해줄 수 있을까?",
      "tags"     : "머신러닝",
      "date"     : "2024-10-23",
      "url"  : "https://woocosmos.github.io/decision-tree/",
      "content": "들어가며아래와 같은 설문조사가 있다. 응답자들은 0 ~ 10점 선지 중 하나를 선택한다.이러한 응답 데이터를 활용해서 특정 값을 예측하려 한다. 어떤 모델을 쓰면 좋을까? 이 문제 상황에서 가장 먼저 떠올린 것은 결정트리 모델이었다. 최소 0부터 최대 10으로 스케일이 한정된 정수 데이터가 주어졌으며, 결정트리 모델은 조건 (예를 들면, $x &gt; 4$ ?)에 따라 샘플을 나누며 가지를 뻗어나가기 때문이다. 문제를 해결하는 데 결정트리가 적절한 모델이라는 직관적인 느낌을 받았다.그러나 내가 설명할 수 있는 최선은 여기까지였다. “내가 모델의 개념과 원리를 근본적으로 이해하고 있나?” 반성하게 되었고, 이 글을 쓰게 된 계기가 되었다.  제한된 범위의 정수로 이루어진 응답 데이터를 처리하는 데 있어서 결정트리 모델이 적절한가?즉, 이 질문에 답변하기 위해 해당 모델을 공부한 글이다.개념결정트리 Decision Tree, 의사결정 나무라고도 부른다.데이터를 기준에 따라 반복 분할함으로써 계층적인 구조로 하위 집합을 형성하는 방법론역사로 이해하기결정트리 모델의 시초는 1960년대 사회과학에서 찾아볼 수 있다. 이후 1980년대 데이터 과학 분야의 발전과 함께 결정트리 방법론이 소개되고 다양한 모델이 개발, 활용되기 시작했다.우선 1963년에 발간된 사회과학서 Problems in the Analysis of Survey Data, and a Proposal에 초기 개념이 소개되었다. 고차원의 다중공선성을 띠는 설문조사 데이터를 다루기 위한 방법론으로, 오차제곱합(SSE)를 최소화하도록 하위 그룹으로 분할한다.사회과학 분야에서 다룰 법한 간단한 예제를 상상해봤다. 응답자의 나이, 성별, 연봉 수준을 수집했으며 이에 따른 직업 만족도를 비교하려 한다. 그런데 나이가 많을수록 연봉 수준이 높아지기도 하고, 분포 상 응답자가 여자일수록 나이대가 낮아진다. 이러한 변수 간 상호작용을 고려하면서 종속변수(직업 만족도)의 차이를 설명하기 위해 응답자를 그룹으로 나눠볼 수 있다. 30대 미만/이상으로 나눠도 좋고, 성별에 따라 나눠도 좋다. 중요한 건 나눠진 그룹끼리 직업 만족도를 비교했을 때 차이가 커지도록 하는 것이다.그룹끼리 차이가 커지도록 그룹을 나누는 이유는, 그룹 안의 동질성이 높다는 것과 같은 의미이기 때문이다. 한 그룹의 평균을 예측값으로 삼는다고 치자. 그룹의 동질성이 높을수록 그룹 내 샘플들은 평균에 가까이 몰려 있을 테니 실제값과 예측값 간의 오차가 전반적으로 줄어든다.이렇듯 직업 만족도가 가장 크게 차이나는 두 그룹으로 나눠 첫번째 분할을 마쳤다. 그 다음에 오차가 더 큰 그룹을 선택한 다음 그 안에서 또 분할한다. 마찬가지로 그룹 간 차이가 크게 발생하는, 다시 말해 오차를 최소화하는 분할 기준을 결정한다. 분할을 반복한다.Problems in the Analysis of Survey Data, and a Proposal(1963), 17페이지. 오늘날 결정트리 모델을 시각화한 모습과 같다.사회과학 분야에서 분할의 결과는 이렇게 활용할 수 있다. ‘30세 미만 남성 노동직 그룹과 여성 고졸 그룹의 직업 만족도가 75점 수준으로 비슷하다’ - 한편 머신러닝 분야라면? 입력 데이터에 대해 예측값을 출력하는 모델을 생성하는 데 활용할 수 있겠다! 30세 미만 남성 노동직이 입력으로 들어오면 75점 가량으로 예측하는 것이다.이어 1984년 Classification and Regression Trees에서 결정트리 방법론을 기반으로 한 알고리즘 CART가 발표됐다. Classification And Regression Tree의 줄임말이다. 이어 CART의 업그레이드 버전인 ID3(Iterative Dichotomiser 3), C4.5, C5.0 등이 발표되면서 결정트리 모델이 더욱 발전해나갔다. 오늘날 많이 사용하는 Random Forest, XGBoost도 결정트리 방법론을 기반으로 개발된 알고리즘들이다.분할의 기준CART, ID3, C4.5, C5.0 알고리즘은 모두 결정트리 방법론을 기반으로 한다. 하지만 분할 기준을 무엇으로 설정하는지에 따라 다른 알고리즘으로 발전한 결과다. 앞서 언급한 예제에서는 간단하게 그룹별 직업 만족도의 평균값 차이를 기준으로 분할했지만, 실제 알고리즘에서는 더 효율적이거나 정교한 지표를 분할의 기준으로 삼는다.분할 기준의 본질은 불순도(impurity)를 낮추는 것이다. 앞서 그룹 내 동질성을 높이는 것이 중요하다고 언급했다. 이는 불순도를 낮춘다는 말과 의미가 동일하다.다른 데이터가 많이 섞여 있을수록 동질성이 떨어지며 이는 곧 불순도가 높은 상태다분할의 결과로 얼마나 동질성이 높아지거나 낮아졌는지, 불순도를 수치화하는 방식은 크게 두 가지. 지니 계수와 엔트로피(정보 이득)다.소개한 알고리즘 중 CART는 지니 계수를 사용하며, ID3, C4.5, C5.0은 엔트로피 및 정보 이득을 사용한다.지니 계수(Gini coefficient)지니 계수가 높을수록 불순도가 높다고 판단한다.\\[\\text{Gini} = \\sum_{j=1}^{J} p_{j}(1-p_{j}) = 1 - \\sum_{j=1}^{J} p_{j}^{2}\\]  $J$  각 클래스  $p_{j}$  샘플이 클래스 $j$에 속할 확률  $p_{j}(1-p_{j})$  같은 클래스의 샘플을 뽑을 확률과 이어 다른 클래스의 샘플을 뽑을 확률을 곱하여 한 노드 안이 얼마나 섞여 있는지 나타내준다지니 계수는 경제학에서 소득이 얼마나 불평등하게 분포되었는지 나타내는 데 쓴다. 결정트리 모델의 맥락에서는 분할된 노드에 얼마나 다른 클래스 샘플이 섞여 있는지 표현해준다. $p_{j}^{2}$은 그 클래스의 샘플이 두 번 연속 추출될 확률이기 때문에 값이 높을수록 동질성이 높다는 의미다. 지니 계수는 이를 전체에서 뺌으로써 불순도를 계산한 결과다.위에서 첨부한 이미지를 통해 분할 결과를 지니 계수로 평가해보겠다.각 클래스에 대해 $p(1-p)$를 구한 후 합친다.좌측 노드  노랑 클래스 : 4개 중 2개이므로, $p_{\\text{노랑}} = 0.5$. 즉, $0.5 * (1 - 0.5)$.  초록 클래스 : 4개 중 1개이므로, $p_{\\text{초록}} = 0.25$. 즉, $0.25 * (1 - 0.25)$.  검정 클래스 : 4개 중 1개이므로, $p_{\\text{초록}} = 0.25$. 즉, $0.25 * (1 - 0.25)$.$\\text{Gini}_{\\text{좌측}} = 0.25 + 0.1875 + 0.1875 = 0.625$우측 노드  노랑 클래스 : 3개 중 1개이므로, $p_{\\text{노랑}} \\approx 0.333$. 즉, $0.333 * (1 - 0.3335)$.  초록 클래스 : 3개 중 2개이므로, $p_{\\text{초록}} \\approx 0.667$. 즉, $0.667 * (1 - 0.667)$.$\\text{Gini}_{\\text{좌측}} = 0.222 + 0.111 = 0.445$그 다음 분할된 개수를 기준으로 가중평균을 구한다. 좌측 노드는 네 개, 우측 노드는 세 개 샘플을 가져갔다.\\[\\text{Gini}_{\\text{예시1}} = \\frac{4}{7} \\times 0.625 + \\frac{3}{7} \\times 0.445 \\approx 0.548\\]해당 분할 결과에 대해서도 동일하게 구한다.\\[\\text{Gini}_{\\text{예시2}} = \\frac{3}{7} \\times 0 + \\frac{4}{7} \\times 0.375 = 0.214\\]좌측 노드에서 다른 클래스가 추출될 확률($1-p$)는 $0$이므로 계산 결과도 $0$이 된다.분할 전 지니 계수가 $0.612$ 라는 점을 고려하면, 두 분할 방식 모두 지니 계수가 줄어들어 불순도가 낮아진 상태이다. 둘 중에서는 이렇게! 의 지니 계수 $0.214$ 가 이렇게? 의 $0.548$ 보다 더 낮으므로 더 적합한 분할이라고 판단할 수 있다.엔트로피(Entropy)엔트로피는 정보량의 기대값을 의미한다.\\[\\text{Entropy} = -\\sum_{j=1}^{J} p_{j} \\log{(p_{j})}\\]  $J$  각 클래스  $p_{j}$  샘플이 클래스 $j$에 속할 확률  $-\\log{(p_{j})}$  클래스 $j$에 대한 정보량정보 이론에서는 의외인 사건이 발생할 때 정보량(Information Content)이 더 많다고 본다. 여기서 ‘의외인 사건’은 곧 사건의 확률이 낮다는 것을 의미한다. (11월은 강우 확률이 낮으니까 11월에 눈이 오는 것은 의외인 사건이며 정보량이 많은 것이기도 하다. 1월에 눈이 오는 사건에 비해서 말이다.)이처럼 확률이 낮을수록 정보량이 많고 높을수록 정보량이 낮아지는 것은 확률에 로그를 취하고 음수화한 값으로 표현할 수 있다. 그리고 이 정보량에 확률을 곱함으로써 구한 기댓값을 엔트로피라고 부른다.\\[-p_{j} \\times \\log{(p_{j})}\\]  확률이 낮다 =&gt; 정보량이 많다 =&gt; 엔트로피가 높다 &lt;=&gt; 사건을 예측하기 어렵다노드에 다양한 클래스가 혼재되어 있을수록 어떤 클래스가 추출될지 예측하기 어렵다. 따라서 불순도를 측정하는 방식으로 엔트로피를 쓸 수 있다.정보획득량 (Information Gain)결정트리 알고리즘 ID3 에서는 ‘정보획득량’을 불순도 지표로 사용하는데, 이는 분할 전후의 엔트로피 차이를 계산한 값이다.\\[IG(S, A) = H(S) - \\sum_{t \\in T} p(t) H(t) = H(S) - H(S|A)\\]  $IG(S, A)$  $A$ 속성을 기준으로 $S$를 분할했을 때의 정보 획득량  $H(S)$  $S$의 엔트로피  $T$  $S$를 분할함으로써 생성된 노드들  $p(t)$  $S$ 대비 노드 $t$의 비율(크기)  $H(t)$  노드 $t$의 엔트로피  $H(S|A)$  $A$ 속성을 기준으로 $S$를 분할하여 생성된 노드들의 $H(t)$를 가중 평균한 값‘이렇게!’ 분할 방식을 정보 획득량에 따라 평가해보자. 나이 30살을 기준으로 샘플을 분할했다고 가정했다.먼저 분할 전 엔트로피 $H(S)$를 구한다. 엔트로피 식 $-\\sum_{j=1}^{J} p_{j} \\log{(p_{j})}$ 을 적용하면 된다.  노랑 클래스 : 7개 중 3개이므로, $- \\frac{3}{7} \\log \\frac{3}{7}$  초록 클래스 : 7개 중 3개이므로, $- \\frac{3}{7} \\log \\frac{3}{7}$  검정 클래스 : 7개 중 1개이므로, $- \\frac{1}{7} \\log \\frac{1}{7}$$H(S) \\approx 0.523 + 0.523 + 0.402 = 1.448$다음으로 분할 후 엔트로피 $H(S|A)$를 구한다. 이는 각 노드의 엔트로피를 가중 평균하여 계산한다.True 노드  초록 클래스 : 3개 중 3개이므로, $- \\frac{3}{3} \\log \\frac{3}{3} = 0$$H(\\text{True}) = 0$False 노드  노랑 클래스 : 4개 중 3개이므로, $- \\frac{3}{4} \\log \\frac{3}{4} = 0$  검정 클래스 : 4개 중 1개이므로, $- \\frac{1}{4} \\log \\frac{1}{4} = 0$$H(\\text{True}) \\approx 0.311 + 0.5 = 0.811$그 다음 분할된 비율을 기준으로 가중평균을 구한다. 좌측 노드는 네 개, 우측 노드는 세 개 샘플을 가져갔다.$H(S|A) = \\frac{3}{7} \\cdot H(\\text{True}) + \\frac{4}{7} \\cdot H(\\text{False}) \\approx 0.463$최종적으로 분할 전에 비해 정보량을 비교한다.\\[IG(S, A) = H(S) - H(S|A) \\approx 1.448 - 0.463 = 0.985\\]분할 전후의 엔트로피를 비교한 결과 정보 이득은 약 $0.985$이다. 반면 이렇게? 분할 방식의 정보 이득을 계산하면 $0.198$로 비교적 정보 이득이 적은 것을 알 수 있다.이처럼 불순도를 낮추는 방향으로 데이터를 분할하는 것을 반복함으로써 계층적인 트리 구조를 형성하는 것이 결정트리 방법론이다. 여기서 ‘불순도’를 어떤 지표로 평가할지, 언제까지 분할을 반복할지, 어떤 노드를 주로 참고할지 등 구체적인 활용 방식에 따라 다른 알고리즘이 될 수 있다.직관 설명하기  제한된 범위의 정수로 이루어진 응답 데이터를 처리하는 데 있어서 결정트리 모델이 적절한가?지금까지 결정트리 방법론의 등장 맥락, 분할의 기준을 살펴보았다. 이제 처음의 질문으로 돌아가보자.“제한된 범위”결정트리 모델은 값이 좁은 범위로 제한된 데이터에 대해 유리할 수 있다. 결정트리가 비모수적 모델(non-parametric models)이기 때문이다.결정트리 모델은 주어진 데이터를 거듭된 조건에 따라 쪼개 나간다. 특정 분포를 가정하지 않고 주어진 데이터에 따라 비선형적인 구조를 형성한다는 의미다. 반면 선형회귀와 같은 모수적 모델은 모든 범위의 데이터에 대해서 유효한 분포(함수)가 있다고 가정하고 이에 맞춰 파라미터를 학습(fit)한다. 실제 데이터에 비해 함수 가정이 과도할 수 있다. 우리는 0 ~ 10 의 범위에서 벗어나는 입력은 전혀 고려하지 않고 있단 말이다.“정수”결정트리 모델은 특정 임계값을 기준으로 데이터를 분할하는데, 정수형 데이터는 이산적(discrete) 성격을 띠므로 분할 자체가 직관적이고 간단해진다. 예를 들어 ‘5번 질문에 대한 응답이 3점 이상’이라는 분할 조건을 세우는 건 간단하고 직관적이다. 하지만 데이터가 연속형이었으면 ‘A 속성이 0.3728 이상’과 같이 정밀한 임계값을 정해야 했을 것이다. 당연히 더 많은 경우의 수를 고려해야 하기 때문에 난이도가 높아진다.“응답 데이터”설문 참여자들은 비슷한 질문에 대해 비슷하게 응답하는 경향이 있을 것이다. 예를 들어 ‘인생이 행복합니까?’ 라는 질문에 그렇다고 응답할수록, ‘생활이 만족스럽습니까?’ 라는 질문에도 그렇다고 응답할 것이다. 이처럼 변수 간 상관관계가 존재하는 다중공선성의 문제에 결정트리 모델이 유리하다. 애초에 결정트리는 사회과학 연구에서 일종의 교차분석과 유사한 방법론으로 제시되지 않았는가?결정트리는 불순도가 (지니 계수든 엔트로피든) 가장 크게 감소하는 조건을 선택하여 분할을 실시하기 때문에 가장 중요한 변수가 우선적으로 선택되는 효과가 있다. 그 결과 비슷한 패턴의 변수들은 자연스럽게 제외되기 때문에 변수 간 상관관계는 결정트리 모델에 큰 해가 되지 않는다.나가며결정트리의 작동 맥락과 주요 지표를 살펴봄으로써 나의 직관을 설명할 수 있었다. 이 과정에서 ‘비모수형 모델’의 개념을 명확하게 이해할 수 있게 되었고, 머신러닝 분야에서 ‘엔트로피’가 가지는 의미를 다시 상기할 수 있었다. 이번 공부를 통해 나의 직관이 어느 정도 의미 있었다는 결론을 내렸으므로, 이제 실제 적용하는 일만 남았다.분량이 너무 길어져서 가지치기나 정지규칙, 코드 등 더 깊은 내용은 다루지 못했지만 모델의 본질과 연결되는 직관을 이해했다는 점에서 만족스러운 공부였다. 이어서 결정트리 방법론을 기반으로 하는 다양한 머신러닝 모델에 대해서 하나씩 공부하는 시간을 가져보겠다."
    } ,
  
    {
      "title"    : "📖 『JAX/Flax로 딥러닝 레벨업』(제이펍) 리뷰",
      "tags"     : "리뷰",
      "date"     : "2024-10-13",
      "url"  : "https://woocosmos.github.io/jax-lev-up-review/",
      "content": "IT 실용서 전문 출판사 제이펍으로부터 책 『JAX/Flax로 딥러닝 레벨업』를 무료로 제공 받았다.개요  도서명  JAX/Flax로 딥러닝 레벨업  지은이  이영빈 , 유현아 , 김한빈 , 조영빈 , 이태호 , 장진우 , 박정현 , 김형섭 , 이승현  발행사  제이펍  초판 발행  2024년 9월 23일  정가  24,000원베타리더 후기에 따르면 『JAX/Flax로 딥러닝 레벨업』은 무려 국내 최초 JAX 입문서라고 한다. 최근 성장하고 있는 JAX 생태계의 활성화에 기여하는 의미가 있겠다.개인적으로 JAX는 ‘고성능 딥러닝 연산이 가능한 numpy’ 정도로 알고 있는 상태였고 직접 활용해본 적은 없었다. 이번 책 리뷰를 계기로 Numpy와의 차이점과 JAX/Flax의 주요 특징을 이해하고 실제 튜토리얼을 따라가는 경험을 쌓으려 한다.책은 크게 JAX/Flax를 소개하는 부분과 JAX/Flax를 활용하여 딥러닝 모델을 구현하는 부분으로 이뤄져 있다. 파이썬 프로그래밍과 기본적인 머신러닝 개념은 책을 읽기 위한 선수 지식으로 요구된다.Jax란?  한마디로 표현하면 자동 미분과 XLA를 결합해서 사용하는 고성능 머신러닝 프레임워크입니다 … JAX의 가장 큰 강점은 XLA를 적용해서 사용할 수 있다는 점입니다.PyTorch, Tensorflow 등 다른 딥러닝 프레임워크도 자동 미분을 지원하지만, JAX는 이에 더해 XLA(Accelerated Linear Algebra)이 가능하다는 것이 핵심이다. XLA는 GPU/TPU 위에서 numpy를 컴파일하고 실행하는 컴파일러다. JAX는 JIT(Just-In-Time) 컴파일을 통해 파이썬 코드를 XLA에 최적화된 기계어로 변환하기 때문에 PyTorch의 동적 그래프보다도 빠르고 효율적으로 연산할 수 있다는 것이다.파이썬은 기본적으로 인터프리터 방식으로 실행되기 때문에 코드를 한 줄씩 읽고 해석하는 데 시간이 소요된다. 여기서 JIT 컴파일을 사용한다면 코드를 실행하는 시점에 성능과 연관되는 일부분을 미리 기계어로 컴파일하여 속도가 빨라진다고 이해했다.Flax  JAX + Flexibility를 합쳐져서 만들었으며 엔지니어들이 JAX를 조금 더 쉽게 사용할 수 있는 프레임워크이며, 다른 딥러닝 프레임워크들처럼 레이어(층) 개념을 지원합니다.여기까지 읽었을 때 Tensorflow &amp; Keras 와 유사한 개념(관계)이 아닌가 싶었는데, JAX/Flax는 Low-level의 섬세한 컨트롤이 가능하다는 점에 방점이 찍혀 있는 것 같다. 그와 달리 Keras는 높은 수준의 추상화가 이루어져 있고 사용자 친화적이다. 똑같이 구글에서 개발한 프레임워크지만 지향하는 철학이 다르다는 점이 재밌다.책에 따르면 구글에서 개발한 모델들은 대부분 JAX로 작성되어 있고, 심지어 Hugging Face의 기존 모델들도 JAX로 변환하고 있다고 한다.함수형 프로그래밍JAX/Flax의 활용 방식을 더 잘 파악할 수 있도록 책은 함수형 프로그래밍에 대해서 별도 챕터로 설명한다. 명령어의 흐름(순서)대로 상태를 변경하고 결과를 전달하는 것이 핵심인 절차적 프로그래밍과 다르게, 함수형 프로그래밍은 외부 상태와 상관없이 주어진 입력에 동일한 출력값을 내놓는 순수 함수를 사용한다. 따라서 부수 효과가 제거되며 상태가 변화하지 않는 불변성을 강조한다. 여기서 절차적 프로그래밍과 함수형 프로그래밍을 설명할 때 간단한 파이썬 예제가 첨부되어 있어서 이해가 편했다.JAX, 나아가 딥러닝 연산에 있어서 이 개념을 이해하는 것이 중요한 이유를 세 가지로 제시하고 있다.  XLA 컴파일에 최적화된 처리가 가능해진다  병렬처리와 분산처리에 유용하다  코드를 모듈화함으로써 재사용성이 높아진다JAX 기본백문이 불여일견, 직접 JAX 를 활용해보며 책의 내용을 따라가보겠다.설치다행히도 JAX가 Mac M1을 공식 지원한다고 하여 conda로 쉽게 설치할 수 있었다.conda create -n jax-env python=3.9conda activate jax-envpip install jax jaxlibimport 하기import jaximport jax.numpy as jnpnumpy와 비교x1 = jnp.array([1.0, 2.0, 3.0])x2 = jnp.array([4.0, 5.0, 6.0])y = x1 + x2print(y)        # [5. 7. 9.]print(type(y))  # &lt;class 'jaxlib.xla_extension.ArrayImpl'&gt;위 예제에서 보듯 jax.numpy는 기존 numpy 와 거의 똑같은 API를 제공하고 있다.grad 함수def func(x):    return x**2grad = jax.grad(func)print(grad(3.))    # Array(6., dtype=float32, weak_type=True)JAX에서 미분, 즉 gradient를 계산해주는 grad를 사용한 예제다.부수 효과의 방지JAX는 부수 효과를 제거하는 함수형 프로그래밍의 제약을 따르고 있다. 책에서 제공해준 아래 예제를 참고해보자.x_1 = np.array([1, 2, 3])x_1[0] = 999print(x_1)      # [999   2   3]numpy로 생성한 배열은 직접 접근해서 요소를 변경할 수 있다.x_2 = jnp.array([1, 2, 3])x_2[0] = 999# TypeError: '&lt;class 'jaxlib.xla_extension.ArrayImpl'&gt;' object does not support item assignment. JAX arrays are immutable.반면 jax.numpy로 생성한 배열은 직접적인 수정을 허용하지 않는다. 이는 ‘외부 데이터’인 배열의 상태가 변형되면서 부수 효과가 발생하는 것을 방지하기 위함이다.만약 배열의 일부를 수정하는 작업을 진행하고 싶다면 부수 효과가 없는 순수 함수를 사용해야 한다.x_2 = jnp.array([1, 2, 3])def modify(x):    return x.at[0].set(999)y = modify(x_2)print(y)    # Array([999,   2,   3], dtype=int32)여기서 modify(x)는 부수 효과가 없는 순수 함수라고 볼 수 있는 것이며, 책은 jax.grad와 jax.jit 같은 함수는 순수 함수로 작성되어야 한다고 설명하고 있다.JIT 컴파일  변환  주어진 함수를 변경하거나 수정하는 방식. 성능 최적화나 자동 미분을 가능하게 함.책은 JAX에서 변환(transformation)이라는 키워드가 중요하다고 말한다. JAX에서 변환은 jaxpr, 즉 JAX 표현식이라는 중간 언어(intermediate language)를 통해 이루어진다. jax.jit가 대표적인 jax 변환이라고 소개된다.def selu(x, alpha=1.67, lamdba_=1.05):    return lamdba_ * jnp.where(x &gt; 0, x, alpha * jnp.exp(x) - alpha)x = jnp.arange(1000000)# 일반selu(x).block_until_ready()# XLAselu_jit = jax.jit(selu)selu_jit(x).block_until_ready() # 비동기 실행위 내용은 활성화 함수 SELU(Scaled Exponential Linear Unit)를 구현하고 호출한 내용이다.selu(x) 대신에 jit 변환을 적용한 selu_jit(x)가 7배 빠르다고 설명하고 있다. (구글 Colab T4 기준)책을 읽으면서 흥미로웠던 부분은 jax.jit은 컴파일된 계산 그래프를 캐싱하여 재사용한다는 점이었다. 다만 jax.jit을 반복문 내부에서 호출할 경우 컴파일 과정이 불필요하게 반복될 수 있으니 지양하라고 안내하고 있다.Flax마지막으로 Flax 를 활용한 예제를 살펴보겠다.import flax.linen as nnfrom jax import randomkey = random.PRNGKey(42)class MLP(nn.Module):    out_dims: int        @nn.compact    def __call__(self, x):        x = x.reshape((x.shape[0], -1))        x = nn.Dense(128)(x)        x = nn.relu(x)        x = nn.Dense(self.out_dims)(x)        return xmodel = MLP(out_dims=10)x = jnp.empty((4, 28, 28, 1))weights = model.init(key, x)y = model.apply(weights, x)책에 import 하는 부분은 없어서 추가했다nn.Module 에서 상속받아 모델을 생성한다는 점에서 PyTorch 와 유사한 방식의 API 라고 느껴졌고 금방 적응할 수 있겠다는 생각이 든다.나가며『JAX/Flax로 딥러닝 레벨업』에서 JAX 핵심 개념을 위주로 살펴보며 책을 리뷰해보았다. 요즘 시점에서 왜 굳이 새로운 딥러닝 프레임워크가 필요할까? 라고 막연히 궁금해 하며 리뷰어 신청을 했는데, 좋은 기회로 책도 제공 받고 JAX와 Flax에 대해 가볍게 배워볼 수 있는 시간이었다.JAX가 지향하는 철학과 함께 그것이 녹아든 핵심 기능을 세세하게 설명해주기 때문에 JAX 입문서로 아주 알맞은 도서라는 생각이 들었다. 특히 함수형 프로그래밍 개념만을 설명하기 위해 별도 지면을 할애했다는 점에서는 JAX의 의미를 제대로 전달하겠다는 강한 의지도 보였다.아마 JAX로 입문하기까지 가장 큰 장벽은 앞서 내가 떠올린 것과 같이 “왜 꼭 이것이어야 하는가?” 라는 의문일 텐데, 이 책을 읽는다면 그 장벽 정도는 충분히 넘을 수 있겠다. numpy 하나로 모델을 구현하는 정도로 low-level에서 모델링과 학습 과정 등을 유연하게 통제할 수 있다는 점이 JAX/Flax의 가장 강력한 정체성이라고 느꼈다. 구글 TPU를 사용하는 ML엔지니어라면 시간을 들여서라도 JAX를 적용할 가치가 있을 듯하다.그 외에 CLIP, GPT 같은 최신 모델의 fine-tuning 을 예제로 다룬 점도 인상적이었다. ML 분야에 입문한 이후로 출판서를 활용해서 공부를 하는 건 정말 오랜만인데, 역시 최신 책이니 최신 모델도 다루는구나 - 싶었다.다만 책 서문에서 이미 밝혔다시피 딥러닝 개념과 프레임워크에 대한 기본적인 지식이 있어야 책의 내용을 제대로 이해할 수 있다는 점은 염두에 둘 필요가 있겠다. 확실히 ‘초급서’라기보단 ‘입문서’로 보는 게 맞다. 또한 워낙에 고급 프레임워크다보니까 JAX/Flax 자체가 서비스(서빙)보다는 연구에 적합한 도구라는 생각이 들었다. 책 읽기 전과 비슷하게 이것이 이것이 필요할까? 라는 질문은 여전히 깔끔하게 해명되진 않았지만, 책을 읽고 나서 언젠가 JAX를 써보고 싶다는 욕심은 보다 뚜렷해졌다.리뷰어로 선정하여 도서를 제공해준 출판사 제이펍에 진심으로 감사하다는 말씀을 표하며 본 리뷰를 마무리하겠다."
    } ,
  
    {
      "title"    : "📖 『컴퓨터 구조와 운영체제 핵심 노트』(길벗) 리뷰",
      "tags"     : "리뷰",
      "date"     : "2024-09-16",
      "url"  : "https://woocosmos.github.io/gilbut-ca-os/",
      "content": "길벗 개발자 리뷰어(25차)에 선정되어 책 『컴퓨터 구조와 운영체제 핵심 노트』를 무료로 제공 받았다.  도서명  컴퓨터 구조와 운영체제 핵심 노트  지은이  서지영  발행사  (주)도서출판 길벗  초판 발행  2024년 5월 30일  정가  25,000원출판사 길벗은 이전에 모두의 네트워크 기초라는 책으로 접해 익숙하다. 컴퓨터 지식 관련 서적을 많이 출판하기 때문에 전공자, 비전공자를 가리지 않고 공부가 필요한 사람들이 참고하면 좋겠다.서문에서 밝히듯 이 책은 개념적으로 이해하기 어려운 컴퓨터 구조와 운영체제를 그림을 통해 핵심 위주로 이해하는 것을 목표로 한다. 추천 독자는 아래와 같다.  IT 분야에 입문하려는 사람  IT 분야 취업을 위해 빠르게 핵심 내용을 익혀야 하는 학생  IT 분야 취업 후, 컴퓨터/운영체제 지식이 필요한 사람  컴퓨터 원리를 이해하려는 개발자책은 2부에 걸쳐 컴퓨터 구조와 운영체제를 각각 다룬다.            주제      키워드                  컴퓨터 구조      데이터 처리, 명령어, CPU, 하드웨어, 캐시 메모리, 보조기억 장치, 입출력 장치, 병렬처리                  운영체제      프로세스, CPU 스케줄링, 교착상태, 메모리 할당, 가상 메모리, 파일 시스템      『컴퓨터 구조와 운영체제 핵심 노트』는 이름 그대로 핵심을 깔끔하게 정리해준다.이를 테면 컴퓨터의 기본 구조(하드웨어)에 대해 책은 아래와 같이 요약한다.  CPU, 기억 장치, 입출력 장치가 컴퓨터를 구성하는 기본 구조입니다. 그리고 이들 간 데이터 및 명령어 전달은 시스템 버스가 담당합니다.중요한 내용은 이렇게 그림으로 표현해 이해에 도움을 준다.각 구성 요소를 짚은 다음 실제 컴퓨터가 동작하는 원리를 살펴보기 때문에 더욱 개념적으로 와닿았다.남는 공간에 스스로 이해한 내용을 다시 정리, 작성하면서 공부했다.나의 경우 기본적인 내용일수록 공부하면서 충분히 이해하고 넘어가도 시간이 지나면 자꾸 까먹는 경향이 있다. 정리본을 계속 들추어보는 것도 한계가 있어서 이번 리뷰를 계기로 아예 시험 공부를 하듯 암기를 해두려 한다.각 챕터 말미에 제공되는 ‘확인 문제’는 이러한 나의 니즈에 딱 맞는다.그뿐만 아니라 간단하게라도 예제 코드를 포함하고 있어 도움에 이해가 됐다. 그런데 어떤 예제는 C언어로, 어떤 예제는 파이썬으로 적혀 있어 일관성이 떨어지는 점은 아쉽다. 물론 본문에서는 참고용으로만 봐달라고 언급하고 있다.전반적인 개념을 핵심 위주로 정리하는 책이기 때문에 세부 주제를 깊이 있게 탐색하고자 하는 독자라면 다소 아쉬움이 남을 수 있겠다. 『컴퓨터 구조와 운영체제 핵심 노트』는 컴퓨터 과학에 입문하거나 배운 내용을 복습하는 용도로 적극 추천한다.베타 리더 후기 중에서 심화 학습용 도서로 (마찬가지로 올해 길벗에서 출판한) 『컴퓨터 밑바닥의 비밀』을 추천하고 있다. 장바구니에 넣어두어야겠다.개발자 리뷰어로 선정해주시고 도서까지 제공해준 출판사 길벗에 감사의 인사를 전달하며 본 리뷰를 마무리하겠다."
    } ,
  
    {
      "title"    : "GitHub Actions 활용한 태스크 및 배포 자동화",
      "tags"     : "DevOps",
      "date"     : "2024-08-30",
      "url"  : "https://woocosmos.github.io/github-action/",
      "content": "요약- CI/CD 플랫폼인 GitHub Actions의 기능과 구성 요소를 살펴봤다- GitHub Actions를 활용하여 특정 스크립트를 작동시키거나 로컬로 빌드한 내용을 배포하는 태스크를 자동화했다개요GitHub Actions를 사용하여 특정 스크립트를 실행시키고 배포하는 워크플로를 자동화한다.자동화가 필요한 이유는 두 가지다.  블로그 포스트가 늘어남에 따라 추천 키워드의 점수를 새롭게 집계하고 업데이트해야 한다  로컬에서 사이트를 직접 빌드한 다음에 gh-pages 브랜치에 반영함으로써 배포해야 한다따라서 이번 포스트에서는 태스크의 최신화와 효율화를 위해 GitHub Actions의 워크플로를 구성해보겠다.GitHub ActionsGitHub Actions는 빌드, 테스트 및 배포 파이프라인을 자동화할 수 있는 CI/CD 플랫폼이다.  CI/CD : Continuous Integration(지속적 통합) 및 Continuous Delivery/Deployment(지속적 제공/배포). 소프트웨어의 개발 라이프사이클을 효율화, 가속화 하는 DevOps 개념.코드 변경에 따라 블로그 콘텐츠를 업데이트한다는 점에서 CI 개념과 연결되고, Jekyll 사이트를 배포한다는 점에서 CD 개념과 연결된다. (정확히 말하자면 CI는 여러 작업자가 commit한 작업을 효율적이고 빠르게 통합함을 목표로 한다)GitHub Actions는 PR, Push 등 이벤트가 발생할 때 워크플로를 실행시킬 수 있다. 자체 인프라에서 워크플로를 실행할 수 있지만 GitHub에서도 주요 OS의 가상 머신을 통해 서버를 제공하고 있다. 각 구성 요소를 살펴보겠다.구성 요소Workflows(워크플로)하나 이상의 작업으로 구성된 프로세스. YAML 파일로 정의하며 특정 조건에 따라 트리거 된다. 여러 워크플로를 생성할 수 있고, 한 워크플로 안에서 다른 워크플로를 참조할 수도 있다.Events(이벤트)워크플로의 실행을 트리거 하는 활동이다. GitHub Actions 에서 사용할 수 있는 이벤트의 리스트를 참고하자.Jobs(작업)워크플로 안에서 단계로 구성된 집합. 이 단계들은 같은 실행기(runner) 안에서 작동하기 때문에 데이터를 공유할 수 있다. 디폴트로 작업들끼리는 종속성이 없기 때문에 병렬로 실행되나, 한 작업이 다른 작업에 종속된다면 완료를 기다린다.runner(실행기)워크플로를 실행하는 서버. 하나의 runner는 한 번에 하나의 작업(job)을 실행할 수 있다.시작하기root 경로에서 .github/workflows 폴더를 생성, 해당 위치에 test.yml 파일을 작성한다.name: Teston:  push:    branches:      - 'post/**'jobs:  my_first_job:    runs-on: ubuntu-latest    steps:    - name: step-example      run: echo Hello World!post로 시작하는 브랜치에 push 이벤트가 발생할 때 트리거 되는 워크플로를 정의했다. my_frist_job 이라는 작업이 실행될 텐데, ubuntu 실행기에서 Hello World 를 출력하는 step을 포함하고 있다.yml 파일을 저장한 후, post/github-actions 브랜치에 push 해보았다. 그 결과는 프로젝트 레포지토리의 Actions 탭에서 확인할 수 있다왼쪽 탭에 Test 라는 워크플로가 생성되어 있고 워크플로의 실행 내역이 표시되어 있다. post/github-actions에 push 함으로써 트리거된 것이다.my_frist_job 작업이 성공적으로 실행되었고 그것을 눌러 step-example 단계도 실행되었음을 확인할 수 있다. 이제 본격 나의 태스크에 적용해보겠다.키워드 업데이트추천 키워드를 업데이트할 조건을 정한 후, 그에 따른 트리거와 작업 내용을 YAML 파일로 작성했다.name: Workflow for updating keywordson:  pull_request:    branches:      - master    types:      - closed  push:    branches:      - masterjobs:  my-job:    if: (github.event_name == 'pull_request' &amp;&amp;            github.event.pull_request.merged == true &amp;&amp;             startsWith(github.event.pull_request.head.ref, 'post/')) ||        (github.event_name == 'push' &amp;&amp;            contains(github.event.head_commit.message, 'post'))    runs-on: ubuntu-latest    steps:      - name: Approach the Codes        uses: actions/checkout@v3      - name: Set up Python        uses: actions/setup-python@v5        with:          python-version: '3.8'          cache: 'pip'                - name: Install Ubuntu Dependencies        run: |          sudo apt-get update          sudo apt-get install -y g++ openjdk-8-jdk            - name: Install Python Dependencies        run: |          python -m pip install -r $/requirements.txt      - name: Run the Script        run:          python $/assets/recommend.py      - name: Configure Git        run: |          git config --global user.name \"${GITHUB_ACTOR}\"          git config --global user.email \"${GITHUB_ACTOR}@users.noreply.github.com\"            - name: Check for Differences        id: check_diff        run: |          git add $/keywords.json          if git diff --cached --quiet; then            echo \"No changes detected\"            echo \"has_changes=false\" &gt;&gt; $GITHUB_OUTPUT          else            echo \"has_changes=true\" &gt;&gt; $GITHUB_OUTPUT          fi      - name: Commit the Change        if: steps.check_diff.outputs.has_changes == 'true'        run: |          echo \"pushing the file ...\"          git commit -m \"[automation] keywords updated\"          git push워크플로가 트리거 되는 조건은 아래와 같다.  post로 시작하는 브랜치의 PR를 완료했을 때 (보통 post 브랜치에서 포스트를 작성한 후 완성했을 때 master로 merge시키기 때문)  push 의 커밋 메세지에 ‘post’가 포함되어 있을 때 (master 브랜치에서 바로 수정하여 push할 때도 있으므로)‘my-job’ 작업은 환경을 세팅하고(1~4번) 스크립트를 실행시키고(5번) 변경 사항을 git push 하는(6~8번) 일련의 단계들을 포함하고 있다.  Approach the Codes  Set up Python          actions/setup-python@v5 : GitHub Actions에서 제공하는 파이썬 환경이다      cache: 'pip' : GitHub Actions는 캐싱 기능을 제공하는데, 이 옵션을 명시해주면 pip 캐시를 복원하여 활용하고 캐시가 없을 경우 새로 설치한다        Install Ubuntu Dependencies  Install Python Dependencies          프로젝트의 root 경로는 ${{ github.workspace }}라는 변수로 접근할 수 있다        Run the Script  Configure Git  Check for Differences          keywords.json 파일만 stage 에 올리고 변경사항이 있는지 확인한다. 이 부분을 추가하지 않으면 nothing to commit 에러가 발생하며 워크플로가 중단된다.      이 플래그는 has_changes라는 변수에 저장되어 다음 step에서 사용된다        Commit the Change          앞선 단계에서 선언한 has_changes으로 if 조건을 명시하고, 앞서 stage에 올린 파일을 리모트에 반영한다      참고로 konlpy 를 사용하는 만큼 환경 구축이 까다로울 것 같아서, docker로 ubuntu 컨테이너를 하나 띄워서 시뮬레이션 했다. 나중의 활용을 위해 여기에 커맨드를 정리해둔다.volume binding 하지 않고 docker cp 명령어로 파이썬 파일을 복제해 썼다.docker run --rm -d --name fake-github-actions ubuntu:latest tail -f /dev/nulldocker cp /path/to/recommend.py fake-github-actions:/root/recommend.pydocker exec it fake-github-actions /bin/bash# 파이썬 설치 후 Ubuntu, python 의 어떤 dependencies가 필요한지 테스트흥미로운 지점은 cache: 'pip'를 사용하지 않았을 때 실행시간이 더 짧았다는 점인데, pip로 설치하는 패키지의 개수가 많지 않아 오히려 캐시를 복원해오는 데 시간이 더 소요되는 것으로 보인다. 이는 전체 시스템 디렉토리를 캐싱해야 하는 Ubuntu 패키지에 대해서도 마찬가지다. 따라서 최종 코드에서는 해당 옵션을 제외했다.배포 자동화배포는 JEKYLL DEPLOY ACTION이라는 액션을 사용했다. 개발 스토리를 살펴보면,  GitHub Pages는 허용된 플러그인만 안전 모드 상에서 실행해주기 때문에, 커스텀 플러그인을 사용하는 경우 로컬에서 직접 빌드하고 gh-pages로 배포해야 할 때가 있다.나의 니즈에 딱 맞는 action이라 바로 사용해보기로 했다.name: Build and Deploy to Github Pageson:  push:    branches:      - masterjobs:  build_and_deploy:    runs-on: ubuntu-latest    steps:      - uses: actions/checkout@v4      - uses: actions/cache@v4        with:          path: |            .asdf/**            vendor/bundle          key: ${{ runner.os }}-cache-${{ hashFiles('**/cache.key') }}          restore-keys: |            ${{ runner.os }}-cache-      - uses: jeffreytse/jekyll-deploy-action@v0.6.0        with:          provider: 'github'                   token: ${{ secrets.GITHUB_TOKEN }}           branch: 'gh-pages'                   jekyll_src: './'          이미 gh-pages 브랜치를 만들어 배포에 사용하고 있었기 때문에 수정 없이 그대로 적용할 수 있었다. 이 워크플로를 통해 jeykll 환경을 구성, master를 기준으로 build하고 gh-pages 에 반영하는 과정을 자동화했다. master 브랜치로 push 이벤트가 발생할 때마다 자동으로 배포가 되는 셈이다.+ 2024-09-16 업데이트2주 만에 블로그를 업데이트하고 master로 push 했더니 error: RPC failed; HTTP 400 curl 92 HTTP/2 stream 7 was not closed cleanly: CANCEL (err 8) 라는 에러와 함께 배포에 실패했다. 이슈 채널에 동일한 에러를 호소하는 사람이 있었다. 답변에서 안내해주는 대로 SSH 옵션을 추가했다.  Note: SSH approach has higher priority than HTTP approach when you provide both at the same time.- uses: jeffreytse/jekyll-deploy-action@v0.6.0  with:  ...  ssh_private_key: $  ...하지만 마찬가지로 에러가 발생해서 jekyll-deploy-action의 버전을 master로 바꾸어보았다.- uses: jeffreytse/jekyll-deploy-action@master배포에 성공했다. master 브랜치에 관련 에러가 이제 막 반영된 모양이다. (심지어 지금 시간 기준 50분 전에 올라온 동일한 이슈도 있다..)나가며이로써 GitHub Actions를 활용하여 매우 간편하게 CI/CD를 자동화해보았다. 액션의 yaml 파일을 구성하는 과정이 docker-compose.yml 를 작성하는 것과 유사해서 금방 해낼 수 있었던 것 같다.이번 경험은 cache를 재사용하는 것이 무조건 빠르다는 선입견을 깨게 된 계기가 되기도 했다.실무 환경에서 CI/CD 작업을 처리할 때 Jenkins(젠킨스)도 많이 활용하는 것으로 알고 있다. 실제로 내가 속한 조직에서 ML모델 추론과 웹 서버 호스팅을 Jenkins으로 관리하기도 했다. 기회가 된다면 다음에는 Jenkins도 써보고 싶다."
    } ,
  
    {
      "title"    : "블로그 키워드 추천 및 검색 기능 구현기",
      "tags"     : "JavaScript, NLP",
      "date"     : "2024-08-26",
      "url"  : "https://woocosmos.github.io/search-page-dev/",
      "content": "요약- simple-jekyll-search 라이브러리를 활용하여 검색 기능을 추가했다- 탭에서 접근할 수 있는 검색 페이지를 구성하고 검색 결과를 적절하게 시각화했다- TF-IDF 값을 활용하여 검색할 키워드를 추천했다개요Kiko Now Jekyll 테마에 검색 기능이 없는 관계로 직접 검색 페이지를 구성하고 JS 라이브러리로 검색 기능을 적용했다. 블로그에 포스트가 쌓일수록 검색 기능이 필수적이라고 생각했다. 태그 기반의 문서 분류 방식은 한계가 명확하고, 특정 문서를 찾을 때도 검색이 가장 간편하기 때문이다.블로그 개발 과정은 특정 포스트에 아카이빙하고 있지만, 해당 기능은 분량이 많아 별도로 기록한다.기본 기능입력 키워드로 블로그 내 모든 컨텐츠에 대해 검색하는 기본 기능부터 적용한다.Simple-Jekyll-Search 라이브러리를 적용했다. 브라우저 단[Client-Side]에서 작동하기 때문에 별도 서버나 DB를 구축할 필요가 없다. 적용 과정 역시 간단하다. 첫째, search.json 생성 블로그의 root 위치에 아래 search.json 파일을 만든다.---layout: none---[  {% for post in site.posts %}    {      \"title\"    : \"{{ post.title | escape }}\",      \"tags\"     : \"{{ post.tags | join: ', ' }}\",      \"date\"     : \"{{ post.date | date: '%Y.%m.%d'}}\",      \"url\"  : \"{{ site.url }}{{ post.url }}\",      \"content\": \"{{ post.content | strip_html | strip_newlines | escape }}\"    } {% unless forloop.last %},{% endunless %}  {% endfor %}]기존 코드에 post.content 를 추가하여 본문 텍스트도 가져오게 했다. 이때 strip_html 등 몇 가지 Jekyll 문법을 더했다. 이를 통해 {baseurl}/search.json 주소로 json 파일에 접근할 수 있다. 브라우저로부터 이 파일을 읽어와 검색 데이터로 활용할 것이다. 둘째, JavaSript 소스 추가 simple-jekyll-search.js를 다운 받아 어디든 위치시킨다. 이 스크립트는 search.json으로부터 데이터를 읽어와 입력어에 매칭되는 내용을 찾는 함수 SimpleJekyllSearch를 정의하고 있다.그 다음 search-and-return.js도 다운 받아 같은 위치에 붙여넣는다. SimpleJekyllSearch 함수를 실행시켜서 결과값을 받아와 처리하는 부분을 별도 스크립트로 작성한 것이다.var sjs = SimpleJekyllSearch({    searchInput: document.getElementById('search-input'),    resultsContainer: document.getElementById('results-container'),    json: '/search.json',    searchResultTemplate: ...    noResultsText: '😴 검색 결과가 없습니다',    templateMiddleware : ...)}나는 assets 라는 폴더를 만들어 이곳에 JS 소스를 모아두기로 했다. 두 스크립트는 다음 서술할 검색 페이지에서 실행시킬 것이다. 셋째, 검색 페이지 구성 root 위치에 search 폴더를 생성하고 그 아래 index.html 파일을 생성한다. 앞서 추가한 JS 스크립트가 실행되면서 검색어 입출력이 진행되는 곳이다.---layout: pagepermalink: /search---&lt;ul class=\"search\"&gt;    &lt;div id=\"search-container\"&gt;        &lt;input type=\"search\" id=\"search-input\" placeholder=\"  🤔 검색어를 입력하세요.\"&gt;        &lt;ul id=\"results-container\"&gt;&lt;/ul&gt;    &lt;/div&gt;&lt;/ul&gt;&lt;script src=\"{{ site.baseurl }}/assets/simple-jekyll-search.js\" type=\"text/javascript\"&gt;&lt;/script&gt;&lt;script src=\"{{ site.baseurl }}/assets/search-and-return.js\" type=\"text/javascript\"&gt;&lt;/script&gt;이렇게 {baseurl}/search 주소로 접근할 수 있는 검색 페이지를 완성하였다.한편, 무엇을 클릭했을 때 이 검색 페이지로 연결되도록 할 것인가? 나는 상단의 탭에 돋보기 아이콘을 추가했다. 이를 위해 돋보기 아이콘 svg와 검색 페이지를  연결하여 _includes/nav.html에 추가했다.&lt;ul class=\"search-icon\"&gt;  &lt;a href=\"{{ site.baseurl }}/search\"&gt;    &lt;svg       width=\"24\"       height=\"24\"       viewBox=\"0 0 24 24\"       fill=\"none\"       xmlns=\"http://www.w3.org/2000/svg\"&gt;      &lt;path d=\"M10 ...생략\" fill=\"currentColor\"&gt;&lt;/path&gt;    &lt;/svg&gt;  &lt;/a&gt;&lt;/ul&gt;이렇게 검색 페이지를 구성하고 검색용 라이브러리를 적용하는 과정을 마무리하였다.응용 기능기본 검색 기능에 더하여 키워드를 더 쉽게 찾고 검색 결과를 더 잘 표현하기 위해 개선한 사항이다.결과창 개선이미지에 보듯 결과창에 추가한 내용은 다음과 같다.  검색어 하이라이트  클릭시 본문 및 태그 페이지로 링크 연결  본문 미리보기  검색어가 언급된 횟수 표시  아이콘과 함께 메타 정보 디스플레이이를 위해 SimpleJekyllSearch 함수의 templateMiddleware와 searchResultTemplate 인자를 활용할 것이다. 전자는 검색 결과가 있을 때 호출되는 함수이며 후자는 그 결과를 출력할 HTML 템플릿을 지정하는 인자다. 하나씩 짚어보도록 하겠다.templateMiddleware 인자는 다음과 같이 정의된 함수를 필요로 한다.function (prop, value, template) {    return value    }prop은 json의 key이고 value는 key에 대한 value를 의미한다. 이제 prop, 즉 ‘항목’에 따라 value를 처리할 것이다.URL과 날짜 항목은 json에 저장된 그대로 사용할 예정이기 때문에 바로 value를 반환하도록 했다.if (prop === \"url\" || prop === 'date') {          return value;        }나머지 제목, 태그, 본문 항목은 검색어 하이라이트를 적용하고 URL과 연결하는 작업이 필요하다.우선 검색어를 저장하고, 그것을 찾는 정규표현식 객체와 최종적으로 리턴될 변수를 선언한다.const searchTerm = document.getElementById(\"search-input\").value;const regex = new RegExp(searchTerm, \"gi\");let highlightedValue;제목 항목은 정규표현식 객체로 검색어($&amp;)를 HTML 태그로 감싼다. 배경색깔을 바꾸고 폰트를 굵게 표시한다.if (prop === 'title') {  highlightedValue = value.replace(regex, '&lt;span style=\"background:gold\"&gt;&lt;b&gt;$&amp;&lt;/b&gt;&lt;/span&gt;')  return highlightedValue;}태그 항목의 경우 태그 페이지의 각 태그 링크와 연동했다.앞서 json을 생성하면서 쉼표로 join 했기 때문에 split하고 join하는 과정을 한번 더 거치는데, search.json을 개선해서 전처리를 간소화하는 대안도 고민해봐야겠다.if (prop === 'tags') {    const dest = window.location.origin;    const theTags = value.split(', ').map(tag =&gt; tag.trim());        highlightedLinkedValue = theTags.map(tag =&gt; {      // 하이라이트를 적용한다      const highlightedTag = tag.replace(regex, '&lt;b style=\"background:gold\"&gt;$&amp;&lt;/b&gt;');      // 링크를 적용한다      return `&lt;a href=\"${dest}/tags/#${tag}\"&gt;&lt;span&gt;${highlightedTag}&lt;/span&gt;&lt;/a&gt;`;    }).join(', ');        return highlightedLinkedValue;        }마지막으로 본문을 보여주는 데 있어 두 가지 지점을 고려했다. 첫번째는 30개 단어까지 미리보기로 보여주는 것이고 두번째는 본문에서 match가 걸린 횟수를 명시하는 것이다.첫번째 매치를 기준으로 앞뒤 15개 단어까지 슬라이스하였다. 본문에 매치가 없을 경우 처음부터 30개 단어를 가져온다. 마지막으로는 정규표현식 객체에 매치된 matches의 개수를 명시했다.value = value.replace(/\\[.*?\\]/g, '');const matches = value.match(regex);let matchCnt;if (matches) {    // 띄어쓰기를 기준으로 토큰화한다    const wordsArray = value.split(/\\s+/);    // 검색된 단어를 기준으로 앞뒤 15개 토큰을 사용한다    const matchIndex = wordsArray.findIndex(word =&gt; regex.test(word));    const start = Math.max(0, matchIndex - 15);     const end = Math.min(wordsArray.length, matchIndex + 15 + 1);     const truncatedValue = wordsArray.slice(start, end).join(\" \");    // 하이라이트를 적용한다    highlightedValue = truncatedValue.replace(        regex,        '&lt;span style=\"background:gold\"&gt;&lt;b&gt;$&amp;&lt;/b&gt;&lt;/span&gt;'    );    matchCnt = matches.length} else {    // 본문에 검색어가 없을 경우 처음부터 30개 토큰을 가져온다    const words = value.split(/\\s+/).slice(0, 30).join(\" \");    highlightedValue = `${words}...`;    matchCnt = 0}// 언급수highlightedValue += `&lt;div style=\"padding-top:5px\"&gt;&lt;span id=\"match-counter\"&gt;본문에 &lt;b&gt;${matchCnt}&lt;/b&gt;번 언급되었습니다&lt;/span&gt;&lt;/div&gt;`;return highlightedValue;이렇게 처리된 결과물은 searchResultTemplate에 정의한 HTML 템플릿대로 디스플레이 된다. 이곳에서 Font Awesome(폰트 어썸) 태그를 정의하여 아이콘으로 영역을 시각적으로 구분했다. 그리고 제목과 본문을 클릭했을 때 본문으로 연결해주는 &lt;a&gt; 태그를 추가했다searchResultTemplate:         '&lt;article&gt;'+        '&lt;div&gt;&lt;i class=\"fas fa-book fa-fw\"&gt;&lt;/i&gt;&lt;a href=\"{url}\"&gt;{title}&lt;/a&gt;&lt;/div&gt;'+        '&lt;div&gt;&lt;i class=\"fas fa-clock fa-fw\"&gt;&lt;/i&gt;&lt;span&gt;{date}&lt;/span&gt;&lt;/div&gt;'+        '&lt;div&gt;&lt;i class=\"fas fa-tag fa-fw\"&gt;&lt;/i&gt;{tags}&lt;/div&gt;'+        '&lt;div style=\"display:inline-flex\"&gt;' +             '&lt;i class=\"fas fa-pencil-alt fa-fw\" style=\"padding-top:5px\"&gt;&lt;/i&gt;&lt;a href=\"{url}\"&gt;&lt;span style=\"color:#343a40\"&gt;{content}&lt;/span&gt;&lt;/a&gt;' +         '&lt;/div&gt;'+        '&lt;/article&gt;'참고로 아이콘을 불러오기 위해서는 search 폴더의 index.html에 폰트 어썸 링크를 추가해주어야 한다.&lt;link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css\"&gt;키워드 추천유저에게 무슨 키워드를 검색할지 가이드를 제공해주는 것은 어떨까?포스트 본문을 데이터로 활용하여 키워드의 중요도를 집계하고 상위 n개 키워드를 추천하는 기능을 떠올렸다.아이디어는 이렇다.  블로그 전체에서 제목과 본문 텍스트를 수집하여 전처리하고 키워드를 추출한다  블로그 포스트 개수를 고려하여 키워드별 TF-IDF를 집계하고 상위 5개를 저장한다  추천 키워드를 불러와 검색 페이지에서 띄워준다1~2번은 Python으로 실행하고 3번은 JavaScript로 구현하기로 했다.데이터 수집과 처리{baseurl}/search.json 주소로 json 파일에 접근할 수 있다는 점을 기억하고 requests 모듈로 불러왔다. 그리고 그 중에서 제목과 본문만 저장했다.def collect_contents(url, pttrn, noTag):    '''    url   : search.json    pttrn : escape 기호 제외하기 위한 정규표현식    noTag : 데이터 수집에서 제외할 태그    '''    response = requests.get(url, verify=False)    cleansed_response = re.sub(pttrn, ' ', response.text)    normalized_response = re.sub(r'\\s+', ' ', cleansed_response)        json_data = json.loads(normalized_response)    contents = [post['title'] + ' ' + post['content'] for post in json_data if noTag not in post['tags']]    return contents수집한 데이터를 키워드 말뭉치 형태로 변환한다. 이때 영어와 한국어를 따로 추출하여 한 글자인 단어와 불용어 사전에 포함된 단어를 제외했다. 명사를 대상으로 하고 싶었기 때문에 한국어는 형태소 분석기를 사용하여 명사를 추출했다.def create_corpus(contents):    # eng_prc, kor_prc는 별도로 정의한 전처리 함수다    keywords_eng = list(map(eng_prc, contents))    # 한국어는 형태소 분석 후 명사만 사용된다    keywords_kor = list(map(kor_prc, contents))    corpus = [' '.join(e+k) for e, k in zip(keywords_eng, keywords_kor)]    return corpus참고로 한국어 형태소 분석은 konlpy를 사용했다.from konlpy.tag import Oktdef kor_prc(c):    okt = Okt()    kor_res =[]    for k in okt.nouns(c):        if (len(k) &gt; 1) &amp; (k not in stop_words):            kor_res.append(k)    return kor_res불용어 사전은 NLTK’s list of english stopwords와 Latex 문법 리스트를 크롤링해서 stopwords.txt 파일로 구축하였다. 더불어 span, div와 같은 HTML 태그도 불용어 사전에 포함했다.TF-IDF 계산TF-IDF(Term Frequency-Inverse Document Frequency)는 문서 안에서의 출현 빈도수와 전체 문서 집합에서의 희귀성을 적용한 가중치이다. scikit-learn 의 TfidfVectorizer을 사용해 계산했다. 여러 개 포스트를 올리는 블로그 특성 상 주요 키워드를 추출하는 데 TF-IDF가 적합하다고 생각하여 적용했다.def extract_keywords(corpus, topN=5, asset_dir=None):    vectorizer = TfidfVectorizer()    tfidf_matrix = vectorizer.fit_transform(corpus)    feature_names = vectorizer.get_feature_names_out()    # 단어가 열, 문서가 행이므로 각 단어에 대해 문서 전반의 값을 sum    sum_tfidf_scores = np.array(tfidf_matrix.sum(axis=0)).flatten()    average_tfidf_score = sum_tfidf_scores.mean().round(2)    top_indices = sum_tfidf_scores.argsort()[-topN:][::-1]    top_keywords = [['말뭉치 평균', average_tfidf_score]]    top_keywords += [[feature_names[idx], round(sum_tfidf_scores[idx], 2)] for idx in top_indices]    ...    # 저장    with open(save_path, 'w') as f:      json.dump(top_keywords, f)문서별로 단어의 TF-IDF 값이 계산되는데, 전체 블로그 관점에서 중요도를 고려해야 하므로 단어 단위로 총합 값을 계산했다. 또 추출된 키워드의 중요도를 상대적으로 비교할 수 있도록 전체 키워드의 평균 TF-IDF 값도 함께 저장했다.추천 키워드 디스플레이파이썬으로 계산한 결과는 keywords.json 이라는 이름으로 따로 저장했다. 첫번째 요소는 전체 말뭉치 평균이고, 그 아래부터 1위, 2위 … 5위에 해당한다. [키워드, 점수] 쌍의 리스트로 이루어져 있다.[['말뭉치 평균', 0.05],  ['클러스터', 0.67],  ['toc', 0.42],  ['포인트', 0.38],  ['편차', 0.38],  ['평균', 0.33]]이제 이것을 읽어 검색 페이지에서 보여주는 JavaScript를 작성한다. 해당 스크립트는 search 폴더의 index.html 에서 실행된다. 먼저 json을 읽어온 후 반복문으로 HTML 태그를 추가했다. 키워드를 클릭하면 바로 검색되도록 click 이벤트를 추가했다.// 미리 추출한 키워드 json 데이터를 불러온다const response = await fetch('keywords.json');const data = await response.json();...// 첫번째 요소는 전체 평균이므로 두번째 요소부터 반복문을 시행한다data.slice(1).forEach((obj, rnk) =&gt; {    const li = document.createElement('li');    const span = document.createElement('span');    // 키워드를 읽어온다    span.textContent = obj[0];    // 클릭할 경우 해당 키워드를 search-input 입력창으로 전달한다    span.addEventListener('click', function(event) {                    const searchInput = document.getElementById('search-input');                    searchInput.value = obj[0];                    // trigger                    const e = new Event('input', { bubbles: true });                    searchInput.dispatchEvent(e);                });    ...클릭하면 검색창에 그 키워드가 입력되면서 검색 기능이 발동된다.마지막으로 이 키워드들이 왜 추천되는지 도움말 팝업을 추가해보았다. 아이콘 위에 마우스를 호버하면 텍스트 설명이 뜨는 방식이다. 각 추천 키워드의 점수도 obj[1]으로 읽어올 수 있으므로 팝업 코드를 forEach문에 추가했다.// 도움말 아이콘 팝업const svgIcon = document.createElement('span')svgIcon.innerHTML = `&lt;svg width=\"25\" height=\"25\" viewBox=\"0 0 16 16\" 생략..&gt; &lt;/svg&gt;`;paragraph.appendChild(svgIcon);const totalAvg = data[0]const tooltip = document.createElement('div');tooltip.className = 'tooltip';tooltip.textContent = `TF-IDF 점수를 기반으로 상위 5개 키워드를 추천합니다\\n                      현재 블로그의 전체 평균 TF-IDF는 ${totalAvg[1]}점입니다\\n                      추천 키워드에 마우스를 올려 점수를 비교해보세요`;tooltip.style.whiteSpace = 'pre'; // '\\n'이 적용되려면document.body.appendChild(tooltip);svgIcon.addEventListener('mouseenter', (event) =&gt; {    tooltip.style.display = 'block';    tooltip.style.left = `${event.pageX + 10}px`;    tooltip.style.top = `${event.pageY + 10}px`;});svgIcon.addEventListener('mousemove', (event) =&gt; {    tooltip.style.left = `${event.pageX + 10}px`;    tooltip.style.top = `${event.pageY + 10}px`;});svgIcon.addEventListener('mouseleave', () =&gt; {    tooltip.style.display = 'none';});자동화끝으로 TF-IDF를 계산하는 파이썬 스크립트의 실행만 자동화하면 된다. 이 부분은 GitHub Actions로 처리했으며 GitHub Actions 활용한 태스크 및 배포 자동화에서 그 과정을 확인할 수 있다.기타JS에서 실행할 수 있는 한국어 토크나이저도 있다. 사이트를 불러올 때마다 점수를 연산하기는 어려울 것 같아서 결국 python으로 구현했지만, 충분히 활용 가치가 있을 것 같아서 기록해둔다"
    } ,
  
    {
      "title"    : "수학, 통계 개념 모음집",
      "tags"     : "수학, 통계",
      "date"     : "2024-08-21",
      "url"  : "https://woocosmos.github.io/basic-statistics/",
      "content": "수학 및 통계 용어와 수식을 정의하고 ML 관점에서의 직관을 서술한다.평균, 분산, 표준편차\\[\\bar{x} = \\frac{x_{1}+ x_{2} + \\dots +x_{n}}{n}\\]데이터 값을 모두 더한 다음(sum), 값의 개수(데이터 사이즈)로 나눈다 👉 평균  데이터 포인트들의 대표값\\[\\frac{\\sum_{n}^{i}{({x}_{i} - \\bar{x})^{2}}}{n}\\]  편차  데이터별로 평균과의 차이를 구한다. 이때 편차의 총합은 $0$이다.\\[\\begin{aligned}&amp; (x_{1}-m) + (x_{2}-m) + \\dots + (x_{n}-m) \\\\&amp; = (x_{1}+x_{2}+\\dots+x_{n}) - n \\times m \\\\&amp; = n \\times m - n \\times m \\\\&amp; = 0\\end{aligned}\\]  편차제곱  편차별로 제곱한다(squared deviations)  편차제곱의 평균  편차제곱을 모두 더한 다음(제곱합, sum of squares), 값의 개수로 나눈다👉 분산  데이터 포인트들이 평균으로부터 얼마나 퍼져 있는지, 얼마나 벗어나 있는지 의미한다  High Variance: 분산이 큰 데이터로 학습된 모델은 입력값이 조금만 변화해도 민감하게 반응하여 출력값이 크게 바뀌기 때문에 예측의 안정성이 떨어진다\\[\\sqrt{\\frac{\\sum_{n}^{i}{({x}_{i} - \\bar{x})^{2}}}{n}}\\]분산의 제곱근 👉 표준편차  제곱된 값에 루트를 씌움으로써 원래 데이터의 스케일(단위)에 맞추고 해석을 직관적으로 만든다Norm벡터의 크기(길이)  $p$는 Norm의 차수를 의미한다.L1 Norm (맨해튼 거리)내용L2 Norm (유클리드 거리)\\[\\sqrt{\\sum_{i=1} |x_{i}|^{2}}\\]$p=2$ 인 Norm. 피타고라스 정리를 기반으로 n차원 좌표 평면(유클리드 공간)에서의 크기를 계산한다.  활용 : L2 정규화, KNN 알고리즘, K-means 알고리즘수학집합단조 감소(monotonically decreasing)\\(\\lim_{x\\to\\infty} A_{n} = \\cap_{n=1}^{\\infty}A_{n}\\)집합이 갈수록 작아져서 모든 집합 $A$들의 교집합이 가장 작은 집합과 같아지는 것이다.  K-Means 알고리즘의 목적함수[WCSS]는 단조 감소한다용어집  identity  항등식. 언제나 성립하는 등식. 좌변과 우변의 값이 항상 같다."
    } ,
  
    {
      "title"    : "K-Means Clustering 군집화 알고리즘",
      "tags"     : "머신러닝",
      "date"     : "2024-08-20",
      "url"  : "https://woocosmos.github.io/kmeans-algo/",
      "content": "요약- K-Means 클러스터링 알고리즘은 클러스터 내 분산을 최소화함으로써 가까운 데이터끼리 클러스터를 형성하는 비지도 학습 모델이다- 분산을 전반적으로 충분히 감소시키면서 데이터 포인트를 유의미하게 그룹화하는 클러스터 개수를 찾는다- 중심점을 세팅하는 초기화 방법론을 적용한 후, 클러스터 할당과 중심점 업데이트를 반복하며 최적화한다개요데이터 분석 업무를 하다보면 유저 그룹별로 특징을 비교하고 싶을 때가 있다. 그렇다면 유저 그룹은 어떤 기준으로 나눠야 하고 각 유저를 어느 그룹으로 분류할 수 있을까? 이런 문제를 만났을 때 곧잘 사용한 것이 K-means 클러스터링 알고리즘이다.그런데 데이터 특성이나 하이퍼파라미터, 최적화 알고리즘 등 다양한 변수에 따라 상이한 군집화 결과가 나오기도 했고, 분석을 진행하기 앞서 군집화가 제대로 이루어지긴 한 건지 긴가민가할 때도 있었다.따라서 이번 포스팅에서는 K-means 알고리즘의 정의와 원리를 수식과 함께 이해하고 코드로 그 내용을 구현한다. 용어의 통일성을 위해 앞으로 군집(화) 는 클러스터(링) 으로 표현하겠다.정의K-means 알고리즘은 데이터를 $k$개의 클러스터로 묶는 클러스터링 알고리즘으로, 비지도 학습 방식에 속한다. 학습은 클러스터 내 분산을 최소화하는 방식으로 수행된다.따라서 클러스터는 가까운 데이터들이 모인 그룹을 뜻한다.목적클러스터 내 제곱합(within-cluster sum of squares, WCSS)을 최소화하는 $k$개 집합 $S$을 찾는 것이 목적이다. 목적함수는 아래와 같다.\\[\\mathop{\\operatorname{arg\\,min}}_{\\mathbf{S}} \\sum_{i=1}^{k} \\sum_{\\mathbf{x} \\in S_{i}} \\left\\|\\mathbf{x} - \\boldsymbol{\\mu}_{i}\\right\\|^{2}\\]  ${\\operatorname{arg\\,min}}_{\\mathbf{S}}$  주어진 식을 최소화하는 $S$  ${\\mu}_{i}$  $S_i$에 속한 포인트들의 평균mean (또는 중앙값median)  $||…||$  편차를 L2 Norm (유클리드 거리) 으로 계산$k$개 클러스터별 ‘데이터 포인트들과 평균의 차이를 제곱하여 합산한 값(=편차제곱합)’의 합을 의미한다. 분산이 편차제곱의 평균이라는 점을 상기한다면, 이 값은 클러스터의 분산에 클러스터의 사이즈를 곱한 것과 같다는 것을 알 수 있다.\\[\\mathop{\\operatorname{arg\\,min}}_{\\mathbf{S}} \\sum_{i=1}^{k} |S_{i}| \\operatorname{Var}(S_{i})\\]  $|S_i|$  $S_i$의 크기, 즉 해당 클러스터에 속한 포인트의 개수  $\\operatorname{Var}(S_{i})$  $S_i$의 분산, 즉 해당 클러스터에 속한 포인트들의 분산따라서 $k$개 클러스터별 ‘크기와 분산을 곱한 값(Product)’의 합으로도 표현할 수 있다. 직관적으로는 클러스터의 크기가 가중치로 작용하여 클러스터가 클수록 합산값에 기여한다고 볼 수 있으며, 클러스터들의 전반적인 분산합을 최소화해야 한다는 점에서 K-Means 알고리즘의 목적과 동일하다.계산 효율성K-Means 알고리즘의 정의로 돌아와 클러스터 내 분산이라는 키워드를 다시 생각해보자. 앞서 목적함수에서는 ‘데이터 포인트($x$)와 중심점($\\mu$) 간의 거리’로 이를 계산했다. 하지만 초점을 ‘데이터 포인트들($x, y$) 간의 거리’로 옮겨보는 건 어떨까?즉 K-Means 알고리즘은 $k$개 클러스터별 ‘데이터 쌍별 편차제곱(pairwise squared deviations)의 평균’들의 합을 최소화하는 것과도 동일하다. 한 클러스터 안에서 데이터 포인트로 만들 수 있는 모든 조합에 대해 편차를 계산하고, 이 포인트들이 평균적으로 얼마나 퍼져 있는지 나타낸 식이다.\\[{\\operatorname{arg\\,min}}_{\\mathbf{S}} \\sum_{i=1}^{k} \\frac{1}{ |S_{i}| }\\sum_{\\mathbf{x,y} \\in S_{i}}\\left\\|\\mathbf{x} - \\mathbf{y}\\right\\|^{2}\\]한편 아래 항등식(identity)에 따르면 ‘데이터 쌍별 편차제곱합’은 ‘데이터 포인트들과 평균 $c_{i}$의 편차를 제곱하여 합산한 값(=편차제곱합)에 클러스터 사이즈의 $2$배를 곱한 것’과 동일하다.\\[\\sum_{\\mathbf{x,y} \\in S_{i}}\\left\\|\\mathbf{x} - \\mathbf{y}\\right\\|^{2}=2|S_{i}|\\sum_{\\mathbf{x} \\in S_{i}}\\left\\|\\mathbf{x} - {c}_{i}\\right\\|^{2}\\]이러한 계산 구조를 활용하여 클러스터의 모든 조합에 대해 연산을 수행하지 않고도 분산을 효율적이고 간단하게 계산할 수 있다. 유도 과정은 아래와 같다.편차의 총합은 $0$이기 때문에 중간의 내적항은 사라진다.\\[\\begin{aligned}\\left\\|\\mathbf{x} - \\mathbf{y}\\right\\|^{2} &amp; = \\left\\|\\mathbf{x} - {c}_{i} + {c}_{i} - \\mathbf{y}\\right\\|^2 \\\\&amp; = \\left\\|\\mathbf{x} - {c}_{i}\\right\\|^{2} -2 (\\mathbf{x} - {c}_{i}) \\cdot (\\mathbf{y} - {c}_{i}) + \\left\\|\\mathbf{y} - {c}_{i}\\right\\|^{2} \\\\&amp; = \\left\\|\\mathbf{x} - {c}_{i}\\right\\|^{2} + \\left\\|\\mathbf{y} - {c}_{i}\\right\\|^{2}\\end{aligned}\\]이렇게 유도된 식에 원래대로 $\\sum$을 적용한다면,\\[\\sum_{\\mathbf{x,y} \\in S_{i}}\\left\\|\\mathbf{x} - \\mathbf{y}\\right\\|^{2}  = \\sum_{\\mathbf{x,y} \\in S_{i}} (\\left\\|\\mathbf{x} - {c}_{i}\\right\\|^{2} + \\left\\|\\mathbf{y} - {c}_{i}\\right\\|^{2})\\]모든 점 $x$, $y$에 대해서 합산하기 때문에 $x$, $y$ 각각 두 겹의 $\\sum$으로 표현할 수 있으며, $c$는 상수이기 때문에 $\\sum$는 $|S|$의 곱셉으로 변환할 수 있다.\\[\\begin{aligned}\\sum_{\\mathbf{x,y} \\in S_{i}}\\left\\|\\mathbf{x} - {c}_{i}\\right\\|^{2}&amp; = \\sum_{\\mathbf{x} \\in S_{i}}\\ \\sum_{\\mathbf{y} \\in S_{i}}\\left\\|\\mathbf{x} - {c}_{i}\\right\\|^{2} \\\\&amp; = |S_{i}| \\sum_{\\mathbf{x} \\in S_{i}}\\left\\|\\mathbf{x} - {c}_{i}\\right\\|^{2} \\end{aligned}\\]마지막으로 $\\sum_{\\mathbf{x,y} \\in S_{i}}\\left|\\mathbf{y} - {c}_{i}\\right|^{2}$ 에 대해서도 동일하게 유도할 수 있으므로 $2$배가 된다.\\[2|S_{i}|\\sum_{\\mathbf{x} \\in S_{i}}\\left\\|\\mathbf{x} - {c}_{i}\\right\\|^{2}\\]최적화이제 이 목적함수를 최적화하는 방식을 알아볼 것이다. 처음 초기화된 식은 아래 두 동작을 반복하며 클러스터 내 제곱합(WCSS)을 감소시킨다.  Assignment : 각 데이터 포인트에 가장 가까운 클러스터를 할당하기  Update : 각 클러스터에 할당된 데이터 포인트들의 평균값을 계산하여 새로운 클러스터의 중심을 업데이트하기이 과정을 반복할 때마다(iteration) 클러스터 내 제곱합은 항상 감소한다. 업데이트를 거듭할수록 데이터 포인트들은 점점 더 클러스터의 중심에 가까워지고 클러스터의 크기는 더 작아지기 때문이다. 이러한 단조 감소(monotone decreasing)의 성질로 인해 K-Means 알고리즘은 수렴이 가능한 식이다. 다만 ‘현재 상태에서’ 가장 가까운 클러스터를 할당하는 탐욕적(greedy) 방식으로 진행되기 때문에 지역 최적해(local optimum)에 수렴하기 쉽다.초기화업데이트에 앞서 중심점을 지정하는 초기화 단계를 거쳐야 한다. 적절하지 않은 중심점에서 시작할 경우 지역 최적해에 빠져버릴 수 있기 때문에 알맞은 초기화 기법을 선택하는 것이 좋다.  무작위 분할(Random Partition) : 모든 데이터 포인터를 $k$개 클러스터 중 하나로 임의 할당한다  Forgy 알고리즘 : 데이터 포인터 중 $k$개를 임의로 선택하고 이들을 중심점으로 사용한다          두 최적화 기법은 중심점을 임의로 설정하기 때문에 결과가 매번 달라질 수 있다는 단점이 있다        K-means++ : 기존 초기화 기법의 한계를 커버하기 위해, 거리가 가중치로 적용된 확률을 기반으로 중심점을 찾는다          데이터 포인트 중 $1$개를 임의로 선택하고 이를 첫번째 클러스터의 중심점 $c_{1}$으로 사용한다      나머지 데이터 포인트에 대해서 첫번째 클러스터의 중심점까지의 거리 제곱합을 계산한다    \\[d(x_{i}, c_{1})^2\\]          나머지 데이터 포인트에 대해서 ‘이 포인트가 다음 중심점으로 선택될 확률’을 계산한다. $c_{1}$로부터 거리가 멀수록 확률은 높아진다.    \\[p(x_{i}) = \\frac{d(x_{i}, c_{1})^2}{\\text{Sum of Squares}}\\]          계산한 확률을 기반으로 두번째 클러스터의 중심점$c_{2}$을 추출한다.      나머지 데이터 포인트에 대해서 더 가까운 클러스터의 중심점까지의 거리를 계산하고, 이에 따라 확률값도 조정한다    \\[\\min{(d(x_{i}, c_{1}), d(x_{i}, c_{2}))}\\]          $k$개의 중심점을 선택할 때까지 반복한다      클러스터의 수지금까지는 $k$를 특정 수로 상정한 상태에서 목적함수를 정의, 초기화, 최적화하는 과정을 살펴보았다. 그렇다면 정작 $k$는 어떻게 결정하는가? K-Means 알고리즘에서 클러스터의 개수 $k$는 사람이 직접 지정해주어야 한다. 이는 해당 알고리즘의 단점으로 항상 지적되는 지점이다.최적의 클러스터 수를 정하는 것은 매우 중요하다. 만약에 $k$가 데이터 포인트 수만큼 크다면 각 데이터 포인트가 중심점이 되어버리기 때문에 목적함수는 0에 가까워지겠지만 클러스터링으로서 의미없는 작업이 된다. 반면 $k$를 $1$로 지정한다면 모든 데이터 포인트가 커다란 하나의 클러스터에 속하기 때문에 이 또한 의미가 없다.즉 단지 WCSS를 최소화하는 것만이 목적이 아니며, 데이터 포인트를 유의미하게 클러스터링하는 것 그리고 충분히 밀도 있는 클러스터를 찾기 위해 WCSS를 최소화하는 것 - 이 둘이 적절한 밸런스를 이루는 클러스터 개수를 찾는 것이 K-Means 알고리즘의 열쇠다.클러스터 수를 결정할 때 사용하는 다양한 방법론 중 세 가지를 살펴보려고 한다.Rule of thumb특정 이론이라기보단 관습적으로 사용하는 경험적인 기준이다. 주어진 데이터의 사이즈가 $n$일 때,\\[k \\approx \\sqrt{n/2}\\]Elbow Method특정 범위의 $k$를 하나씩 대입하며 클러스터 내 제곱합(WSCC)을 구한다. $k$가 커질수록 WCSS는 감소하는데 이 감소율이 급격하게 떨어지는 구간, 즉 Elbow Point의 $k$로 클러스터 개수를 결정한다. 왜 값이 급격하게 떨어진 직후를 최적의 $k$로 뽑는가? $k=1$로 시작할 경우 WCSS는 급격하게 감소하기 마련이다. 하나의 클러스터가 모든 데이터 포인트를 포함한다면 클러스터 내 분산은 매우 클 것이며, 여기에 개수가 하나씩 추가될수록 전반적인 클러스터의 분산합은 빠르게 줄어들 것이다.계속해서 클러스터가 추가된다면 WCSS는 계속 감소할 테지만 감소하는 폭은 줄어들 것이다. 데이터 포인트들이 이미 중심점에 충분히 가까워져 있기 때문이다.$k$가 클수록 모델의 복잡도(Complexity)는 커지는데 그에 비해 WCSS가 크게 감소하지 않는다면, 더이상 유의미하게 클러스터가 컴팩트해지지 않고 모델 성능이 개선되지 않는다는 의미다. 결국 Elbow Point 이후로는 클러스터 개수를 늘려가면서까지 모델의 복잡도를 증가시킬 가치가 없다. 다시 말해 모델의 복잡도를 감수할 수 있을 정도로 WCSS가 ‘충분히’ 줄어드는 지점을 Elbow Point로 지정하는 것이다.한번 더 강조하자면 K-Means 알고리즘의 목적은 단지 WCSS를 최소화하는 것만이 아니라, 충분히 WCSS를 감소시키면서 데이터 포인트를 유의미하게 클러스터링하는 최적의 Trade-off 지점을 찾는 것이다.참고로 Scikit-learn과 같은 ML라이브러리에서는 Elbow Method에서 계산하는 WCSS를 inertia라고 표현한다.Silhouette ScoreSilhouette Score(실루엣 지수)은 어떤 데이터 포인트가 자신이 속한 클러스터에 얼마나 응집되어 있는지(Cohesion), 그리고 자신이 속하지 않은 인근 클러스터로부터 얼마나 떨어져 있는지(Separation) 측정하는 지표다. 주로 K-Means 클러스터링의 결과를 평가할 때 사용하지만 $k$의 개수를 지정하는 데에도 활용할 수 있다.먼저 주어진 특정 데이터 포인트 $i$에 대해서 두 개의 값을 계산한다.  Cohesion(응집도) : 소속 클러스터의 다른 데이터 포인트들까지의 평균 거리, $a(i)$ 라고 표시한다.  Separation(분리도) : 가장 가까운 외부 클러스터의 데이터 포인트들까지의 평균 거리, $b(i)$ 라고 표시한다.이를 활용하여 Silhouette Score를 계산한다. 지수는 $-1$과 $1$ 사이이며, $1$에 가까울수록 클러스터링 성능이 좋다고 평가한다.\\[s(i) = \\frac{b(i) - a(i)}{\\max{(a(i), b(i))}}\\]  $1$에 가까울수록, $a(i)$가 $b(i)$에 비해 훨씬 작다는 의미이므로 알맞게 클러스터링 됐다고 평가한다  $0$ 주변일수록, 해당 포인트가 두 클러스터의 경계 부분에 놓여 있다고 볼 수 있다  $-1$에 가까울수록, $a(i)$가 $b(i)$에 비해 크다는 의미이므로 해당 포인트가 다른 클러스터에 더 가까울 수 있음을 시사한다 (아예 잘못 클러스터링된 경우를 고려할 수 있다) 활용 Silhouette Score가 각 데이터 포인트에 대해 계산하는 값이기 때문에 이를 활용할 수 있는 방법은 다양하다.  모든 데이터 포인트에 대해 Silhouette Score를 각각 계산한 다음, 잘 클러스터링된/경계에 있는/잘못 클러스터링된 샘플들을 가려내기  Silhouette Score의 평균값을 클러스터별로 집계하고 비교하여 잘못 응집된 클러스터를 다른 클러스터와 합치거나 더 쪼개기  Silhouette Score의 전체 평균값을 집계하여 전반적인 클러스터링 성능을 평가하기          이때 클러스터 개수 $k$별로 평균 Silhouette Score을 비교하여 클러스터 개수를 지정할 수 있다      코드sklearn 라이브러리 활용sklearn.Cluster의 KMeans 모듈과 sklearn.metrics silhouette_score를 적용할 수 있다.from sklearn.cluster import KMeansfrom sklearn.metrics import silhouette_scorek = 3model = KMeans(n_clusters=k)pred = model.fit_predict(X)# k별로 해당 속성값을 시각화하면 elbow point를 찾을 수 있다print(model.inertia_) # 모든 데이터 포인트의 실루엣 스코어의 평균값을 출력한다metric = silhouette_score(X, pred)print(metric)입력 파라미터  n_clusters : 클러스터 개수  init : 초기화 기법 (디폴트는 k-means++)속성  cluster_centers_ : 각 클러스터 중심점의 좌표  inertia_ : 클러스터 내 제곱합(WCSS)Python 구현 (from scratch)거리 함수 (L2 Norm)import numpy as npdef euclidean(point, data):    return np.sqrt(np.sum((point - data)**2, axis=1))중심점 초기화 (Kmeans++)import randomk = 3# 최초 클러스터 중심점 1개 추출centroids = [random.choice(X)]for _ in range(k-1):    # 더 가까운 클러스터의 중심점까지 거리 계산    dists = np.min([euclidean(centroid, X) for centroid in centroids], axis=0)    dists = dists**2    dists /= np.sum(dists)    # 거리 기반의 확률로 추출    new_centroid_idx = np.random.choice(range(len(X)), size=1, p=dists)    centroids.append(X[new_centroid_idx[0]])목적함수 최적화 (iteration)max_iter = 100n_iter = 0prev_centroids = None# 중심점이 하나라도 변경되거나 max_iter 미만일 때 반복while np.not_equal(centroids, prev_centroids).any() and n_iter &lt; max_iter:    sorted_points = [[] for _ in range(k)]    # Assignment : 포인트마다 가장 가까운 클러스터 할당    for point in X:        dists = euclidean(point, centroids)        centroid_idx = np.argmin(dists)        sorted_points[centroid_idx].append(point)    prev_centroids = np.copy(centroids)    # Update : 클러스터별로 평균값 업데이트하기    centroids = []    for cluster in sorted_points:        if len(cluster) &gt; 0:            centroid = np.mean(cluster, axis=0)        else:            # 클러스터가 비어 있으면 업데이트 전 기존 평균값 사용            centroid = prev_centroid[len(centroids)]        centroids.append(centroid)    n_iter += 1실루엣 지수 (Silhouette Score)# i번째 데이터 포인트에 대한 점수 계산# Cohesion (a(i))own_cluster = labels[i]own_cluster_points = X[labels == own_cluster]if len(own_cluster_points) &gt; 1:    a_i = np.mean(euclidean(point, own_cluster_points))else:    a_i = 0# Separation (b(i))b_i = float('inf')for centroid_idx, centroid in enumerate(centroids):    if centroid_idx != own_cluster:        other_cluster_points = X[labels == centroid_idx]        if len(other_cluster_points) &gt; 0:            avg_dist = np.mean(euclidean(point, other_cluster_points))            b_i = min(b_i, avg_dist)score = (b_i - a_i) / max(a_i, b_i) if max(a_i, b_i) &gt; 0 else 0"
    } ,
  
    {
      "title"    : "Jekyll을 사용한 github.io 블로그 개발기",
      "tags"     : "JavaScript",
      "date"     : "2024-08-19",
      "url"  : "https://woocosmos.github.io/blog-history/",
      "content": "요약- 지금 이 블로그(woocosmos.github.io)를 구축한 전반적인 내용- Jekyll 설치를 위해 ruby 개발 환경을 세팅한다- 목적과 취향에 맞는 Jekyll 테마를 골라 fork 한다- HTML/CSS/JavaScript를 활용하여 다양한 기능을 추가, 수정한다개요티스토리에서 운영하던 개발 블로그를 github.io 로 이관하는 작업을 진행하고 있다.그 이유는 첫째, 티스토리에서 Markdown이 불안정하게 적용되기 때문이다. 둘째, HTML 및 JavaScript를 활용하여 자유자재로 커스터마이징할 수 있다는 점이 매력적으로 느껴졌기 때문이다.물론 github.io 블로그는 카테고리 설정이 까다로워 대부분 태그 기반이라는 점, 그리고 검색이나 목차와 같은 기능은 직접 구현해야 한다는 번거로움이 단점으로 작용한다. 그러나 이참에 개발 블로그를 정식으로 세팅하고 JavaScript를 직접 부딪치며 배워보는 기회로 여겨보려 한다.곧 게시물을 전부 이쪽으로 옮길 예정이지만 기존 티스토리 블로그도 열려 있으니 언제든 놀러오시라.기본 세팅github.io 블로그를 시작하기 위해서는 jekyll(지킬)을 세팅하는 것이 우선이다. Jekyll은 마크다운 언어로 작성한 텍스트를 정적 웹사이트로 생성해주는 변환 엔진이다. 한국어로 번역된 공식 문서를 이곳에서 확인할 수 있다.RubyJekyll은 Ruby 프로그램이기 때문에 Ruby를 먼저 설치해야 한다. 또한 Ruby의 라이브러리(즉, Gem)를 관리해주는 프레임워크인 RubyGems도 필요하다. OS에 따른 설치 방법은 공식 문서를 포함하여 다양한 칼럼에 소개되어 있으니 참고하면 되겠다.개인적으로 Windows(회사 컴퓨터)와 Mac(개인 노트북)에 각각 Ruby를 설치하면서 다양한 트러블슈팅을 경험했는데, 대개 Ruby의 버전 관리 프레임워크인 rbenv로 설치를 관리하면서 많은 문제를 회피할 수 있었다.  특히 Ruby의 버전이 3.0.0 이상이어야 한다는 에러를 가장 많이 부딪혔는데 rbenv로 원하는 버전으로 지정함으로써 문제에서 벗어날 수 있었다. 해당 블로그 프로젝트는 ruby-3.2.0으로 빌드했다.준비물의 순서로 정리하자면 rbenv → Ruby → gem → bundler → jekyll 이다.gem install jekyll 더하여 개발의 편의성을 위해 jekyll 로컬 서버를 띄우려면 github-pages를 설치하는 것이 좋다. 이를 활용하여 파일의 변경사항을 save할 때마다 바로 반영된 것을 확인할 수 있다.gem install github-pagesjekyll server --force_polling  별도 옵션을 주지 않는 한 http://127.0.0.1:4000로 접속한다  --force_polling : 해당 플래그 옵션으로 블로그를 새로고침해서 바로 변경된 내용을 확인할 수 있다. 이를 설정하지 않으면 매번 로컬 서버를 내렸다가 다시 올려야 한다.Jekyll 테마from scratch로 블로그를 구성하기보다는 특정 테마로 베이스 사이트를 세팅한 후에 기능을 추가하거나 변경하기로 했다. Jekyll 테마 사이트에서 목적과 취향에 맞는 테마를 골라보자.결론적으로 한국어 가독성을 고려한 Kiko Now를 기본으로 하되, Tale이나 Catbook 등 다양한 테마를 레퍼런스로 삼아 기능을 추가하는 방향으로 진행했다.이제 선택한 테마의 github 레포지토리로 이동하여 나의 레포지토리로 fork 해오면 된다. 이때 레포지토리 이름을 {username}.github.io로 설정하면 github에서 자동으로 해당 도메인으로 호스팅해준다. fork 직후에는 위 도메인으로 접속이 안 될 수도 있는데, 수 분 기다리거나 최소 하나의 변경 사항을 push 해주면 들어가진다.마지막으로 작업 폴더에서 git clone하여 로컬 레포지토리를 생성한다.config 수정대부분 Jekyll 테마에서 그렇듯 커스터마이징의 첫 단계는 _config.yml 파일을 수정하는 것이다. 블로그 이름, SNS 링크 등 기본적인 내용을 이곳에 입력하도록 되어 있다.이제 기본적인 세팅은 끝이다. 바로 MD 파일을 생성해서 포스트를 업로드할 수 있다. 개인적으로는 약간 번거롭더라도 복구 가능성을 위해 항상 git branch로 작업 후 master에 merge 하는 편이다.git checkout -b post/blog-historyvi _posts/2024-08-19-blog-history.md# 포스트 작성 후 ...git add .git commit -m '[post/init] 블로그 개발기'git push# master 에 반영git checkout mastergit merge post/blog-history브랜치 네임이나 커밋 메시지의 컨벤션은 스스로 아래와 같이 정했다.  post/… : 블로그글 관련 브랜치  feature/… : 블로그 기능 관련 브랜치  [post/init] : 블로그글 최초 배포 커밋  [post/modi] : 이후 블로그글 수정 커밋구글 검색 연동내용기능 추가이제부터 이어지는 내용은 기능을 추가한 히스토리를 기록한 것이다.JavaScript를 잘 모르다보니 Workaround 형식으로 구현한 내용도 많다. 개선 지점은 언제든 덧글이나 연락처로 알려주시면 감사하겠다.back-to-top 버튼 구현_includes/top.html에 인라인 JS로 구현되어 있다. 외부 라이브러리 없이 순수 JavaScript로 작성했다.  스크롤 위치를 감지하여 일정 이상 내려가면 우측 하단에 원형 버튼이 나타남  클릭 시 cosine easing 애니메이션으로 부드럽게 최상단으로 이동  레이아웃(default.html, page.html, post.html)에서 {% include top.html %}로 삽입검색 페이지 추가검색 기능Simple-Jekyll-Search 오픈소스를 활용했다.  Jekyll 빌드 시 search.json에 전체 포스트의 제목, 태그, 날짜, 본문을 JSON으로 생성  search-and-return.js에서 SimpleJekyllSearch를 초기화하고, 검색창 입력 시 JSON을 대상으로 클라이언트 사이드 검색 수행  검색 결과는 제목, 날짜, 태그, 본문 미리보기로 구성검색 결과 하이라이트search-and-return.js의 templateMiddleware에서 구현했다.  검색어를 정규식으로 변환하여 제목, 태그, 본문에서 매칭되는 부분을 &lt;b style=\"background:gold\"&gt; 로 강조 표시  본문의 경우 매칭 위치 앞뒤 15단어만 잘라서 표시하고, 본문 내 총 언급 횟수를 함께 출력  태그는 각각 태그 페이지로의 링크를 포함LaTex(수학 수식) 적용하기LaTex를 렌더링해주는 KaTex 를 _includes/head.html의 head 부분에 추가한다.  &lt;link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css\"&gt;  &lt;script defer src=\"https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.js\"&gt;&lt;/script&gt;  &lt;script defer src=\"https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/contrib/auto-render.min.js\"          onload=\"renderMathInElement(document.body, {            delimiters: [              {left: '\\\\[', right: '\\\\]', display: true},              {left: '$$', right: '$$', display: true},              {left: '$', right: '$', display: false},            ]          });\"&gt;  &lt;/script&gt;  delimiter를 명시한 이유는 inline LaTex를 제대로 인식하지 못했던 문제 때문이다  명시한 이후에도 display LaTex가 적용되지 않는 문제가 있었다 : $$로 감싸도 \\[\\] 로 출력되었다 (사실 이것이 표준 display LaTex notation이라고 한다) 그래서 delimiter 항목으로 더 추가했다이제 LaTex 문법대로 수식을 $ 기호 1개 혹은 2개 사이에 작성하면 알맞게 렌더링 된다display LaTex\\[\\sum_{i=1}^{k} \\sum_{\\mathbf{x} \\in S_{i}} \\left\\|\\mathbf{x} - \\boldsymbol{\\mu}_{i}\\right\\|^{2}\\]$$\\sum_{i=1}^{k} \\sum_{\\mathbf{x} \\in S_{i}} \\left\\|\\mathbf{x} - \\boldsymbol{\\mu}_{i}\\right\\|^{2}$$  inline LaTex문장 중간에 이렇게 $\\sum_{\\mathbf {x} \\in S_{i}}\\mathbf {x}$ 넣을 수 있다문장 중간에 이런 수식을 $\\sum_{\\mathbf {x} \\in S_{i}}\\mathbf {x}$ 넣을 수 있다목차(TOC) 추가블로그글 옆 사이드바 형식의 목차를 추가한다. 아래는 플러그인을 설치해서 사용할 수 있는 jekyll-toc을 적용한 내용이다.동일한 이름의 플러그인이 있는데, 후술할 Github Pages 이슈로부터 자유로운 것으로 보인다. 처음으로 돌아간다면 이것을 적용해볼지도…설치 방법Gemfile에 아래 라인을 추가한다gem 'jekyll-toc'bundle로 설치를 진행한다bundle install_config.yml 파일 중 플러그인 부분에 요소를 추가한다plugins:  - jekyll-sitemap    ...  - jekyll-toc # 추가사용 방법post 헤드에 toc 플래그를 추가한다---layout: posttitle: \"Jekyll을 사용한 github.io 블로그 개발기\"tags: [JavaScript]comments: Truetoc: true---post.html에 toc을 추가한다.단순히 {{ content | toc }}로 수정해서 본문 위에 목차가 생성되도록 하는 방법도 있지만 목차의 레이아웃이나 기능을 다양하게 커스터마이징 하기 위해 별도 태그인 {% toc %}로 추가했다.&lt;section class=\"entry\"&gt;    {% if page.toc %}    &lt;aside&gt;        &lt;nav class=\"nav-toc\"&gt;            &lt;h3&gt; 목차 &lt;/h3&gt;            {% toc %}        &lt;/nav&gt;    &lt;/aside&gt;    &lt;script src=\"{{ site.baseurl }}/assets/scroll-spy.js\" type=\"text/javascript\"&gt;&lt;/script&gt;    {% endif %}    {{ content }}&lt;/section&gt;  {% if page.toc %} : 헤드에 toc 플래그를 명시한 경우에만 목차가 추가되게 했다.  &lt;aside&gt; : 본문 옆 사이드바 형식으로 표시하기 위해 사용했다  &lt;nav&gt; : 목차의 제목을 클릭했을 때 해당 영역으로 이동하도록 링크를 연결하기 위해 사용했다  &lt;h3&gt; 목차 &lt;/h3&gt; : 플러그인으로 자동 생성되는 HTML에는 제목이 없길래 따로 추가해주었다 (사실 page.toc 조건을 굳이 넣은 것도 이 제목 때문이다. toc: false으로 세팅해도 h3 태그는 남아 있었기 때문이다.)목차 레이아웃목차의 위치와 모양을 세팅하는 과정이다.목차를 본문 좌측에 맞추고 스크롤과 상관없이 상단에 고정시키기aside {    float: right;    position: sticky;    width: fit-content;    top: 10px;    margin-right: -300px;  }  position을 sticky로 설정하고 top값을 조금이라도 부여하면 스크롤과 상관없이 화면 한 쪽에 고정되는 효과를 구현할 수 있다  margin-right를 조정해서 본문과 너무 멀지도, 가깝지도 않게 위치시켰다레이아웃을 심플하게 디자인하기.nav-toc {  font-size: smaller;  border-left: 1px solid $lightGray;  h3 {    padding-left: 20px;  }  ul &gt; li {    list-style-type: none;     &amp;:before {      content: '';    }    ul {      display: inline;    }  }  ul &gt; li &gt; a.active {    font-size: larger;    font-weight: bold;  }}  기본적으로 폰트 사이즈는 작게, 리스트 앞에 붙는 마커는 생략했다          list-style-type: none를 주었는데도 마커가 생성되어 &amp;:before {content: '';}를 별도로 추가했다        목차와 본문 사이 가는 구분선을 추가했다 (border-left)  일부 하위 목차들이 가로로 나열되는(?) 이상한 현상이 있어서 display: inline을 추가했다  목차가 하이라이트 대상일 때 폰트 사이즈와 굵기를 조금 키운다목차 하이라이트 기능스크롤의 위치에 따라 현재 보고 있는 콘텐츠의 목차를 하이라이트하는 기능이다. 위에서 toc 태그를 추가한 HTML 코드를 보면 scroll-spy.js라는 스크립트를 실행시키는 것을 볼 수 있는데, 이것이 스크롤 위치에 따라 하이라이트할 목차를 지정하는 역할을 한다.전체 코드를 살펴보겠다.// 브라우저가 HTML을 전부 읽고 DOM 트리를 완성했을 때 발생하는 이벤트document.addEventListener('DOMContentLoaded', () =&gt; {    // links : H1, H2, H3 깊이 까지만 목차를 읽어온다 (a 태그 셀렉트)    const Hs = document.querySelectorAll('.nav-toc ul.section-nav li.toc-entry.toc-h1, .nav-toc ul.section-nav li.toc-entry.toc-h2, .nav-toc ul.section-nav li.toc-entry.toc-h3');    const links = Array.from(Hs).map(h =&gt; { return h.querySelector('a') })        // anchors : links 의 각 요소로부터 href 를 읽어온다    const anchors = Array.from(links).map(link =&gt; {        const href = link.getAttribute('href');        if (href) {            return document.querySelector(href);        }        return null;    }).filter(anchor =&gt; anchor !== null);    // 스크롤 발생시    window.addEventListener('scroll', () =&gt; {        if (anchors.length &gt; 0 &amp;&amp; links.length &gt; 0) {            let scrollTop = window.scrollY;            let activeIndex = -1;            // 스크롤 위치와 제목의 위치가 가까울 경우 (격차가 300 이하)            // 활성화할 제목의 인덱스를 저장한다            anchors.forEach((anchor, i) =&gt; {                if (scrollTop &gt;= anchor.offsetTop - 300) {                activeIndex = i;                 }            });            // 나머지 제목은 비활성화 한다            links.forEach((link) =&gt; {                link.classList.remove('active');            });                        // 인덱스가 유효하면 제목을 활성화한다            if (activeIndex &gt;= 0) {                links[activeIndex].classList.add('active');            }        }  });});코드의 동작 원리는 주석을 참고하면 된다.개인적으로 까다로웠던 점은 특정 깊이(h3)까지만 목차를 읽어오는 것이었다. 모든 제목이 하이라이트되는 것을 원하지 않았고 h4 이상부터는 상위 제목을 하이라이트하는 게 목차로서 의미가 있다고 판단했다.querySelectorAll를 사용해서 모든 제목의 a 태그를 바로 긁어올 수 있지만 특정 깊이까지만 읽어오기 위해서 H1부터 H3까지 직접 지정해서 읽어온 다음 그 안에서 a 태그를 가져오게 했다. 그 뿐만 아니라 숫자로 시작하는 제목( 예를 들면 … )은 href를 읽어올 때 에러가 발생했기 때문에 애초에 지정한 만큼만 읽고 그 안에서 파싱하는 방식이 가장 깔끔하다고 생각했다.또 지속적으로 마주했던 에러는 links와 anchors 변수가 빈 배열을 반환하는 문제였는데, 이는 DOMContentLoaded 이벤트를 조건으로 추가함으로써 해결했다.하여, 나만의 sticky highlighted TOC 이 완성되었다gh-pages 생성그러나 TOC 기능을 추가한 후 Github Pages에서 빌드/배포 실패가 떴다. 분명 로컬 서버에서는 잘 돌아갔는데 말이다.에러 메시지에 ‘Unknown tag toc’이라고 적힌 것으로 보아 Jekyll-toc 플러그인 쪽 문제로 보였다.실제로 Jekyll-toc 레포지토리의 이슈 채널에서 동일한 문제를 호소하는 글들을 확인할 수 있었다.  Is Github Pages not supported? #151  TOC on GitHub Pages #29  Is Github Pages not supported? #151  GitHub Pages cannot build sites using unsupported plugins. If you want to use unsupported plugins, generate your site locally and then push your site’s static files to GitHub.이는 Github Pages 서비스에서 내가 사용한 toc 플러그인을 지원하지 않아 발생한 문제였다.따라서 로컬로 사이트를 직접 빌드한 후 해당 내용을 배포하도록 하는 방법으로 문제를 해결할 수 있다. 이때 gh-pages라는 브랜치로 밀어넣고 root 경로로 설정해야 한다. 참고한 칼럼은 여기.# 로컬에서 빌드한다jekyll build# _site 폴더를 어딘가로 대피시킨다mv -r _site /path/to/tmpgit checkout --orphan gh-pages # 폴더를 비우고 _site 데이터를 다시 가져온다rm -rf * cp -r /path/to/tmp/_site/* ./git add -Agit commit -m \"build locally and create gh-pages\"git push origin gh-pages브랜치를 생성할 때 --orphan 옵션을 추가한 이유는 부모(master)로부터 커밋 히스토리를 이어 받지 않은 독립적인 브랜치를 새로 만들기 위함이다. 마치 레포지토리 안에 새로운 레포지토리를 시작한다고 생각할 수 있겠다.마지막으로는 레포지토리에서 Settings &gt; Pages &gt; Source 그리고 Branch를 gh-pages로 수정하면 된다문제는 이제 앞으로 변경사항을 반영할 때마다 _site 의 내용을 매번 옮겨놨다가 다른 데이터를 삭제하는 식으로 업데이트 해야 한다는 것이다. 위 과정은 향후 Github Actions를 활용해 자동화된 workflow로 구축할 예정이다."
    } ,
  
    {
      "title"    : "아이폰 iOS앱 개발 체험하기: 환경 세팅과 시뮬레이션",
      "tags"     : "Swift",
      "date"     : "2024-08-16",
      "url"  : "https://woocosmos.github.io/swift-start/",
      "content": "요약- Xcode 개발 환경을 세팅하고 Figma에서 직접 디자인한다- 버튼을 누르면 콘솔에 텍스트가 출력되는 간단한 기능을 구현한다- 개인 디바이스를 통해 App을 시뮬레이션한다  개요광복절 + 코로나 자가격리 연휴를 맞이하여 간단하게 iOS 앱 개발을 경험해보려고 한다. 언젠가 피트니스 분야의 앱을 만들어 배포해보는 게 목표이기도 하고, 애플 개발 생태계에 대해서도 늘 궁금했기 때문이다.오늘은 iOS 앱 개발의 ‘체험’ 수준으로, 버튼을 누르면 콘솔에 Hello, World!를 출력하는 매우 단순한 기능을 구현할 것이다. 그로써 개발 도구와 디자인 툴, 디바이스의 상호 작용을 개괄적으로나마 이해할 수 있을 것이다.개발 환경  MacBook Air M1          Xcode : 맥북 앱스토어에서 설치      Figma : 공식 사이트에서 데스크톱 용을 다운받아 설치        iPhone 15 Pro (시뮬레이션용)          맥북과 유선 연결되어야 함      개발 과정Xcode 세팅1. 플랫폼 선택처음 Xcode를 설치한 후 실행하면 어떤 플랫폼의 앱을 개발할지 고른다.개인적으로 애플워치 앱을 개발할 계획이 있기 때문에 iOS와 watchOS를 선택했다.2. 프로젝트 템플릿 선택신규 프로젝트를 생성한 뒤 프로젝트의 템플릿은 ‘App’으로 진행한다3. 프로젝트 정보 입력  Product Name : 앱 이름  Team : 애플 계정을 추가한 다음 선택하면 된다  Organization Identifier : 앱 식별을 위한 문자열  Storage : iCloud 관련으로 보이는데 우선 None 으로 그대로 두고 다음 단계로 넘어갔다... 키체인을 사용하고자 합니다 에러프로젝트가 세팅되는 동안 이런 팝업이 떴는데, 나는 키체인 암호를 등록한 적이 없다.원인을 정확하기 이해하기 어려우나 이곳 칼럼에서는 인증서를 시스템 키체인에서 로그인 키체인으로 옮기는 방식을 제안하고 있다.우선 나는 맥북(cmd+space) &gt; ‘키체인 접근’에서 Apple Development 에 등록된 키들에 대해 정보 가져오기 &gt; 접근 제어 &gt; 모든 응용 프로그램이 이 항목에 접근할 수 있도록 허용 으로 변경해주었다. 다음 단계로 넘어가기 위한 임시방편으로 보안상 불안정해보이니 본격 앱 프로젝트 진행할 때는 원인을 제대로 파악하고 짚고 넘어가야 할 듯.참고로 키체인 암호를 별도로 등록한 적이 없다면 아무것도 입력하지 않거나(null) 맥 암호를 입력하면 팝업을 지나갈 수 있다Fimga 디자인피그마는 무료로 사용할 수 있는 대신 제공되는 기능이 꽤 제한된다. 무료 이용권(starter)의 경우,  프로젝트 하나당 파일 세 개  제한된 페이지 수  최대 30일까지 히스토리 추적1. 신규 프로젝트피그마에서 신규 파일(프로젝트)를 시작한다.2. 프레임 생성하단 Frame 을 눌러 원하는 사이즈 - 나의 경우 아이폰 15 Pro - 로 프레임을 생성한다3. 버튼 디자인조악하지만 이미지를 포함하여 간단한 버튼을 디자인 해보았다.피그마 사용법은 이 글에서 자세히 다루는 주제가 아니므로 자세한 과정은 생략하겠다.(파워포인트 잘 다루는 사람이라면 처음이라도 눈치껏 잘 할 수 있을 것 같다)4. 디자인 내보내기우측 탭 맨 아래 Export 에서 png 혹은 svg 파일로 내보내기 한다. 이때 파일명은 ButtonImage.png로 설정했다.버튼 기능 구현다시 Xcode 로 돌아와 버튼 기능을 구현하고 Figma에서 제작한 디자인에 적용할 차례이다디자인 가져오기내보내기 한 png 파일을 프로젝트의 Assets에 끌어다 놓는다.기능 구현하기Xcode에서 프로젝트를 새로 시작하면 ContentView에 기본 스크립트가 작성되어 있다.////  ContentView.swift//  HelloWorldApp////  Created by yunsoowoo on 8/16/24.//import SwiftUIstruct ContentView: View {    var body: some View {        VStack {            Image(systemName: \"globe\")                .imageScale(.large)                .foregroundStyle(.tint)            Text(\"Hello, world!\")        }        .padding()    }}#Preview {    ContentView()}이 부분을 아래 코드로 덮어썼다. 앞서 내보내기할 때 설정한 파일명(ButtonImage)만 코드상에 적용해주면 된다.import SwiftUIstruct ContentView: View {    var body: some View {        VStack {            Button(action: {                print(\"Hello, World!\")            }) {                Image(\"ButtonImage\")                    .resizable()            }        }    }}struct ContentView_Previews: PreviewProvider {    static var previews: some View {        ContentView()    }}코드를 수정하고 얼마 있다가 미리보기 뷰어에 화면이 나타났다.시뮬레이션스크립트를 실행하기 전, 시뮬레이션할 디바이스에서 설정해야 할 항목이 있다. 아이폰 설정 &gt; 개인정보 보호 및 보안 &gt; 개발자 모드를 ‘켬’으로 바꾸는 것이다. (이때 한 번의 부팅 과정이 필요하다)상단에서 프로젝트를 실행할 디바이스를 선택할 수 있는데, 개발자 모드로 잘 설정해두었다면 Management Run Destinations … 에서 유선 연결한 나의 휴대폰을 확인할 수 있었다.이렇게 나의 iPhone으로 선택한 후, 실행 (RUN)신뢰하지 않는 개발자 에러스크립트를 처음 빌드하고 실행한다면 디바이스와 Xcode 에서 이러한 에러를 마주할 것이다.    이는 아이폰 설정 &gt; VPN 및 기기 관리 &gt; 해당 앱 &gt; ‘신뢰’ 를 체크하는 것으로 해결 된다. 성공 휴대폰에 HelloWorldApp이 생성되었고, 그것을 실행할 수 있게 되었다.또한 목표했던 대로 이미지를 클릭하면 콘솔에 ‘Hello, World!’가 출력된다.마무리지금까지 매우 간단한 앱을 만들어 시뮬레이션 하는 작업을 진행해보았다.이를 통해 개발자 도구와 디바이스[destination]가 어떻게 상호작용하는지 배울 수 있었다.앞으로는  여러 페이지를 구현하고  입출력에 따라 페이지 간의 이동, 동작을 구현하고  파일 형식이 되든 앱스토어가 되든 앱을 배포하는… 등등의 과정을 차근차근 해내가려고 한다.간단히 Xcode 만져보겠다고 시작한 조촐한 블로그글였는데, 한 시간 이내의 짧은 시간 동안 많은 것을 배웠고 성취감도 느껴진다.ChatGPT에 많은 것을 의존하며 진행했지만 앞으로 본격적으로 커스터마이징하며 나만의 앱을 구현해보겠다.끝!"
    } ,
  
    {
      "title"    : "PyTorch의 확률분포 클래스 Categorical",
      "tags"     : "PyTorch",
      "date"     : "2024-05-16",
      "url"  : "https://woocosmos.github.io/pytorch-categorical/",
      "content": "요약- 분포값(softmax❌ 정규화❌)을 인자로 주어 Categorical 클래스 객체를 생성하면 파라미터에 따라 정규화된다- 입력값이 probs 일 경우 log을 적용하여 logits 속성을, logits일 경우 softmax를 적용하여 probs 속성을 저장한다- sample() 메소드를 사용하여 주어진 확률분포 기반으로 샘플링 할 수 있으며, log_prob() 메소드는 샘플된 값에 해당하는 logits를 리턴한다공식 문서와 공식 소스를 참고하여 작성Categoricalfrom torch.distributions.categorical import Categoricalprobs 또는 logits 를 파라미터로 받아 확률분포를 구성하는 클래스.  logits : unnormalized log probabilities객체softmax를 거치지 않은 raw output 배치 샘플을 준비한다. 4개 클래스에 대한 확률값이고 배치 사이즈는 2이다.# torch.Size([2, 1, 4])raw_output = torch.tensor(            [[[0.25, 0.25, 0.25, 0.25,]],             [[0.33, 0.33, 0.33, 0.01]]]            )Categorical 클래스를 사용하여 두 개 확률분포 객체를 생성한다. 하나는 probs로, 하나는 logits로 파라미터화한다. 명시하지 않는 경우 default는 prob이다.dist1 = Categorical(probs=raw_output)dist2 = Categorical(logits=raw_output)클래스 내부적으로 input을 정규화(normalize) 하는 과정이 포함되어 있다. logits의 경우 log-sum-exp 공식을 활용한다.def __init__ ...    # dist1    if probs is not None:        self.probs = probs / probs.sum(-1, keep_dim=True)    # dist2    else:        self.logits = logits - logits.logsumexp(dim=-1, keepdim=True)속성내부적으로 probs, logits 속성을 계산하는데, 파라미터에 따라 다른 모듈을 사용한다.  logits 입력 =&gt; probs 속성 계산    # logits_to_probsF.softmax(logits, dim=-1)        probs 입력 =&gt; logits 속성 계산    # probs_to_logitps_clamped = clamp_probs(probs) # eps ~ 1-eps 사이의 값으로 클립핑torch.log(ps_clamped)      정리하자면,  probs 파라미터 입력          probs 속성 : 입력값이 이미 정규화된 상태였기 때문에 동일하게 출력      logits 속성 : probs_to_logit 모듈을 적용한 결과 출력          dist1.probs  # tensor([[[0.2500, 0.2500, 0.2500, 0.2500]],  #        [[0.3300, 0.3300, 0.3300, 0.0100]]])      dist1.logits  # tensor([[[-1.3863, -1.3863, -1.3863, -1.3863]],  #        [[-1.1087, -1.1087, -1.1087, -4.6052]]])        logits 파라미터 입력          probs 속성 : logits_to_probs 모듈을 적용한 결과 출력      logits 속성 : log-sum-exp 공식으로 정규화된 결과          dist2.probs  # tensor([[[0.2500, 0.2500, 0.2500, 0.2500]],  #        [[0.2684, 0.2684, 0.2684, 0.1949]]])      dist2.logits  # tensor([[[-1.3863, -1.3863, -1.3863, -1.3863]],  #        [[-1.3154, -1.3154, -1.3154, -1.6354]]])      이들 속성은 @lazy_property로 값이 계산되기 때문에 호출하기 전까지 계산되지 않는다. 예를 들어 dist1.logits 을 호출하기 전까지 내부적으로 logits==None 으로 유지된다.샘플링sample 함수는 확률분포를 기반으로 표본을 샘플링할 수 있다. torch.multinomial으로 계산하는 것과 동일.s1 = dist1.sample() # [[1], [1]]s2 = dist2.sample() # [[2], [0]]이렇게 샘플링된 값에 해당하는 logits을 추적할 수도 있다. logits 속성에서 인덱싱해온 결과로 보면 된다.dist1.log_prob(s1) # [[-1.3863], [-1.1087]]dist2.log_prob(s2) # [[-1.3863], [-1.3154]]"
    } ,
  
    {
      "title"    : "PyTorch 파이토치 기초 모음집",
      "tags"     : "PyTorch",
      "date"     : "2024-01-18",
      "url"  : "https://woocosmos.github.io/pytorch-basic/",
      "content": "PyTorch의 구성 요소와 동작 원리를 살펴본다.개발 환경Windows  파이썬과 패키지툴을 준비한다          python 3.8 : 공식 문서에 따르면 현재2024-09-03 윈도우에서 파이토치는 파이썬 3.8-3.11 버전만 지원된다. 나는 파이썬 버전 관리를 위해 윈도우용 pyenv인 pyenv-win를 사용했다      pipenv : 파이썬에서 공식으로 권장하는 패키지 관리툴        프로젝트 폴더에 접근하여 pipenv 의 파이썬 버전과 가상환경을 설정한다    cd /path/to/projectpipenv --python 3.8pipenv shell        생성된 Pipfile파일에 파이토치 설치를 위한 주소를 추가한다.    [[source]]url = \"https://download.pytorch.org/whl/cu118\"verify_ssl = truename = \"pytorch\"        파이토치를 설치한다    pipenv install --index=pytorch torch          Ubuntu (WSL)  miniconda 로 파이썬 버전을 명시한 새 환경을 생성한다    conda create -n my-env python==3.8.2        PyTorch 및 CUDA를 설치한다    conda install cudatoolkit=11.8 -c conda-forgeconda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia          테스트pipenv shell 혹은 conda activate my-env 으로 가상 환경에 접근한 다음, 파이썬으로 아래 요소들을 호출한다.import torchtorch.cuda.is_available()torch.cuda.device_count()torch.cuda.current_device()torch.cuda.device(0)torch.cuda.get_device_name(0)torch.__version__ torch.version.cudaTensor(텐서)Tensor는 모델의 파라미터와 입출력을 인코딩할 때 사용하는, 배열 및 행렬과 매우 유사한 자료구조이다. list나 numpy array로부터 텐서를 생성할 수 있다.# 원본 데이터my_list = [[1, 2, 3], [4, 5, 6]]my_arr = np.array(my_list) # 방법1. tensor() 함수t1 = torch.tensor(my_arr) # 방법2. from_numpy() 함수t2 = torch.from_numpy(my_arr) # 방법3. as_tensor() 함수t3 = torch.as_tensor(my_arr)전달된 배열을 sharing 하는 from_numpy(), as_tensor() 과 다르게, tensor()은 배열을 복제(copy)한다. 따라서 원본 배열의 요소를 수정하여도 tensor()로 변환된 텐서는 영향을 받지 않는다.my_arr[0, 0] = 999 print(t1)   # tensor([[1, 2, 3], [4, 5, 6]])print(t2)   # tensor([[999, 2, 3], [4, 5, 6]])print(t3)   # tensor([[999, 2, 3], [4, 5, 6]])별도 인자를 주어 dtype을 오버라이딩할 수 있다. t1.dtype을 출력하여 비교할 수 있다. 단, from_numpy는 이런 기능이 없다.t1 = torch.tensor(my_list, dtype=torch.float)텐서를 생성할 때 shape(size)를 지정할 수 있다.shape = (2,3,)rand_tensor = torch.rand(shape)ones_tensor = torch.ones(shape)zeros_tensor = torch.zeros(shape)✅ torch.Tensor vs. torch.tensortorch.Tensor 는 클래스(class)다. 따라서 모든 텐서는 torch.Tensor 클래스의 객체이다. 반면 torch.tensor() 는 텐서를 반환하는 함수다.각각을 인자 없이 호출할 경우, 전자는 빈 텐서 객체로 생성되지만 후자는 변환할 배열 즉 data를 인자로 받지 못해 에러가 발생한다.torch.Tensor() # tensor([])torch.tensor() # TypeError: tensor() missing 1 required positional arguments: \"data\"PyTorch의 텐서는 CPU에 생성되는 것이 디폴트지만 GPU에 올려 연산할 수 있다. GPU의 RAM에 저장된다는 의미다.t1 = torch.FloatTensor([0., 1., 2.])t1.is_cuda  #False텐서를 GPU에 올리는 두 가지 방법이 있다.  처음부터 GPU에서 텐서를 생성한다    gpu_tensor = torch.tensor([[1, 2], [3, 4]], device='cuda')        CPU에서 생성된 텐서를 GPU로 복사한다. 한번 GPU에 복사된 텐서는 계속해서 GPU에서 연산된다.    gpu_tensor = cpu_tensor.to(device='cuda')        다시 CPU로 돌아가게 만드는 방법도 있다.    back_to_cpu = gpu_tensor.to(dvice='cpu')    # 모델을 GPU에 올릴 때에도 사용back_to_cpu = gpu_tensor.cpu()          ✅ CUDA Caching Memory Allocator  파이토치는 tensor를 GPU에 올릴 때 메모리를 빠르게 할당하기 위해 caching allocator를 사용한다. allocator는 CUDA로부터 메모리 블럭들을 요청한 후, 블럭을 쪼개고 (CUDA에 반환하지 않은 채로) 재사용한다. 따라서 tensor가 지워져도 allocator는 메모리를 keep해둔다. 결국에 메모리가 실제로는 사용되고 있지 않더라도 이 allocator에 의해 차지되어 사용 중인 것으로 표기될 수 있다.# 파이토치에서 실제로 사용하고 있는 메모리를 확인하기 (1024로 두 번 나누어주어야 MB 단위가 됨)torch.cuda.memory_allocated()# 실제로는 사용하지 않는 메모리를 반환시키기torch.cuda.empty_cache() ✅ in-place operations언더스코어(_)로 끝나는 연산 함수는 텐서 변수를 in-place로 변환한다. 메모리를 아끼는 데 도움이 될 수 있지만 derivative를 계산할 때 문제가 될 수 있으므로 사용을 권장하지는 않는다.t1 = torch.rand(2,3,)t1.add_(5)Autograd (자동미분)Autograd는 역전파(back propagation)를 시행하여 체인룰(chain rule)에 따라 말단 노드(leaf)까지 변화도(=gradient)를 연산하는 기능이다.  Forward  loss function의 값을 계산하는 것. 레이어의 output이 다음 레이어의 input으로 전달됨으로써 연산된다.  Backward  학습 파라미터의 gradient를 계산하는 것. chain rule를 기반으로, 파라미터가 loss fuction에 기여한 가중치를 연산한다.자동미분은 뿌리 노드가 계산되기까지 사용된 모든 변수의 미분값, 즉 ‘history’ 를 그래프 형태로 저장함으로써 동작한다. 이때 그래프는 방향이 있는 비순환 그래프(DAG, directed acyclic graph) 이며 학습 iteration 마다 새로 구성된다. (c.f. tensorflow의 경우 Static Computational Graphs 를 사용한다)예를 들어 아래 연산 과정을 그래프로 나타낼 수 있다.A = torch.tensor([10.], requires_grad=True)B = torch.tensor([20.], requires_grad=True)F = A * BG = F * 2이렇듯 역전파는 그래프를 거꾸로 거슬러 올라가며 G가 계산되기까지 A, B의 gradient를 계산하는 과정이다.✅ Leaf Tensor &amp; gradient텐서는 두 가지 조건에 따라 분류할 수 있다.  그래디언트 계산이 필요한가? (requires gradient)  연산의 결과인가? (explicitly created by the user, 즉 사용자가 명시적으로 생성한)  “그래디언트 계산이 필요한가?” 는 결국 “상수(constant)인가 변수(variable)인가?”라는 질문과 같다이는 텐서의 두 가지 속성과 관련되는 것이다.  requires_grad : boolean으로 표현  grad_fn : 연산에 사용된 함수(ex. Add, Mul, …)  유저가 생성했거나 (=연산의 결과가 아님) 그래디언트 계산이 필요하지 않으면, grad_fn는 값이 없다(None)user_created_tensor = torch.tensor([10.], requires_grad=True)   # 텐서를 초기화하면서 gradient 계산이 필요하다고 설정하였습니다print(user_created_tensor.requires_grad)                        # Trueprint(user_created_tensor.grad_fn)                              # None calcuated_tensor = user_created_tensor*2                        # 이 텐서는 곱셈 연산의 결과입니다print(calcuated_tensor.requires_grad)                           # Trueprint(calcuated_tensor.grad_fn)                                 # &lt;MulBackward0 object at 0x7fc5d71a5df0&gt;이처럼 텐서는 조건에 따라 leaf tensor인지, gradient를 저장할지(populated) 결정된다.  Grad Populated : 해당 텐서에 대한 gradient 저장. backward 후 grad 속성이 존재.            case      requires_grad      grad_fn      is_leaf      grad                  gradient 계산이 필요하고, 유저가 생성하였다      True      None      True      True              gradient 계산이 필요하고, 연산의 결과이다      True      not None      False      False              gradient 계산이 필요하지 않고, 유저가 생성하였다      False      None      True      False              gradient 계산이 필요하지 않고, 연산의 결과이다      False      None      True      False      leaf는 그래프 상 자식이 없는 말단 노드를 말한다. 파이토치에서 gradient 계산이 필요하지 않은 텐서는 모두 leaf tensor로 여긴다. 중요한 것은 ‘gradient 계산이 필요하고 유저가 명시적으로 생성한 텐서’를 leaf tensor로 여긴다는 것이다.grad는 누적 계산된 gradient다. gradient 계산이 필요한 leaf tensor에 대해 누적 연산한 결과다. 이때 어떤 tensor가 gradient 계산이 필요하더라도 연산의 결과라면 non-leaf tensor로 여겨 grad가 저장되지 않는다.print(user_created_tensor.is_leaf)  # Trueprint(calcuated_tensor.is_leaf)     # False # gradient 계산하여 grad 가 저장되는지 확인calcuated_tensor.backward()print(user_created_tensor.grad)     # tensor([2.])print(calcuated_tensor.grad)        # None (UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed)  왜 이러한 개념들을 알아야 하는가? 자동 미분은 어떤 변수가 계산되는 데 사용된 모든 변수의 미분값, 즉 history를 computational graph 형태로 저장함으로써 동작하기 때문이다. requires_grad=True 옵션으로 표시해야만 처음 연산이 시작된 곳까지 거슬러 올라가며 그 변수의 연산들을 역추적한다.참고 // 텐서에 저장된 gradient 관련 속성을 확인하는 코드.def check_attr(t : torch.tensor):    print(t.requires_grad)    print(t.grad_fn)    print(t.is_leaf)    print(t.grad)✅ requires_gradrequires_grad를 통해 어떤 텐서에 대해 gradient 계산이 필요한지 설정할 수 있다.# 방법1. 변수 초기화할 때 인자로 표시하기t1 = torch.randn((3, 3), requires_grad=True) # 방법2. 변수를 생성한 후에 속성 바꾸기t2 = torch.randn((3, 3))t2.requires_grad = Truet2.requires_grad_(True)requires_grad는 전염된다. 연산에 사용된 텐서 중 하나라도 requires_grad 가 True로 설정되어 있다면, 그 연산 결과의 requires_grad도 True이다# 두 텐서 중 하나만 requires_grad=True로 설정한다t1 = torch.randn((3, 3))t2 = torch.randn((3, 3))t1.requires_grad = True # 연산 결과가 requires_grad=True로 나타난다t3 = t1 @ t2print(t3.requires_grad)     # True이러한 특성을 활용해 layer freezing과 같은 기법을 구사할 수 있다. 이미지는 b 의 requires_grad를 false로 세팅한 경우.✅ torch.no_grad()모델로부터 값을 추론(inference)할 때는 그래프를 생성하고 gradient를 계산하는 과정이 필요없기 때문에 context manager를 활용해 메모리를 아낄 수 있다.with torch.no_grad():    ...eval()과는 무슨 차이?eval 함수는 train과 evaluation 시 다르게 동작하는 layer들 (ex. Dropout, BatchNorm) 을 eval 모드로 바꿔준다. 따라서 모델을 평가할 때는 no_grad와 eval을 모두 사용하는 것이 옳다.model.eval()with torch.no_grad():   ... ✅ backward()지금까지의 개념을 토대로 역전파 과정을 유사코드로 구현해본다. graph 를 거꾸로 거슬러 올라가며 recursive하게 반복하다가, leaf node에 다다르면 grad_fn가 None이기 때문에 중단된다.def backward(gradients):    '''self.Tensor : 역전파 연산의 대상이 되는 텐서(=loss)'''    self.Tensor.grad = gradients         for inp in self.inputs:        if inp.grad_fn is not None:            new_gradients = gradients * local_grad(self.Tensor, inp)            inp.grad_fn.backward(new_gradients)        else:            pass한편 backward() 함수는 scalar 텐서에만 작동한다. vector 텐서에 부를 경우 이러한 에러를 마주하게 된다.  RuntimeError: grad can be implicitly created only for scalar outputs만약 벡터 텐서에 대해 역전파를 수행하고 싶다면 Jacobian Matix를 활용하거나, 사이즈에 맞는 1값 텐서를 입력으로 주어 처리할 수 있다.G.backward(torch.ones(G.shape))예제앞서 살펴본 연산식을 활용해 backward 함수를 호출하고 grad 속성을 확인해본다.a = torch.tensor([10.], requires_grad=True)b = torch.tensor([20.], requires_grad=True)F = a * bG = F * 2            case      node                  gradient 계산이 필요하고, 유저가 생성하였다      A, B              gradient 계산이 필요하고, 연산의 결과이다      F, G              gradient 계산이 필요하지 않고, 유저가 생성하였다      -              gradient 계산이 필요하지 않고, 연산의 결과이다      -      여기서 최적화하고자 하는 대상은 G이다. chain rule에 따라 backward 하여 G가 출력되기까지 a, b의 gradient를 계산한다.F 또한 requires_grad=True를 상속 받기 때문에 gradient를 계산하며 grad_fn을 확인할 수 있다. 하지만 leaf tensor가 아니기 때문에 grad를 저장하지 않는다.G.backward() # a의 gradient(가중치)가 상대적으로 높게 계산됨print(a.grad) # tensor([40.])print(b.grad) # tensor([20.])print(F.grad) # UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward()print(F.grad_fn)    # &lt;MulBackward0 object at 0x7f0cb7703250&gt;만약 F.grad를 저장하고 싶다면 backward 전에 retain_grad()를 호출하여 중간 텐서 F에 대해서도 gradient를 유지하도록 할 수 있다.F.retain_grad()  G.backward()     print(F.grad)  # F의 gradient 출력✅ GPU 케이스앞선 예제와 동일한 연산을 수행하되, 이번에는 a와 b를 GPU에 할당하는 중간 단계를 거친다.a = torch.tensor([10.], requires_grad=True)b = torch.tensor([20.], requires_grad=True)# 추가a_cuda = a.to('cuda')b_cuda = b.to('cuda') F = a_cuda * b_cudaG = F * 2backward 결과 CPU 위의 a, b에만 gradient가 저장되고, GPU 위의 a_cuda, b_cuda는 비어 있다. 이는 CPU 위의 텐서들이 최적화 변수로 남아있고, a와 b 로부터 생성된 a_cuda, b_cuda는 leaf tensor가 아닌 중간 텐서(intermediate tensor)로 여겨지기 때문이다. 텐서를 GPU로 옮기는 intermediate한 과정으로 인한 것이다.G.backward()print(a.grad)       # tensor([40.])print(b.grad)       # tensor([20.])print(a_cuda.grad)  # None (UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed.)print(b_cuda.grad)  # None (UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed.)따라서 처음부터 GPU 위에서 변수를 생성한 후 역전파를 수행하자.a_cuda = torch.tensor([10.], requires_grad=True, device='cuda')b_cuda = torch.tensor([20.], requires_grad=True, device='cuda') F = a_cuda * b_cudaG = F * 2 G.backward() print(a_cuda.grad)      # tensor([40.], device='cuda:0')print(b_cuda.grad)      # tensor([20.], device='cuda:0')학습파이토치에서의 학습 과정을 예제를 통해 살펴본다. 전체 과정은 아래 단계로 이루어진다.  DataLoader 클래스 정의 및 객체 생성  model 클래스 정의 및 객체 생성  loss function, optimizer 정의  epoch와 training loop1. DataLoader 클래스 정의 및 객체 생성커스텀 DataLoader 클래스에는 __init__, __len__, __getitem__ 함수를 정의해야 한다. 이 중 __getitem__ 함수는 인자 index를 통해 데이터 샘플을 반환하는 역할을 한다.학습과 평가를 위한 DataLoader 객체를 각각 생성한다: train_dataloader, test_dataloaderimport pandas as pdfrom torch.utils.data import Datasetfrom torch.utils.data import DataLoaderclass CustomDataset(Dataset):    def __init__(self):        self.sample = pd.read_csv('/path/to/data.csv')        self.x = np.array(self.sample[['feature1', 'feature2', 'feature3']])        self.y = np.array(self.sample[['label']])     def __len__(self):        return len(self.sample)         def __getitem__(self, index):        x = torch.tensor(self.x[index].reshape(1, -1), dtype=torch.float32)        y = self.y[index]        return x, y training_data = CustomDataset()train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)DataLoader 객체에서 데이터 샘플을 뽑아보고 싶다면 iter, next 내장함수를 사용한다.data_iterator = iter(train_dataloader)features, label = next(data_iterator)2. model 클래스 정의 및 객체 생성파이토치의 nn.Module를 상속 받아 모델 클래스를 정의한다. __init__ 함수에서 레이어를 초기화하고 forward 함수에서 순전파 구조를 정의한다.입력 데이터와 아웃풋의 사이즈를 잘 고려해야 한다. 연속된 레이어를 쌓아주는 nn.Sequential도 쓸 수 있다.import torch.nn as nnimport torch.nn.functional as F class CustomClassifier(nn.Module):    def __init__(self):        super(CustomClassifier, self).__init__()         self.input_size = 3        self.output_size = 10        self.hidden_dim1 = 64        self.hidden_dim2 = 128         self.fc1 = nn.Linear(self.input_size, self.hidden_dim1)        self.fc2 = nn.Linear(self.hidden_dim1, self.hidden_dim2)        self.fc3 = nn.Linear(self.hidden_dim2, self.output_size)        # 이렇게도 할 수 있다        # self.net = nn.Sequential(        #     nn.Linear(self.feature_num, self.hidden_dim1),        #     nn.ReLU(),        #     nn.Linear(self.hidden_dim1, self.hidden_dim2),        #     nn.ReLU(),        #    nn.Linear(self.hidden_dim2, self.output_size),        # )     def forward(self, x):        x = F.relu(self.fc1(x))        x = F.relu(self.fc2(x))        output = self.fc3(x)                # 이것과 같다        # output = net(x)        return output모델 클래스를 생성하고 전반적인 정보를 확인한다. torchsummary를 활용하면 그냥 print하는 것보다 훨씬 보기 좋게 확인할 수 있다.from torchsummary import summarymodel = CustomClassifier()model = model.to('cuda')batch_size, feature_num = 64, 3summary(model, input_size=(batch_size,  feature_num))특정 레이어의 파라미터 값을 확인하고 싶다면 named_parameters() 혹은 parameters()를 사용한다.# for param in model.parameters():for name, param in model.named_parameters():    if param.requires_grad:        print(name, param.data)3. loss function, optimizer 정의라벨 수가 10인 다중분류를 상정하여 CrossEntropyLoss를 손실함수로 정의하고 임의의 입력값을 통해 loss를 연산한다.loss_fn = nn.CrossEntropyLoss() output_size = 10dummy_outputs = torch.rand(batch_size, output_size)dummy_labels = torch.randint(output_size, (batch_size,)) loss = loss_fn(dummy_outputs, dummy_labels)옵티마이저(otpimizer)는 아주 다양한 종류가 있지만 이곳에서는 SGD를 예시로 든다.optimizer = torch.optim.SGD(model.parameters(), lr=0.001)4. epoch와 training loop한 번의 학습 epoch를 정의했다.  dataloader 객체에서 한 배치의 데이터를 꺼내며 gradient를 초기화한다(zero_grad)  한 배치의 데이터를 모델에 통과시키고(model) 그 값으로 loss를 계산한다(loss_fn)  loss에 대해 gradient를 계산한 후(loss.backward), 그 값을 기반으로 모델을 업데이트한다(optimizer.step)running_loss = 0.0model.train()for data in training_loader:    inputs, labels = data        optimizer.zero_grad()     outputs = model(inputs)    loss = loss_fn(outputs, labels)    loss.backward()    optimizer.step()     running_loss += loss.item() ✅ 동 떨어져 있는 optimizer.zero_grad(), loss.backward(), optimizer.step()이 각각 호출되는데 어떻게 모델이 업데이트되는가? optimizer 를 정의할 때 모델의 파라미터를 넘겨주기 때문에 - torch.optim.SGD(model.parameters(), lr=0.001)내부적으로 grad를 저장하고 값을 업데이트한다. 달리 말해 optimizer가 활용하는 값은 loss도 아닌 model.parameters()의 param.grad 이다.이때 모델을 GPU로 옮기고 난 후에 optimizer를 정의하는 것이 좋다.만약 모델을 GPU로 옮기기 전에 optimizer를 정의하면, optimizer는 CPU 위 파라미터를 추적하게 된다. 그 결과 optimizer가 추적하는 파라미터와 실제 모델의 파라미터가 달라지는 문제가 발생할 수 있다.비슷한 방식으로 validation 을 정의할 수 있다. 단 모델 평가 단계이므로 모델을 eval 모드로 바꾸고 torch.no_grad 안에서 추론이 이루어진다.또한 “loss에 대해 gradient를 계산한 후 그 값을 기반으로 모델을 업데이트”하는 과정은 생략된다.test_loss = 0.0model.eval()with torch.no_grad():    for data in test_dataloader:        inputs, labels = data        outputs = model(inputs)        loss = loss_fn(outputs, labels)        test_loss += loss.item()전자를 train_loop, 후자를 test_loop 이라는 함수로 정의한다면 전체 학습 loop는 이렇게 정의할 수 있다.num_epochs = 100for epoch in range(num_epochs):    train_loop()        if epoch % 10 == 0:        test_loop()"
    } ,
  
    {
      "title"    : "websocket: 클라이언트-서버의 양방향 통신",
      "tags"     : "네트워크, python",
      "date"     : "2023-11-10",
      "url"  : "https://woocosmos.github.io/websocket-base/",
      "content": "Websockets클라이언트(브라우저)와 서버 간의 실시간 양방향 통신을 가능하게 하는 프로토콜 WebSocketHTTP 프로토콜과의 차이는?HTTP 프로토콜은 요청(request)에 응답(response)을 주면 연결이 끊긴다.반면, websocket은 연결을 지속적으로 유지할 수 있기 때문에 실시간 통신이 가능하다.작동 원리  클라이언트가 HTTP프로토콜을 사용해 websocket handshake 요청을 서버로 전송  handshake가 종료되면 HTTP에서 WebSocket 프로토콜로 전환하는 ‘Protocol Swtiching’ 진행, 새로운 소켓 생성          ws:// : 일반 WebSocket      wss:// : SSL을 사용한 보안 WebSocket (HTTPS와 유사하게 암호화됨)        서버와 클라이언트가 실시간 양방향 통신을 수행  이벤트 발생에 따라 클라이언트나 서버가 연결을 종료예제server.pyimport websocketsimport asyncio async def hello(websocket, path):    name = await websocket.recv()    print(f\"&lt; {name}\")     greeting = f\"안녕하세요, {name}!\"         await websocket.send(greeting)    print(f\"&gt; {greeting}\") async def main():    async with websockets.serve(hello, '0.0.0.0', 8765):        await asyncio.Future()         if __name__ == '__main__':    asyncio.run(main())client.pyimport websocketsimport asyncio async def hello():    uri = \"ws://0.0.0.0:8765\"    async with websockets.connect(uri) as websocket:        name = input('what is your name? ')        await websocket.send(name)        print(name)         greeting = await websocket.recv()        print(greeting) if __name__ == '__main__':    asyncio.run(hello())  server.py 를 실행한다  client.py 를 실행한다  클라이언트가 이름[입력 내용]을 서버에 전달한다  이름[입력 내용]을 전달 받은 서버가 인사말을 붙여 클라이언트에 전달한다  인사말+이름을 전달받은 클라이언트가 내용을 출력한다클라이언트 종료. 서버는 유지된다."
    } ,
  
    {
      "title"    : "포모도로 윈도우 앱 &#39;토스터&#39; 미니 프로젝트",
      "tags"     : "네트워크, python",
      "date"     : "2023-10-13",
      "url"  : "https://woocosmos.github.io/toatser-project/",
      "content": "개요효율적이고 건강한 근무 습관을 위한 간단한 윈도우 앱을 만들어보기로 했다. 윈도우 팝업을 toast notification이라고 부르는 데서 착안하여 프로젝트명을 토스터toaster라고 지었다.포모도로 테크닉이란?  1980년대 후반 프란체스코 시릴로가 제안한 시간 관리 방법론. 타이머를 이용해서 25분간 집중해서 일을 한 다음 5분간 휴식하는 방식이다. 토마토(Pomodoro는 이탈리아어로 토마토를 뜻함) 모양의 요리용 타이머에서 이름이 유래했다.기획구현하고 싶은 기능은 단순하다.  출근해서 컴퓨터를 부팅하면 그날의 목표 퇴근시간을 기입한다.  52분 근무하고 15분 휴식하는 타임 테이블을 윈도우 팝업으로 운영한다.  45분마다 물 마시기를 팝업으로 리마인드한다.  목표 퇴근 시간 20분 전부터 팝업으로 퇴근 준비를 알린다.흐름도 형식으로 프로그램의 전체 로직을 표현했다. 크게 initializer와 main으로 나눠진다.  initializer : PC가 켜졌을 때 최초 시간 정보를 세팅한다  main : 주요 팝업 이벤트를 발생시킨다main 파트의 ‘베이스 타임 모듈’의 로직을 자세히 살펴보자.  30초마다 현재 시간을 체크한다 → PC 종료 전 마지막으로 체크된 현재 시간이 ‘종료 시간’이 된다  목표 퇴근 시간 전 k분은 현재 20, 10, 5분으로 설정되어 있는데, config.py 에서 before_end 리스트에 추가해주면 된다  목표 퇴근 시간 전 0분의 경우, 퇴근을 알리는 팝업이 발동된다‘타이머’는 config.py 에서 설정한 값대로 반복된다  물 : 45분마다 알림  근무/휴식 : 52+15분마다 알림          근무 타이머를 먼저 시작한 후, 52분이 지나면 휴식 팝업을 임시 발동한 후 이어 휴식 타이머를 시작한다      구현  Windows 11  Python 3.12  pipenv 로 패키지 관리Plyer데스크톱, 모바일 등 다양한 플랫폼에서 작동하는 GUI 앱 개발을 위한 파이썬 프레임워크다. 이것을 설치해서,pipenv install plyer바로 윈도우 샘플 토스트를 띄울 수 있다from plyer import notification notification.notify(    title = '제목입니다.',    message = '메시지 내용입니다.',    app_name = \"앱 이름\",    app_icon = './dao.ico', # 'C:\\\\icon_32x32.ico'    timeout = 10,  # seconds)모듈부팅시 실행윈도우 부팅할 때 파이썬 스크립트가 실행되어야 한다. 파이썬 스크립트(py)를 실행시키는 스크립트(bat)을 윈도우 시작프로그램 폴더에 추가해준다.python.exe /path/to/main.py위와 같이 정의한 run_toaster.bat 파일을 윈도우 시작프로그램 폴더에 위치시키는 것이다. 시작프로그램 폴더의 경로는 이쪽이다. C:\\Users\\{사용자명}\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Startup입력 팝업창tkinter 를 사용해서 아주 간단한 대화창 UI를 생성했다.import tkinter as tkfrom tkinter import simpledialog ROOT = tk.Tk()ROOT.withdraw() # the input dialogUSER_INP = simpledialog.askstring(title=\"테스트 입력창\",                                  prompt=\"몇 시에 퇴근 할 거야?:\")# check itprint(\"Hello\", USER_INP)입력어 처리내가 어떤 형식으로 입력하더라도 (시간, 분) 튜플로 처리하는 모듈이 필요하다.예를 들어, 12시55분/열두시55분/12:55/1255 … 모두 (12, 55)로 처리할 것이다.def process_str(INPUT_STRING):    UHR_dict = {        \"한\": 1,        \"두\": 2,        \"세\": 3,        \"네\": 4,        \"다섯\": 5,        \"여섯\": 6,        \"일곱\": 7,        \"여덟\": 8,        \"아홉\": 9,        \"열\": 10,        \"열한\": 11,        \"열두\": 12,    }    MINT_dict = {        \"일\": 1,        \"이\": 2,        \"삼\": 3,        \"사\": 4,        \"오\": 5,        \"육\": 6,        \"칠\": 7,        \"팔\": 8,        \"구\": 9,        \"십\": 10,        \"반\": 30,    }     INPUT_STRING = INPUT_STRING.replace(\" \", \"\")    INPUT_STRING = INPUT_STRING.replace(\":\", \"시\")    INPUT_STRING = INPUT_STRING.replace(\"분\", \"\")      # 숫자로만 이루어진 경우    if INPUT_STRING.isdigit():         if len(INPUT_STRING) &lt;= 2:            UHR = int(INPUT_STRING)            MINT = 0                 else:            tmp_UHR = int(INPUT_STRING[:2])            if tmp_UHR &gt; 25:                UHR = int(INPUT_STRING[0])                MINT = int(INPUT_STRING[1:])            else:                UHR = tmp_UHR                MINT = int(INPUT_STRING[2:])                 if (UHR &gt; 24) or (MINT &gt;= 60):            return (0, 0)                 return (UHR, MINT)          elif '시' in INPUT_STRING:        cans = INPUT_STRING.split(\"시\")        UHR,  MINT = cans[0], cans[1]         if UHR.isdigit() and int(UHR) &lt;= 24:            UHR = int(UHR)        elif UHR in UHR_dict.keys():            UHR = UHR_dict[UHR]         if MINT.isdigit() and int(MINT) &lt; 60:            MINT = int(MINT)        elif len(MINT) == 1:            MINT = MINT_dict[MINT]        elif \"십\" in MINT:            ten = 1 if not MINT.split(\"십\")[0] else MINT_dict[MINT.split(\"십\")[0]]            one = 0 if not MINT.split(\"십\")[1] else MINT_dict[MINT.split(\"십\")[1]]            MINT = ten * 10 + one         if not MINT:            MINT = 0        elif isinstance(MINT, str):            return (0, 0)         return (UHR, MINT)     return (0, 0)타이머두 개의 타이머가 필요하다.  타이머1. 시작 52분 후 휴식 알림, 그로부터 15분 후 근무 알림, 반복 …  타이머2. 매 45분 음수 알림이 두 개의 타이머가 동시에 돌아가도록 스레딩(Threading)을 이용했다.…내용 추가 예정데이터data 폴더에 work_schedule.csv 파일로 시간 데이터를 업데이트 한다            날짜      booting      goal      actual                  2023-10-10      10:11      19:50      20:51              2023-10-11      10:12      18:20      18:24              2023-10-12      10:36      19:00      18:48        booting : 최초 부팅 시간  goal : 목표 퇴근 시간  actual : PC 종료 시간"
    } ,
  
    {
      "title"    : "Markdown Cheatsheet",
      "tags"     : "샘플",
      "date"     : "1996-12-10",
      "url"  : "https://woocosmos.github.io/style-test-ko/",
      "content": "글 작성시 참고할 수 있는 스타일 치트시트1. 제목제목 1제목 2제목 3제목 4제목 5제목 61-1. 제목 정렬왼쪽(기본)가운데오른쪽2. 본문탄핵소추의 의결을 받은 자는 탄핵심판이 있을 때까지 링크 그 권한행사가 정지된다. 제2항과 제3항의 처분에 대하여는 굵게 법원에 제소할 수 없다. 선거와 국민투표의 공정한 관리 기울임 및 정당에 관한 사무를 처리하기 위하여 53 = 125 선거관리위원회를 둔다. 국무총리는 대통령을 보좌하며, H2O 행정에 관하여 대통령의 명을 받아 행정각부를 통할한다. 정당은 법률이 정하는 바에 의하여 밑줄 국가의 보호를 받으며, 국가는 법률이 정하는 바에 의하여 파일명 정당운영에 필요한 자금을 보조할 수 있다. 모든 국민은 종교의 자유를 가진다. 국회의원은 법률이 정하는 직을 겸할 수 없다. 감사원은 가운데 줄 세입·세출의 결산을 매년 검사하여 대통령과 차년도국회에 형광펜 그 결과를 보고하여야 한다.3. 이미지3-1. 이미지 가운데 정렬4. 인용구  나는 헌법을 준수하고 국가를 보위하며 조국의 평화적 통일과 국민의 자유와 복리의 증진 및 민족문화의 창달에 노력하여 대통령으로서의 직책을 성실히 수행할 것을 국민 앞에 엄숙히 선서합니다. 모든 국민은 통신의 비밀을 침해받지 아니한다. 국회의원은 법률이 정하는 직을 겸할 수 없다. 국회는 헌법 또는 법률에 특별한 규정이 없는 한 재적의원 과반수의 출석과 출석의원 과반수의 찬성으로 의결한다. 가부동수인 때에는 부결된 것으로 본다.5. 리스트순서가 없는 리스트  국가원로자문회의의 조직·직무범위 기타 필요한 사항은 법률로 정한다.  모든 국민은 법률이 정하는 바에 의하여 국방의 의무를 진다.  탄핵소추의 의결을 받은 자는 탄핵심판이 있을 때까지 그 권한행사가 정지된다.          국회는 의원의 자격을 심사하며, 의원을 징계할 수 있다. 국가는 모성의 보호를 위하여 노력하여야 한다.      공개하지 아니한 회의내용의 공표에 관하여는 법률이 정하는 바에 의한다.        국회는 의장 1인과 부의장 2인을 선출한다. 정부는 회계연도마다 예산안을 편성하여 회계연도 개시 90일전까지 국회에 제출하고, 국회는 회계연도 개시 30일전까지 이를 의결하여야 한다.순서가 있는 리스트  대통령으로 선거될 수 있는 자는 국회의원의 피선거권이 있고 선거일 현재 40세에 달하여야 한다.  각급 선거관리위원회는 선거인명부의 작성등 선거사무와 국민투표사무에 관하여 관계 행정기관에 필요한 지시를 할 수 있다.  예비비는 총액으로 국회의 의결을 얻어야 한다. 예비비의 지출은 차기국회의 승인을 얻어야 한다.          국가는 농지에 관하여 경자유전의 원칙이 달성될 수 있도록 노력하여야 하며, 농지의 소작제도는 금지된다.      모든 국민은 법률이 정하는 바에 의하여 납세의 의무를 진다.        형사피의자 또는 형사피고인으로서 구금되었던 자가 법률이 정하는 불기소처분을 받거나 무죄판결을 받은 때에는 법률이 정하는 바에 의하여 국가에 정당한 보상을 청구할 수 있다.정의 리스트  생각보다 길어진 키워드  키워드1에 대한 설명입니다. 키워드1에 대한 설명입니다. 키워드1에 대한 설명입니다. 키워드1에 대한 설명입니다.  짧은 키워드  키워드2에 대한 설명입니다. 키워드2에 대한 설명입니다. 키워드2에 대한 설명입니다. 키워드2에 대한 설명입니다.6. 표            상단1      상단2      상단3                  셀1      셀2      셀3              셀4      셀5      셀6                  셀1      셀2      셀3              셀4      셀5      셀6                  하단1      하단2      하단3      7. 코드 스타일코드 하이라이팅#container {  float: left;  margin: 0 -240px 0 0;  width: 100%;}일반적인 코드&lt;div id=\"awesome\"&gt;  &lt;p&gt;This is great isn't it?&lt;/p&gt;&lt;/div&gt;8. LaTexdisplay LaTex\\[\\sum_{i=1}^{k} \\sum_{\\mathbf{x} \\in S_{i}} \\left\\|\\mathbf{x} - \\boldsymbol{\\mu}_{i}\\right\\|^{2}\\]$$\\sum_{i=1}^{k} \\sum_{\\mathbf{x} \\in S_{i}} \\left\\|\\mathbf{x} - \\boldsymbol{\\mu}_{i}\\right\\|^{2}$$  inline LaTex문장 중간에 이렇게 $\\sum_{\\mathbf {x} \\in S_{i}}\\mathbf {x}$ 넣을 수 있습니다문장 중간에 이런 수식을 $\\sum_{\\mathbf {x} \\in S_{i}}\\mathbf {x}$ 넣을 수 있습니다기타포스트에 목차를 추가하고 싶다면 toc 옵션을 참으로 줍니다.- toc: true본문에서 TOC에 추가하고 싶지 않은 항목은 no_toc 클래스를 활용합니다. 제목만큼 커지지만 목차에는 포함되지 않아요 &lt;h4 class=\"no_toc\"&gt; 제목만큼 커지지만 목차에는 포함되지 않아요 &lt;/h4&gt;본문에서 특정 요소에 속하는 하위 요소가 모두 TOC에 추가되지 않도록 하고 싶다면 no_toc_section 클래스를 활용합니다.  목차에 안 들어감  목차에 안 들어감&lt;div class=\"no_toc_section\"&gt;  &lt;h5&gt;목차에 안 들어감&lt;/h5&gt;  &lt;h5&gt;목차에 안 들어감&lt;/h5&gt;&lt;/div&gt;HTML 태그로 이렇게 억지로 스타일을 적용할 수도 있습니다.&lt;b&gt;이렇게 &lt;i&gt;억지로&lt;/i&gt; 스타일을&lt;/b&gt;Jekyll 문법에 대해서 기술하는 과정에서 {{ content }}와 같은 문자열을 추가했다가 실제 content가 불러와지는 경우가 있습니다.이는 \\\\{\\\\% raw \\\\%\\\\} ... \\\\{\\\\% endraw \\\\%\\\\} 태그로 감싸서 escape할 수 있습니다."
    } ,
  
    {
      "title"    : "삶의 지도(1)",
      "tags"     : "회고",
      "date"     : "1996-12-10",
      "url"  : "https://woocosmos.github.io/map-of-life-01/",
      "content": "작성일: 2024-09-15(일)판교, 2024년 가을. 길고도 짧은 대기발령 끝에 본사 OO본부로 배치되었다. 이번주가 입사 첫 주였다. 이제 막 셋업되어 가는 팀이라 해야 할 게 너무나도 많았다. 사업 중심의 조직은 처음이어서 심적으로 낯설기도 했다. 금요일 밤 9시가 다 되어가는 시간에 퇴근하고 곧바로 자취방 근처 삼겹살 집으로 향했다. 고깃집이 혼밥이야말로 만렙이라더니, 생각보다 할 만 했다. 집에서 챗지피티에게 취한 상태로 샤워를 해도 되는지 물어봐놓고 껌뻑 잠에 들어버렸다.테헤란로, 2024년인원 감축이 공식적으로 발표되었다. 발표 직후 팀장님이 얼빠진 우리를 회의실로 불러 모았다. 그 자리에서 J님은 안경을 한번 치켜올리고 “이제 우린 어떻게 되나요?”라고 물어보았다. 왜 “왜 이렇게 되었나요?”는 묻지 않지? 지금 당장 살아남기 위해 던져야 할 질문은 그게 아니기 때문이다. 며칠 후, 모두가 퇴근한 저녁에 J님은 나에게 다가와 다시 한 번 말해주었다. “앞으로 무엇을 해야 할지를 생각해야 돼요. 왜 이렇게 되었는지는 누구도 설명할 수 없기 때문에.” 그 다음 절차는 하나씩 신속하게 이루어졌다. 오전 안에 본체와 모니터를 뽑아 자리를 옮기고 백지의 워드 파일에 이력서를 작성하기 시작했다. 뉴스에 우리 회사 이름이 오르내리고 사내 메신저에는 갈 곳을 잃은 사람들이 옹기종기 모였다. 아침 10시에 출근해서 저녁 7시에 퇴근할 때까지 ‘앞으로 무엇을 해야 할지’ 생각했다. 취업을 준비하던 4년 전 2020년으로 거슬러 올라가 자료를 뒤져 포트폴리오를 기워 넣었다. 상황이 왜 이렇게 되었는지는 해명할 수 없었지만 ‘왜 지금의 내가 되었는지’는 스스로 증명해야 했다.이곳 회사는 2022년 겨울에 전환형 인턴으로 들어왔다. 회사에 몇 없는 공채 출신이라며 사람들은 농담 반 진담 반으로 놀리곤 했다. 동기는 다섯 명이었고 다들 실력이 출중했다. 그들에 비해 부족한 내가 합격한 것은 운이 매우 좋았기 때문에 그리고 이 회사 서비스의 열혈 유저였기 때문이라고 생각했다. 사실 다들 비슷한 불안감을 하나씩 품고 있었던 터라 동기들과는 참 끈끈하게 지냈다. 게다가 프로그래머, 아트, 기획, 데이터 - 일하는 분야가 제각각이어서 흥미로웠고 애정이 갔다. 회사 사정이 불안정할 때마다 우리는 퇴근 후 술잔을 부딪히며 함께하기를 다짐했다. 불확신으로 가득한 6개월이 지나고 인턴 평가 결과가 나왔다. 동기 한 명은 다른 길을 걷게 되었다. 그토록 원하던 정직원 전환에 성공했는데 기쁘지 않았다. 이 회사가, 이 사회가 원망스러웠다. 그래도 해야 할 일은 해야 했다. 바뀐 건 인사 시스템 상 정보일 뿐, 똑같은 자리에서 데이터를 분석하고 대시보드를 만드는 일을 이어나갔다. 금새 일 년이 지나고 미지의 영역이었던 강화학습에도 발을 들였다. 인원 감축 발표가 있던 그날까지도 gymnasium 의 wrapper 객체를 뜯어 고쳤다.그 전 회사에서 정확히 일 년을 채우고 제 발로 나온 게 2022년 여름이었다. 그곳은 공덕역의 한 공유 오피스에 입주해 있었다. 서로를 영어 닉네임으로 부르고 주에 한 번씩 개발자들끼리 스터디도 하는 전형적인 개발자 중심의 젊은 스타트업이었다. 입사와 동시에 실험적이고 도전적인 프로젝트에 투입되었다. 그 프로젝트는 이름에 인공지능이나 청년과 같은 단어가 들어가는 국가 사업에 지원할 때 요긴하게 쓰였다. 그렇게 지급 받은 지원금으로 꽤 괜찮은 워크스테이션을 하나 맞출 수 있었다. 워크스테이션의 서버명은 페르세포네(Persephone), 하데스에게 납치 당해 지하 세계로 끌려간 페르세포네의 신화에서 딴 이름이었다. 페르세포네를 통해서 처음으로 리눅스 환경에서 개발을 하기 시작했다. 잘 몰라서 버벅이는 스스로가 답답했다. 컴퓨터에 대해 더 알고 싶었다. 잘하고 싶었다. 이렇게나 갈 길이 멀었는데도 퇴사를 결심한 것은 회사가 그 프로젝트에 이만 흥미를 잃었기 때문이다. 사업 지원금이 끊긴 이후로 회사는 서운할 정도로 나의 작업에 무관심해졌지만 혼자서 프로젝트를 끝까지 마무리했고 앞으로 더 잘하기 위해 그곳에서 나왔다.오늘날 개발자 양성 학원 광고에서 전형적으로 어필하는 것과 같이 - 아무런 스펙 없는 비전공자가 그곳에 취업한 것은 2020년 겨울부터 6개월 동안 진행된 국비지원 부트캠프에서 연계해준 덕분이었다. 코로나로 온 나라가 뒤집혀 강의는 온라인으로 진행되었지만 주관 측에 양해를 구하고 홀로 강의실에 나와 수업을 들었다. 1.5평짜리 대학가 자취방에서 접이식 테이블 앞에 쪼그려 앉아 그램 노트북으로 8시간 수업을 듣느니 충무로에 위치한 교육장으로 가겠다고 결심한 것이다. 같은 공간에서 강사님과 나 이렇게 두 사람이 각자 노트북 카메라를 바라보며 수업을 진행하는 건 퍽 웃긴 광경이었다. 게다가 40명 규모의 강의실에 달랑 두 인원만 들어 앉아 있으니 히터를 최대로 틀어놔도 입김이 풀풀 나왔다. 굳이 강의장에 오겠다고 고집 부린 수강생 한 명 때문에 덩달아 출근하게 된 J 선생님은 나의 존재가 야속할 법한데도 매일 점심 밥을 사주었다. 뭣도 없이 지원한 인턴 공고에 떨어져 좌절하던 날에도 J 선생님은 똑같이 밥을 사주며 앞으로 많이 떨어져봐야 한다고 말해주었다. 신기하게도 그 말이 어떤 위로보다도 도움이 되어 다시 다른 인턴에 다시 지원했다. 결과적으로 그 공고에서도 떨어졌지만, 판교까지 목발 짚고 가서 면접을 보는 귀한 경험도 할 수 있었다.국제관 라운지, 2020년부트캠프 직전에는 자대 교직원 인턴으로 일하고 있었다. 대학교 막학기를 다니면서 졸업 후 무엇을 해야 할지 보이는 대로 고민하던 시기였다. 그간 대외 스펙은 엄두가 안 나서 학내 활동에만 무식하게 참여했다. 그 이력이 오히려 기회가 되어 교직원 인턴에 합격하게 됐다. 입사 경쟁률은 어마어마하지만 공무원만큼 안정적이라는 교직원을 본격적으로 준비해볼까, 자연스럽게 고민하게 되었다. 졸업 이후 공무원 시험의 길로 흘러 간 그 많은 친구들과 비슷하게 말이다. 한편 외국어로서의 한국어 강사라는 직종도 오래 전부터 고려하고 있었다. 강사 처우와 체계도 제대로 갖춰지지 않았으면서 석박사는 필수여야 한다는 그 분야. 마침 인턴으로 일하던 부서도 학내 국제교류를 담당하는 곳이었다.인턴 종료일이 다가오던 어느날 P 부장님이 슬쩍 내 옆으로 와서 졸업 후 무슨 일을 하고 싶냐고 물어보았다. 학내 어학당에서 일하다가 지금 부서로 넘어온 만큼 한국어 교육 분야로 잔뼈가 굵은 사람이었다. 그때 P 부장님이 들려준 이야기들은 내가 완전히 다른 분야로 고개를 돌리게 될 만큼 현실적이었다. 그저 외국어를 할 줄 알고 모국어를 애정한다는 것은 그 분야로 향하는 고생을 감수할 만큼 확실한 동기가 아니었다. 적당히 할 만한 일만 좇아 살아 가다가 현실에 잘근잘근 잡아 먹히는 것뿐이었다. 일종의 생존본능인지 역설적으로 용기와 도전정신이 샘솟았다. ‘원래의 나라면 안 해 볼만한 재밌는 일에 한 번 도전해보자.‘이어 나의 관심은 교직원도, 한국어 교육도 아닌 인공지능으로 향했다. 막학기에 수강한 ‘인문학도를위한한국어인공지능데이터과학이론과실제’ 수업이 재밌었기 때문이다. 터무니 없이 긴 강의명이 주는 압도감 때문인지 인문학도 자존심에 순수 학문도 아닌 코딩을 한다고 해서 그런지 개강 첫 주에 수강생 대부분이 도망갔다. 폐강 기준을 아슬아슬하게 넘긴 인원끼리 파이썬 print 함수부터 배우기 시작했다. 기말 과제로는 데이팅앱 프로필 자기소개 문구를 다룬 자연어처리 프로젝트를 진행했다. 신기하게도 과제를 수행하다가 어딘가 막혀도 인터넷에 검색하면 같은 고민을 한 사람들이 넘쳐나서 스스로 해결할 수가 있었다. 또 매일 같이 새로운 연구와 모델이 공개되고 트렌드가 바뀌어가는 것이 눈에 띄었다. 기껏 최근 연구라봤자 1990년대 그것도 논문 스캔본을 직접 출력해서 한자어 독음을 하나씩 표시해 가며 공부하던 원래 방식과는 딴판이었다. 만족스러운 성적으로 학기를 마무리하고 졸업을 신청했다. 컴퓨터 앞에 앉아 무작정 자연어처리 취업을 구글링하기 시작했다. 모집 중인 부트캠프가 있었다."
    } ,
  
    {
      "title"    : "Code Highlighting Cheatsheet",
      "tags"     : "샘플",
      "date"     : "1996-12-10",
      "url"  : "https://woocosmos.github.io/code-highlighting-post/",
      "content": "Demo post displaying the various ways of highlighting code in Markdown.Syntax highlighting is a feature that displays source code, in different colors and fonts according to the category of terms. This feature facilitates writing in a structured language such as a programming language or a markup language as both structures and syntax errors are visually distinct. Highlighting does not affect the meaning of the text itself; it is intended only for human readers.1Highlighted Code BlocksTo modify styling and highlight colors edit /_sass/_highlighter.scss.#container {    float: left;    margin: 0 -240px 0 0;    width: 100%;}&lt;nav class=\"pagination\" role=\"navigation\"&gt;    {% if page.previous %}        &lt;a href=\"{{ site.url }}{{ page.previous.url }}\" class=\"btn\" title=\"{{ page.previous.title }}\"&gt;Previous article&lt;/a&gt;    {% endif %}    {% if page.next %}        &lt;a href=\"{{ site.url }}{{ page.next.url }}\" class=\"btn\" title=\"{{ page.next.title }}\"&gt;Next article&lt;/a&gt;    {% endif %}&lt;/nav&gt;&lt;!-- /.pagination --&gt;module Jekyll  class TagIndex &lt; Page    def initialize(site, base, dir, tag)      @site = site      @base = base      @dir = dir      @name = 'index.html'      self.process(@name)      self.read_yaml(File.join(base, '_layouts'), 'tag_index.html')      self.data['tag'] = tag      tag_title_prefix = site.config['tag_title_prefix'] || 'Tagged: '      tag_title_suffix = site.config['tag_title_suffix'] || '&amp;#8211;'      self.data['title'] = \"#{tag_title_prefix}#{tag}\"      self.data['description'] = \"An archive of posts tagged #{tag}.\"    end  endendStandard Code Block&lt;nav class=\"pagination\" role=\"navigation\"&gt;    {% if page.previous %}        &lt;a href=\"{{ site.url }}{{ page.previous.url }}\" class=\"btn\" title=\"{{ page.previous.title }}\"&gt;Previous article&lt;/a&gt;    {% endif %}    {% if page.next %}        &lt;a href=\"{{ site.url }}{{ page.next.url }}\" class=\"btn\" title=\"{{ page.next.title }}\"&gt;Next article&lt;/a&gt;    {% endif %}&lt;/nav&gt;&lt;!-- /.pagination --&gt;GitHub Gist EmbedAn example of a Gist embed below.            http://en.wikipedia.org/wiki/Syntax_highlighting &#8617;      "
    } 
  
]